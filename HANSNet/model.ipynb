{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HANS-Net Components Implementation\n",
    "\n",
    "# This notebook implements the core building blocks for the Hyperbolic Attention Network for Segmentation (HANS-Net):\n",
    "# - **WaveletDecomposition**: 2D DWT for multi-frequency feature extraction\n",
    "# - **SynapticPlasticity & PlasticConvBlock**: Adaptive convolutions with Hebbian-inspired learning\n",
    "# - **HyperbolicConvBlock**: Convolutions in Poincaré ball geometry\n",
    "# - **TemporalAttention**: Cross-frame attention for temporal context\n",
    "# - **INRBranch**: Implicit Neural Representation for boundary refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab26435",
   "metadata": {},
   "source": [
    "## 1. Wavelet Decomposition\n",
    "\n",
    "Applies 2D Discrete Wavelet Transform using Haar wavelets to decompose input into four subbands:\n",
    "- **LL**: Low-Low (approximation coefficients)\n",
    "- **LH**: Low-High (horizontal details)\n",
    "- **HL**: High-Low (vertical details)  \n",
    "- **HH**: High-High (diagonal details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletDecomposition(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Discrete Wavelet Transform using Haar wavelets.\n",
    "    \n",
    "    Decomposes each input channel into 4 subbands (LL, LH, HL, HH),\n",
    "    effectively converting [B, C, H, W] -> [B, C*4, H/2, W/2].\n",
    "    This captures multi-frequency information useful for segmentation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Haar wavelet filters (normalized)\n",
    "        # Low-pass filter: [1, 1] / sqrt(2)\n",
    "        # High-pass filter: [1, -1] / sqrt(2)\n",
    "        ll = torch.tensor([[1, 1], [1, 1]], dtype=torch.float32) / 2.0   # LL: avg\n",
    "        lh = torch.tensor([[1, 1], [-1, -1]], dtype=torch.float32) / 2.0  # LH: horizontal\n",
    "        hl = torch.tensor([[1, -1], [1, -1]], dtype=torch.float32) / 2.0  # HL: vertical\n",
    "        hh = torch.tensor([[1, -1], [-1, 1]], dtype=torch.float32) / 2.0  # HH: diagonal\n",
    "        \n",
    "        # Stack filters: [4, 1, 2, 2]\n",
    "        filters = torch.stack([ll, lh, hl, hh], dim=0).unsqueeze(1)\n",
    "        self.register_buffer('filters', filters)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [B, C, H, W]\n",
    "        Returns:\n",
    "            Wavelet coefficients [B, C*4, H//2, W//2]\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Apply filters to each channel independently\n",
    "        # Reshape to process all channels: [B*C, 1, H, W]\n",
    "        x_reshape = x.view(B * C, 1, H, W)\n",
    "        \n",
    "        # Convolve with 4 wavelet filters, stride=2 for downsampling\n",
    "        # Output: [B*C, 4, H//2, W//2]\n",
    "        coeffs = F.conv2d(x_reshape, self.filters, stride=2, padding=0)\n",
    "        \n",
    "        # Reshape back: [B, C*4, H//2, W//2]\n",
    "        _, _, H_out, W_out = coeffs.shape\n",
    "        coeffs = coeffs.view(B, C * 4, H_out, W_out)\n",
    "        \n",
    "        return coeffs\n",
    "\n",
    "\n",
    "class WaveletReconstruction(nn.Module):\n",
    "    \"\"\"\n",
    "    Inverse 2D Discrete Wavelet Transform (optional, for reconstruction).\n",
    "    Converts [B, C*4, H, W] -> [B, C, H*2, W*2]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Inverse Haar filters (transposed)\n",
    "        ll = torch.tensor([[1, 1], [1, 1]], dtype=torch.float32) / 2.0\n",
    "        lh = torch.tensor([[1, 1], [-1, -1]], dtype=torch.float32) / 2.0\n",
    "        hl = torch.tensor([[1, -1], [1, -1]], dtype=torch.float32) / 2.0\n",
    "        hh = torch.tensor([[1, -1], [-1, 1]], dtype=torch.float32) / 2.0\n",
    "        \n",
    "        filters = torch.stack([ll, lh, hl, hh], dim=0).unsqueeze(0)  # [1, 4, 2, 2]\n",
    "        self.register_buffer('filters', filters)\n",
    "    \n",
    "    def forward(self, coeffs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coeffs: Wavelet coefficients [B, C*4, H, W] where C*4 is divisible by 4\n",
    "        Returns:\n",
    "            Reconstructed tensor [B, C, H*2, W*2]\n",
    "        \"\"\"\n",
    "        B, C4, H, W = coeffs.shape\n",
    "        C = C4 // 4\n",
    "        \n",
    "        # Reshape: [B*C, 4, H, W]\n",
    "        coeffs = coeffs.view(B * C, 4, H, W)\n",
    "        \n",
    "        # Transposed conv for upsampling\n",
    "        x = F.conv_transpose2d(coeffs, self.filters, stride=2, padding=0)\n",
    "        \n",
    "        # Reshape: [B, C, H*2, W*2]\n",
    "        x = x.view(B, C, H * 2, W * 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e0745",
   "metadata": {},
   "source": [
    "## 2. Synaptic Plasticity & PlasticConvBlock\n",
    "\n",
    "Implements Hebbian-inspired adaptive weight modulation:\n",
    "- **SynapticPlasticity**: Learns per-channel gain and modulation based on input statistics\n",
    "- **PlasticConvBlock**: Convolution with plasticity-enhanced weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynapticPlasticity(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable weight modulation inspired by Hebbian learning.\n",
    "    \n",
    "    Implements activity-dependent gain modulation where the effective\n",
    "    weights are scaled based on learned per-channel parameters and\n",
    "    input statistics. This allows the network to adaptively strengthen\n",
    "    or weaken connections during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        # Learnable per-channel gain (multiplicative)\n",
    "        self.gain = nn.Parameter(torch.ones(channels))\n",
    "        # Learnable threshold for activation-dependent modulation\n",
    "        self.threshold = nn.Parameter(torch.zeros(channels))\n",
    "        # Plasticity rate (how strongly input affects modulation)\n",
    "        self.plasticity_rate = nn.Parameter(torch.ones(1) * 0.1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute plasticity-modulated features.\n",
    "        \n",
    "        Args:\n",
    "            x: Input features [B, C, H, W]\n",
    "        Returns:\n",
    "            Modulated features [B, C, H, W]\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Compute per-channel activation statistics\n",
    "        # Mean activation per channel: [B, C]\n",
    "        channel_mean = x.mean(dim=(2, 3))\n",
    "        \n",
    "        # Hebbian modulation: strengthen active channels\n",
    "        # modulation = sigmoid(plasticity_rate * (activation - threshold))\n",
    "        modulation = torch.sigmoid(\n",
    "            self.plasticity_rate * (channel_mean - self.threshold.view(1, -1))\n",
    "        )  # [B, C]\n",
    "        \n",
    "        # Apply gain and modulation\n",
    "        # Effective scale = gain * (1 + modulation)\n",
    "        effective_scale = self.gain.view(1, -1, 1, 1) * (1 + modulation.view(B, C, 1, 1))\n",
    "        \n",
    "        return x * effective_scale\n",
    "\n",
    "\n",
    "class PlasticConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional block with synaptic plasticity, batch normalization, and GELU activation.\n",
    "    \n",
    "    This is the fundamental building block for encoder/decoder stages.\n",
    "    The plasticity mechanism allows adaptive feature modulation during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        use_plasticity: bool = True,\n",
    "        use_residual: bool = False,\n",
    "        dropout_p: float = 0.0  # GPT-4 EDIT\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_plasticity = use_plasticity\n",
    "        self.use_residual = use_residual and (in_channels == out_channels) and (stride == 1)\n",
    "        \n",
    "        padding = kernel_size // 2\n",
    "        \n",
    "        # Main convolution path\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Synaptic plasticity (applied after second conv)\n",
    "        if use_plasticity:\n",
    "            self.plasticity = SynapticPlasticity(out_channels)\n",
    "        \n",
    "        # Activation\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout2d(dropout_p) if dropout_p > 0.0 else nn.Identity()  # GPT-4 EDIT\n",
    "\n",
    "        \n",
    "        # Residual projection if dimensions change\n",
    "        if use_residual and (in_channels != out_channels or stride != 1):\n",
    "            self.residual_proj = nn.Conv2d(in_channels, out_channels, 1, stride, bias=False)\n",
    "        else:\n",
    "            self.residual_proj = None\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [B, C_in, H, W]\n",
    "        Returns:\n",
    "            Output tensor [B, C_out, H', W']\n",
    "        \"\"\"\n",
    "        identity = x\n",
    "        \n",
    "        # First conv block\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        # Second conv block\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Apply synaptic plasticity\n",
    "        if self.use_plasticity:\n",
    "            out = self.plasticity(out)\n",
    "        \n",
    "        # Residual connection\n",
    "        if self.use_residual:\n",
    "            if self.residual_proj is not None:\n",
    "                identity = self.residual_proj(identity)\n",
    "            out = out + identity\n",
    "        \n",
    "        out = self.dropout(out)         # GPT-4 EDIT\n",
    "        out = self.act(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a8bc2",
   "metadata": {},
   "source": [
    "## 3. Hyperbolic Convolution Block\n",
    "\n",
    "Operations in the Poincaré ball model of hyperbolic space:\n",
    "- **exp_map_zero**: Maps Euclidean vectors to the Poincaré ball (from origin)\n",
    "- **log_map_zero**: Maps Poincaré ball vectors back to Euclidean (to origin)\n",
    "- **HyperbolicConvBlock**: Performs convolution in hyperbolic space, better capturing hierarchical structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2124e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Hyperbolic Geometry Helpers (Poincaré Ball Model)\n",
    "# ============================================================================\n",
    "\n",
    "def exp_map_zero(v: torch.Tensor, c: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Exponential map from the origin in the Poincaré ball.\n",
    "    \n",
    "    Maps a Euclidean vector v to a point on the Poincaré ball with curvature -c.\n",
    "    Formula: exp_0(v) = tanh(sqrt(c) * ||v||) * v / (sqrt(c) * ||v||)\n",
    "    \n",
    "    Args:\n",
    "        v: Euclidean vector [*, D] (tangent vector at origin)\n",
    "        c: Curvature parameter (positive scalar, represents -1/c^2 curvature)\n",
    "        eps: Small constant for numerical stability\n",
    "    Returns:\n",
    "        Point on Poincaré ball [*, D]\n",
    "    \"\"\"\n",
    "    sqrt_c = torch.sqrt(c)\n",
    "    v_norm = v.norm(dim=-1, keepdim=True).clamp(min=eps)\n",
    "    \n",
    "    # Clamp the argument to tanh to avoid saturation\n",
    "    tanh_arg = (sqrt_c * v_norm).clamp(max=15.0)  # tanh(15) ≈ 1.0\n",
    "    \n",
    "    # tanh(sqrt(c) * ||v||) * v / (sqrt(c) * ||v||)\n",
    "    return torch.tanh(tanh_arg) * v / (sqrt_c * v_norm + eps)\n",
    "\n",
    "\n",
    "def log_map_zero(y: torch.Tensor, c: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Logarithmic map to the origin in the Poincaré ball.\n",
    "    \n",
    "    Maps a point y on the Poincaré ball back to Euclidean tangent space at origin.\n",
    "    Formula: log_0(y) = arctanh(sqrt(c) * ||y||) * y / (sqrt(c) * ||y||)\n",
    "    \n",
    "    Args:\n",
    "        y: Point on Poincaré ball [*, D]\n",
    "        c: Curvature parameter (positive scalar)\n",
    "        eps: Small constant for numerical stability\n",
    "    Returns:\n",
    "        Euclidean vector [*, D]\n",
    "    \"\"\"\n",
    "    sqrt_c = torch.sqrt(c)\n",
    "    y_norm = y.norm(dim=-1, keepdim=True).clamp(min=eps)\n",
    "    \n",
    "    # Clamp to ensure arctanh input is strictly in (-1, 1)\n",
    "    # This is CRITICAL - arctanh(1) = inf, arctanh(>1) = NaN\n",
    "    y_norm_scaled = (sqrt_c * y_norm).clamp(min=eps, max=1.0 - eps)\n",
    "    \n",
    "    # arctanh(sqrt(c) * ||y||) * y / (sqrt(c) * ||y||)\n",
    "    return torch.arctanh(y_norm_scaled) * y / (sqrt_c * y_norm + eps)\n",
    "\n",
    "\n",
    "def project_to_ball(x: torch.Tensor, c: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Project points to be inside the Poincaré ball.\n",
    "    \n",
    "    Ensures ||x|| < 1/sqrt(c) by scaling down points that are outside.\n",
    "    \n",
    "    Args:\n",
    "        x: Points [*, D]\n",
    "        c: Curvature parameter\n",
    "        eps: Small margin from boundary\n",
    "    Returns:\n",
    "        Projected points inside the ball [*, D]\n",
    "    \"\"\"\n",
    "    max_norm = (1.0 - eps) / torch.sqrt(c)\n",
    "    x_norm = x.norm(dim=-1, keepdim=True).clamp(min=eps)\n",
    "    \n",
    "    # Scale down if outside ball\n",
    "    scale = torch.clamp(max_norm / x_norm, max=1.0)\n",
    "    return x * scale\n",
    "\n",
    "\n",
    "def mobius_add(x: torch.Tensor, y: torch.Tensor, c: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Möbius addition in the Poincaré ball.\n",
    "    \n",
    "    This is the hyperbolic analog of vector addition.\n",
    "    \n",
    "    Args:\n",
    "        x, y: Points on Poincaré ball [*, D]\n",
    "        c: Curvature parameter\n",
    "        eps: Numerical stability constant\n",
    "    Returns:\n",
    "        Result of Möbius addition [*, D]\n",
    "    \"\"\"\n",
    "    x_sq = (x * x).sum(dim=-1, keepdim=True)\n",
    "    y_sq = (y * y).sum(dim=-1, keepdim=True)\n",
    "    xy = (x * y).sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    num = (1 + 2 * c * xy + c * y_sq) * x + (1 - c * x_sq) * y\n",
    "    denom = (1 + 2 * c * xy + c * c * x_sq * y_sq).clamp(min=eps)\n",
    "    \n",
    "    result = num / denom\n",
    "    \n",
    "    # Project result back into ball\n",
    "    return project_to_ball(result, c, eps)\n",
    "\n",
    "\n",
    "class HyperbolicConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution in hyperbolic (Poincaré ball) space.\n",
    "    \n",
    "    The forward pass:\n",
    "    1. Normalize input features to bounded range\n",
    "    2. Map to Poincaré ball via exponential map\n",
    "    3. Perform convolution in tangent space\n",
    "    4. Map result back and normalize\n",
    "    \n",
    "    Hyperbolic geometry is well-suited for capturing hierarchical structures\n",
    "    in medical images (organ → region → tissue → boundary).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        curvature: float = 1.0,\n",
    "        learnable_curvature: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Curvature parameter (positive value, represents |c| in Poincaré ball)\n",
    "        if learnable_curvature:\n",
    "            # Initialize with log so softplus gives ~curvature\n",
    "            init_val = math.log(math.exp(curvature) - 1)  # inverse of softplus\n",
    "            self.curvature = nn.Parameter(torch.tensor(init_val))\n",
    "        else:\n",
    "            self.register_buffer('curvature', torch.tensor(curvature))\n",
    "        \n",
    "        # Input normalization to control feature magnitudes\n",
    "        self.input_norm = nn.GroupNorm(min(8, in_channels), in_channels)\n",
    "        \n",
    "        # Convolution in tangent space\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        \n",
    "        # Output normalization (more stable in hyperbolic setting)\n",
    "        self.output_norm = nn.GroupNorm(min(8, out_channels), out_channels)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        # Scale factor to keep features bounded before hyperbolic mapping\n",
    "        self.scale = nn.Parameter(torch.ones(1) * 0.1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [B, C_in, H, W]\n",
    "        Returns:\n",
    "            Output tensor [B, C_out, H, W]\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Ensure curvature is positive and bounded\n",
    "        c = F.softplus(self.curvature).clamp(min=0.1, max=10.0)\n",
    "        \n",
    "        # Normalize input to control magnitudes\n",
    "        x = self.input_norm(x)\n",
    "        \n",
    "        # Scale features to be small (important for hyperbolic stability)\n",
    "        x_scaled = x * torch.abs(self.scale)\n",
    "        \n",
    "        # Reshape for hyperbolic operations: [B, H, W, C]\n",
    "        x_bhwc = x_scaled.permute(0, 2, 3, 1).contiguous()\n",
    "        \n",
    "        # Map to Poincaré ball\n",
    "        x_hyp = exp_map_zero(x_bhwc, c)\n",
    "        \n",
    "        # Project to ensure we're inside the ball\n",
    "        x_hyp = project_to_ball(x_hyp, c)\n",
    "        \n",
    "        # Map back to tangent space for convolution\n",
    "        x_tangent = log_map_zero(x_hyp, c)\n",
    "        \n",
    "        # Back to conv format: [B, C, H, W]\n",
    "        x_tangent = x_tangent.permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        # Apply convolution in tangent space\n",
    "        out = self.conv(x_tangent)\n",
    "        \n",
    "        # Add bias\n",
    "        out = out + self.bias.view(1, -1, 1, 1)\n",
    "        \n",
    "        # Normalize and activate (stay in Euclidean space for stability)\n",
    "        out = self.output_norm(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d11bd",
   "metadata": {},
   "source": [
    "## 4. Temporal Attention\n",
    "\n",
    "Cross-frame attention mechanism that aggregates information from T=3 consecutive slices, focusing on the center frame. Uses the center slice as query and all slices as keys/values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ceaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-frame attention for aggregating temporal context from T consecutive slices.\n",
    "    \n",
    "    Uses the center frame as the query and all frames as keys/values.\n",
    "    This allows the model to leverage adjacent slice information while\n",
    "    ultimately producing a segmentation for the center slice.\n",
    "    \n",
    "    Input: [B, T, C, H, W] - T frames of features\n",
    "    Output: [B, C, H, W] - Temporally-attended center frame features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Query projection (for center frame only)\n",
    "        self.q_proj = nn.Conv2d(embed_dim, embed_dim, 1, bias=False)\n",
    "        \n",
    "        # Key and Value projections (for all frames)\n",
    "        self.k_proj = nn.Conv2d(embed_dim, embed_dim, 1, bias=False)\n",
    "        self.v_proj = nn.Conv2d(embed_dim, embed_dim, 1, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Conv2d(embed_dim, embed_dim, 1, bias=False)\n",
    "        \n",
    "        # Learnable temporal position embeddings\n",
    "        self.temporal_pos = nn.Parameter(torch.randn(1, 3, embed_dim, 1, 1) * 0.02)  # For T=3\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [B, T, C, H, W]\n",
    "        Returns:\n",
    "            Output tensor [B, C, H, W] (center frame with temporal context)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        \n",
    "        # Add temporal position embeddings\n",
    "        x = x + self.temporal_pos[:, :T]\n",
    "        \n",
    "        # Extract center frame for query\n",
    "        center_idx = T // 2  # For T=3, this is index 1\n",
    "        q_input = x[:, center_idx]  # [B, C, H, W]\n",
    "        \n",
    "        # Compute query from center frame\n",
    "        q = self.q_proj(q_input)  # [B, C, H, W]\n",
    "        \n",
    "        # Compute keys and values from all frames\n",
    "        # Reshape for batch processing: [B*T, C, H, W]\n",
    "        x_flat = x.view(B * T, C, H, W)\n",
    "        k = self.k_proj(x_flat)  # [B*T, C, H, W]\n",
    "        v = self.v_proj(x_flat)  # [B*T, C, H, W]\n",
    "        \n",
    "        # Reshape for attention computation\n",
    "        # Q: [B, num_heads, head_dim, H*W]\n",
    "        q = q.view(B, self.num_heads, self.head_dim, H * W)\n",
    "        \n",
    "        # K, V: [B, T, num_heads, head_dim, H*W]\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim, H * W)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim, H * W)\n",
    "        \n",
    "        # Temporal attention: each spatial position in center attends to same position across time\n",
    "        # Attention: [B, num_heads, H*W, T]\n",
    "        attn_logits = torch.einsum('bhdn,bthdm->bhnt', q, k) * self.scale\n",
    "        \n",
    "        # For each spatial position, only attend to same position across time\n",
    "        # Simplification: aggregate across time dimension\n",
    "        attn_logits = attn_logits.diagonal(dim1=-2, dim2=-1)  # [B, num_heads, min(H*W, T)]\n",
    "        \n",
    "        # Recompute: simpler version - spatial attention pooled across time\n",
    "        # Q: [B, num_heads, H*W, head_dim] K: [B, T, num_heads, H*W, head_dim]\n",
    "        q = q.permute(0, 1, 3, 2)  # [B, num_heads, H*W, head_dim]\n",
    "        k = k.permute(0, 2, 3, 4, 1)  # [B, num_heads, head_dim, H*W, T]\n",
    "        v = v.permute(0, 2, 3, 4, 1)  # [B, num_heads, head_dim, H*W, T]\n",
    "        \n",
    "        # Per-position temporal attention: for each spatial position, attend across T frames\n",
    "        # [B, num_heads, H*W, T]\n",
    "        attn = torch.einsum('bnsd,bndst->bnst', q, k) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Weighted sum: [B, num_heads, H*W, head_dim]\n",
    "        out = torch.einsum('bnst,bndst->bnsd', attn, v)\n",
    "        \n",
    "        # Reshape back: [B, C, H, W]\n",
    "        out = out.permute(0, 1, 3, 2).contiguous()  # [B, num_heads, head_dim, H*W]\n",
    "        out = out.view(B, C, H, W)\n",
    "        \n",
    "        # Output projection\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        # Residual connection with center frame\n",
    "        out = out + q_input\n",
    "        \n",
    "        # Layer norm (reshape for LN)\n",
    "        out = out.permute(0, 2, 3, 1)  # [B, H, W, C]\n",
    "        out = self.norm(out)\n",
    "        out = out.permute(0, 3, 1, 2)  # [B, C, H, W]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ea274",
   "metadata": {},
   "source": [
    "## 5. INR Branch (Implicit Neural Representation)\n",
    "\n",
    "Continuous coordinate-based refinement for precise boundary delineation:\n",
    "- **PositionalEncoding**: Fourier feature encoding of 2D coordinates\n",
    "- **INRBranch**: MLP that predicts per-pixel refinement logits from coordinates + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2976bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Fourier feature positional encoding for 2D coordinates.\n",
    "    \n",
    "    Maps coordinates from [-1, 1] to a higher-dimensional space using\n",
    "    sinusoidal functions at multiple frequencies. This allows the MLP\n",
    "    to learn high-frequency details (important for sharp boundaries).\n",
    "    \n",
    "    Based on: \"Fourier Features Let Networks Learn High Frequency Functions\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_frequencies: int = 10, include_input: bool = True):\n",
    "        super().__init__()\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.include_input = include_input\n",
    "        \n",
    "        # Frequency bands: 2^0, 2^1, ..., 2^(L-1)\n",
    "        freq_bands = 2.0 ** torch.linspace(0, num_frequencies - 1, num_frequencies)\n",
    "        self.register_buffer('freq_bands', freq_bands)\n",
    "        \n",
    "        # Output dimension: 2 (xy) * num_freq * 2 (sin/cos) + optional 2 (raw coords)\n",
    "        self.out_dim = 2 * num_frequencies * 2\n",
    "        if include_input:\n",
    "            self.out_dim += 2\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coords: Normalized coordinates [*, 2] in range [-1, 1]\n",
    "        Returns:\n",
    "            Encoded coordinates [*, out_dim]\n",
    "        \"\"\"\n",
    "        # coords: [..., 2]\n",
    "        # Scale by frequency bands: [..., 2, num_freq]\n",
    "        scaled = coords.unsqueeze(-1) * self.freq_bands * math.pi\n",
    "        \n",
    "        # Apply sin and cos: [..., 2, num_freq, 2]\n",
    "        encoded = torch.stack([torch.sin(scaled), torch.cos(scaled)], dim=-1)\n",
    "        \n",
    "        # Flatten: [..., 2 * num_freq * 2]\n",
    "        encoded = encoded.view(*coords.shape[:-1], -1)\n",
    "        \n",
    "        # Optionally include raw coordinates\n",
    "        if self.include_input:\n",
    "            encoded = torch.cat([coords, encoded], dim=-1)\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "\n",
    "def make_coord_grid(H: int, W: int, device: torch.device = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a grid of normalized 2D coordinates in [-1, 1].\n",
    "    \n",
    "    Args:\n",
    "        H, W: Grid dimensions\n",
    "        device: Target device\n",
    "    Returns:\n",
    "        Coordinate grid [H, W, 2]\n",
    "    \"\"\"\n",
    "    # Create normalized coordinates\n",
    "    y = torch.linspace(-1, 1, H, device=device)\n",
    "    x = torch.linspace(-1, 1, W, device=device)\n",
    "    \n",
    "    # Create meshgrid\n",
    "    yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "    \n",
    "    # Stack: [H, W, 2]\n",
    "    coords = torch.stack([xx, yy], dim=-1)\n",
    "    \n",
    "    return coords\n",
    "\n",
    "\n",
    "class INRBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Implicit Neural Representation branch for boundary refinement.\n",
    "    \n",
    "    Combines high-resolution coordinate encoding with image features\n",
    "    to produce continuous, resolution-independent refinement signals.\n",
    "    This helps recover fine boundary details lost in downsampling.\n",
    "    \n",
    "    Input: Features [B, C, H, W] + implicit coordinate grid\n",
    "    Output: Refinement logits [B, 1, H, W]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        num_frequencies: int = 10,\n",
    "        num_layers: int = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Positional encoding for coordinates\n",
    "        self.pos_encoder = PositionalEncoding(num_frequencies, include_input=True)\n",
    "        coord_dim = self.pos_encoder.out_dim\n",
    "        \n",
    "        # Feature projection (reduce channel dimension)\n",
    "        self.feature_proj = nn.Conv2d(feature_dim, hidden_dim // 2, 1)\n",
    "        \n",
    "        # MLP: coord_encoding + projected_features -> refinement logit\n",
    "        input_dim = coord_dim + hidden_dim // 2\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_dim if i == 0 else hidden_dim\n",
    "            out_dim = hidden_dim if i < num_layers - 1 else 1\n",
    "            \n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if i < num_layers - 1:\n",
    "                layers.append(nn.GELU())\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, features: torch.Tensor, coords: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Image features [B, C, H, W]\n",
    "            coords: Optional pre-computed coordinates [H, W, 2]. \n",
    "                    If None, generates grid automatically.\n",
    "        Returns:\n",
    "            Refinement logits [B, 1, H, W]\n",
    "        \"\"\"\n",
    "        B, C, H, W = features.shape\n",
    "        device = features.device\n",
    "        \n",
    "        # Generate coordinate grid if not provided\n",
    "        if coords is None:\n",
    "            coords = make_coord_grid(H, W, device)  # [H, W, 2]\n",
    "        \n",
    "        # Encode coordinates\n",
    "        coord_enc = self.pos_encoder(coords)  # [H, W, coord_dim]\n",
    "        \n",
    "        # Expand for batch: [B, H, W, coord_dim]\n",
    "        coord_enc = coord_enc.unsqueeze(0).expand(B, -1, -1, -1)\n",
    "        \n",
    "        # Project features and reshape: [B, H, W, hidden_dim//2]\n",
    "        feat_proj = self.feature_proj(features)  # [B, hidden_dim//2, H, W]\n",
    "        feat_proj = feat_proj.permute(0, 2, 3, 1)  # [B, H, W, hidden_dim//2]\n",
    "        \n",
    "        # Concatenate coordinates and features\n",
    "        combined = torch.cat([coord_enc, feat_proj], dim=-1)  # [B, H, W, input_dim]\n",
    "        \n",
    "        # MLP forward\n",
    "        out = self.mlp(combined)  # [B, H, W, 1]\n",
    "        \n",
    "        # Reshape to standard format: [B, 1, H, W]\n",
    "        out = out.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f49be6",
   "metadata": {},
   "source": [
    "## 6. Validation Tests\n",
    "\n",
    "Instantiate each module with dummy inputs to verify shapes and forward passes work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37467d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_modules():\n",
    "    \"\"\"\n",
    "    Test all implemented modules with dummy data to verify shapes.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HANS-Net Module Validation Tests\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nUsing device: {device}\\n\")\n",
    "    \n",
    "    # Test parameters\n",
    "    B, T, C, H, W = 2, 3, 1, 128, 128  # Batch, Time, Channels, Height, Width\n",
    "    base_ch = 32\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. Test WaveletDecomposition\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"-\" * 40)\n",
    "    print(\"1. WaveletDecomposition\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    wavelet = WaveletDecomposition().to(device)\n",
    "    x_in = torch.randn(B, C, H, W, device=device)\n",
    "    x_wav = wavelet(x_in)\n",
    "    \n",
    "    print(f\"   Input shape:  {list(x_in.shape)}\")\n",
    "    print(f\"   Output shape: {list(x_wav.shape)}\")\n",
    "    print(f\"   Expected:     [B={B}, C*4={C*4}, H/2={H//2}, W/2={W//2}]\")\n",
    "    assert x_wav.shape == (B, C * 4, H // 2, W // 2), \"Shape mismatch!\"\n",
    "    print(\"   ✓ PASSED\\n\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. Test SynapticPlasticity\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"-\" * 40)\n",
    "    print(\"2. SynapticPlasticity\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    plasticity = SynapticPlasticity(channels=base_ch).to(device)\n",
    "    x_in = torch.randn(B, base_ch, H, W, device=device)\n",
    "    x_out = plasticity(x_in)\n",
    "    \n",
    "    print(f\"   Input shape:  {list(x_in.shape)}\")\n",
    "    print(f\"   Output shape: {list(x_out.shape)}\")\n",
    "    print(f\"   Expected:     [B={B}, C={base_ch}, H={H}, W={W}]\")\n",
    "    assert x_out.shape == x_in.shape, \"Shape mismatch!\"\n",
    "    print(\"   ✓ PASSED\\n\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. Test PlasticConvBlock\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"-\" * 40)\n",
    "    print(\"3. PlasticConvBlock\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    plastic_conv = PlasticConvBlock(in_channels=base_ch, out_channels=base_ch * 2).to(device)\n",
    "    x_in = torch.randn(B, base_ch, H, W, device=device)\n",
    "    x_out = plastic_conv(x_in)\n",
    "    \n",
    "    print(f\"   Input shape:  {list(x_in.shape)}\")\n",
    "    print(f\"   Output shape: {list(x_out.shape)}\")\n",
    "    print(f\"   Expected:     [B={B}, C={base_ch*2}, H={H}, W={W}]\")\n",
    "    assert x_out.shape == (B, base_ch * 2, H, W), \"Shape mismatch!\"\n",
    "    print(\"   ✓ PASSED\\n\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. Test Hyperbolic Helpers\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"-\" * 40)\n",
    "    print(\"4. Hyperbolic Maps (exp_map_zero, log_map_zero)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    c = torch.tensor(1.0, device=device)\n",
    "    v = torch.randn(B, H, W, base_ch, device=device) * 0.1  # Small vectors\n",
    "    \n",
    "    # Exp map\n",
    "    v_hyp = exp_map_zero(v, c)\n",
    "    print(f\"   exp_map_zero input:  {list(v.shape)}\")\n",
    "    print(f\"   exp_map_zero output: {list(v_hyp.shape)}\")\n",
    "    \n",
    "    # Verify points are inside Poincaré ball (norm < 1)\n",
    "    norms = v_hyp.norm(dim=-1)\n",
    "    print(f\"   Max norm (should be < 1): {norms.max().item():.4f}\")\n",
    "    assert norms.max() < 1.0, \"Points outside Poincaré ball!\"\n",
    "    \n",
    "    # Log map (inverse)\n",
    "    v_back = log_map_zero(v_hyp, c)\n",
    "    print(f\"   log_map_zero output: {list(v_back.shape)}\")\n",
    "    \n",
    "    # Check round-trip reconstruction\n",
    "    reconstruction_error = (v - v_back).abs().max().item()\n",
    "    print(f\"   Round-trip error: {reconstruction_error:.6f}\")\n",
    "    assert reconstruction_error < 1e-4, \"Round-trip reconstruction failed!\"\n",
    "    print(\"   ✓ PASSED\\n\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5. Test HyperbolicConvBlock\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"-\" * 40)\n",
    "    print(\"5. HyperbolicConvBlock\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    hyp_conv = HyperbolicConvBlock(in_channels=base_ch, out_channels=base_ch * 2).to(device)\n",
    "    x_in = torch.randn(B, base_ch, H // 4, W // 4, device=device) * 0.1\n",
    "    x_out = hyp_conv(x_in)\n",
    "    \n",
    "    print(f\"   Input shape:  {list(x_in.shape)}\")\n",
    "    print(f\"   Output shape: {list(x_out.shape)}\")\n",
    "    print(f\"   Expected:     [B={B}, C={base_ch*2}, H={H//4}, W={W//4}]\")\n",
    "    assert x_out.shape == (B, base_ch * 2, H // 4, W // 4), \"Shape mismatch!\"\n",
    "    print(f\"   Learned curvature: {F.softplus(hyp_conv.curvature).item():.4f}\")\n",
    "    print(\"   ✓ PASSED\\n\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6. Test TemporalAttention\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"-\" * 40)\n",
    "    print(\"6. TemporalAttention\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    temp_attn = TemporalAttention(embed_dim=base_ch * 2, num_heads=4).to(device)\n",
    "    x_in = torch.randn(B, T, base_ch * 2, H // 2, W // 2, device=device)\n",
    "    x_out = temp_attn(x_in)\n",
    "    \n",
    "    print(f\"   Input shape:  {list(x_in.shape)} (B, T, C, H, W)\")\n",
    "    print(f\"   Output shape: {list(x_out.shape)} (B, C, H, W)\")\n",
    "    print(f\"   Expected:     [B={B}, C={base_ch*2}, H={H//2}, W={W//2}]\")\n",
    "    assert x_out.shape == (B, base_ch * 2, H // 2, W // 2), \"Shape mismatch!\"\n",
    "    print(\"   ✓ PASSED\\n\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7. Test INRBranch\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"-\" * 40)\n",
    "    print(\"7. INRBranch\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    inr = INRBranch(feature_dim=base_ch, hidden_dim=128, num_frequencies=10).to(device)\n",
    "    x_in = torch.randn(B, base_ch, H, W, device=device)\n",
    "    x_out = inr(x_in)\n",
    "    \n",
    "    print(f\"   Input shape:  {list(x_in.shape)}\")\n",
    "    print(f\"   Output shape: {list(x_out.shape)}\")\n",
    "    print(f\"   Expected:     [B={B}, 1, H={H}, W={W}]\")\n",
    "    assert x_out.shape == (B, 1, H, W), \"Shape mismatch!\"\n",
    "    print(\"   ✓ PASSED\\n\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Summary\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ALL TESTS PASSED! ✓\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Print parameter counts\n",
    "    print(\"\\nParameter counts:\")\n",
    "    modules = {\n",
    "        'WaveletDecomposition': wavelet,\n",
    "        'SynapticPlasticity': plasticity,\n",
    "        'PlasticConvBlock': plastic_conv,\n",
    "        'HyperbolicConvBlock': hyp_conv,\n",
    "        'TemporalAttention': temp_attn,\n",
    "        'INRBranch': inr\n",
    "    }\n",
    "    \n",
    "    total = 0\n",
    "    for name, module in modules.items():\n",
    "        params = sum(p.numel() for p in module.parameters())\n",
    "        total += params\n",
    "        print(f\"   {name}: {params:,} parameters\")\n",
    "    print(f\"   {'Total':}: {total:,} parameters\")\n",
    "\n",
    "\n",
    "# Run the tests\n",
    "test_all_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2470f80d",
   "metadata": {},
   "source": [
    "## 7. HANSNet - Complete U-Net Architecture\n",
    "\n",
    "The full HANS-Net model combining all components:\n",
    "- **Encoder**: Wavelet decomposition + PlasticConvBlocks with downsampling\n",
    "- **Temporal Attention**: Fuses T=3 slices at mid-level, outputs center slice features\n",
    "- **Bottleneck**: HyperbolicConvBlock for hierarchical feature learning\n",
    "- **Decoder**: Upsampling + skip connections from center slice encoder features\n",
    "- **INR Refinement**: Boundary refinement via implicit neural representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1eb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANSNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Hyperbolic Attention Network for Segmentation (HANS-Net).\n",
    "    \n",
    "    A U-Net style architecture for liver/tumor CT segmentation that combines:\n",
    "    - Wavelet decomposition for multi-frequency feature extraction\n",
    "    - Synaptic plasticity for adaptive feature learning\n",
    "    - Temporal attention to fuse information from adjacent CT slices\n",
    "    - Hyperbolic convolutions in the bottleneck for hierarchical representations\n",
    "    - INR branch for boundary refinement\n",
    "    \n",
    "    Input:  [B, T=3, 1, H, W] - 3 consecutive CT slices\n",
    "    Output: [B, 1, H, W]      - Segmentation logits for center slice\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_channels: int = 32, num_classes: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_channels = base_channels\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Channel progression: base -> 2x -> 4x -> 8x\n",
    "        c1 = base_channels       # 32\n",
    "        c2 = base_channels * 2   # 64\n",
    "        c3 = base_channels * 4   # 128\n",
    "        c4 = base_channels * 8   # 256\n",
    "        \n",
    "        # =====================================================================\n",
    "        # ENCODER (processes all T slices)\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Level 1: Wavelet decomposition + initial conv\n",
    "        # Input: [B*T, 1, H, W] -> Wavelet: [B*T, 4, H/2, W/2] -> Conv: [B*T, c1, H/2, W/2]\n",
    "        self.wavelet = WaveletDecomposition()\n",
    "        self.enc1 = PlasticConvBlock(4, c1, use_plasticity=True)  # 4 wavelet subbands -> c1\n",
    "        \n",
    "        # Level 2: Downsample + conv\n",
    "        # [B*T, c1, H/2, W/2] -> [B*T, c2, H/4, W/4]\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = PlasticConvBlock(c1, c2, use_plasticity=True)\n",
    "        \n",
    "        # Level 3: Downsample + conv (temporal attention applied here)\n",
    "        # [B*T, c2, H/4, W/4] -> [B*T, c3, H/8, W/8]\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = PlasticConvBlock(c2, c3, use_plasticity=True)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # TEMPORAL ATTENTION (fuses T slices -> center slice only)\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Input: [B, T, c3, H/8, W/8] -> Output: [B, c3, H/8, W/8]\n",
    "        self.temporal_attn = TemporalAttention(embed_dim=c3, num_heads=4)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # BOTTLENECK (hyperbolic convolution)\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Downsample and apply hyperbolic conv\n",
    "        # [B, c3, H/8, W/8] -> [B, c4, H/16, W/16]\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.bottleneck = HyperbolicConvBlock(c3, c4, curvature=1.0, learnable_curvature=True)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # DECODER (U-Net style with skip connections from center slice)\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Decoder Level 3: Upsample + concat skip + conv\n",
    "        # [B, c4, H/16, W/16] -> [B, c3, H/8, W/8]\n",
    "        self.up3 = nn.ConvTranspose2d(c4, c3, kernel_size=2, stride=2)\n",
    "        self.dec3 = PlasticConvBlock(c3 + c3, c3, use_plasticity=True, dropout_p=0.3)  # Skip from enc3 # GPT-4 EDIT\n",
    "        \n",
    "        # Decoder Level 2: Upsample + concat skip + conv\n",
    "        # [B, c3, H/8, W/8] -> [B, c2, H/4, W/4]\n",
    "        self.up2 = nn.ConvTranspose2d(c3, c2, kernel_size=2, stride=2)\n",
    "        self.dec2 = PlasticConvBlock(c2 + c2, c2, use_plasticity=True, dropout_p=0.3)  # Skip from enc2 # GPT-4 EDIT\n",
    "        \n",
    "        # Decoder Level 1: Upsample + concat skip + conv\n",
    "        # [B, c2, H/4, W/4] -> [B, c1, H/2, W/2]\n",
    "        self.up1 = nn.ConvTranspose2d(c2, c1, kernel_size=2, stride=2)\n",
    "        self.dec1 = PlasticConvBlock(c1 + c1, c1, use_plasticity=True, dropout_p=0.3)  # Skip from enc1 # GPT-4 EDIT\n",
    "        \n",
    "        # Final upsample to original resolution\n",
    "        # [B, c1, H/2, W/2] -> [B, c1, H, W]\n",
    "        self.final_up = nn.ConvTranspose2d(c1, c1, kernel_size=2, stride=2)\n",
    "        self.final_conv = PlasticConvBlock(c1, c1, use_plasticity=True, dropout_p=0.3)  # GPT-4 EDIT\n",
    "        \n",
    "        # =====================================================================\n",
    "        # OUTPUT HEADS\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Coarse segmentation head\n",
    "        self.seg_head = nn.Conv2d(c1, num_classes, kernel_size=1)\n",
    "        \n",
    "        # INR refinement branch\n",
    "        self.inr_branch = INRBranch(\n",
    "            feature_dim=c1,\n",
    "            hidden_dim=128,\n",
    "            num_frequencies=10,\n",
    "            num_layers=3\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of HANS-Net.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [B, T=3, 1, H, W] - 3 consecutive CT slices\n",
    "        Returns:\n",
    "            Segmentation logits [B, 1, H, W] for center slice\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        assert T == 3, f\"Expected T=3 slices, got T={T}\"\n",
    "        assert C == 1, f\"Expected C=1 channel, got C={C}\"\n",
    "        \n",
    "        # =====================================================================\n",
    "        # ENCODER - Process all T slices together\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Merge temporal and batch dimensions: [B*T, 1, H, W]\n",
    "        x_flat = x.view(B * T, C, H, W)\n",
    "        \n",
    "        # Level 1: Wavelet + PlasticConv\n",
    "        # [B*T, 1, H, W] -> [B*T, 4, H/2, W/2] -> [B*T, c1, H/2, W/2]\n",
    "        e1 = self.wavelet(x_flat)\n",
    "        e1 = self.enc1(e1)\n",
    "        \n",
    "        # Level 2: Pool + PlasticConv\n",
    "        # [B*T, c1, H/2, W/2] -> [B*T, c2, H/4, W/4]\n",
    "        e2 = self.pool1(e1)\n",
    "        e2 = self.enc2(e2)\n",
    "        \n",
    "        # Level 3: Pool + PlasticConv\n",
    "        # [B*T, c2, H/4, W/4] -> [B*T, c3, H/8, W/8]\n",
    "        e3 = self.pool2(e2)\n",
    "        e3 = self.enc3(e3)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # TEMPORAL ATTENTION - Fuse slices, extract center\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Reshape for temporal attention: [B, T, c3, H/8, W/8]\n",
    "        _, c3_ch, h3, w3 = e3.shape\n",
    "        e3_temporal = e3.view(B, T, c3_ch, h3, w3)\n",
    "        \n",
    "        # Apply temporal attention -> [B, c3, H/8, W/8]\n",
    "        f_center = self.temporal_attn(e3_temporal)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # BOTTLENECK - Hyperbolic convolution\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Downsample and apply hyperbolic conv\n",
    "        # [B, c3, H/8, W/8] -> [B, c4, H/16, W/16]\n",
    "        bottleneck = self.pool3(f_center)\n",
    "        bottleneck = self.bottleneck(bottleneck)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # EXTRACT CENTER SLICE FEATURES FOR SKIP CONNECTIONS\n",
    "        # =====================================================================\n",
    "        \n",
    "        center_idx = T // 2  # Index 1 for T=3\n",
    "        \n",
    "        # e1 skip: [B*T, c1, H/2, W/2] -> [B, c1, H/2, W/2]\n",
    "        _, c1_ch, h1, w1 = e1.shape\n",
    "        e1_center = e1.view(B, T, c1_ch, h1, w1)[:, center_idx]\n",
    "        \n",
    "        # e2 skip: [B*T, c2, H/4, W/4] -> [B, c2, H/4, W/4]\n",
    "        _, c2_ch, h2, w2 = e2.shape\n",
    "        e2_center = e2.view(B, T, c2_ch, h2, w2)[:, center_idx]\n",
    "        \n",
    "        # e3 is already fused via temporal attention, but we can use f_center\n",
    "        # For skip connection at level 3, use f_center (already [B, c3, H/8, W/8])\n",
    "        e3_center = f_center\n",
    "        \n",
    "        # =====================================================================\n",
    "        # DECODER - U-Net style upsampling with skip connections\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Decoder Level 3: Upsample + skip from e3_center\n",
    "        # [B, c4, H/16, W/16] -> [B, c3, H/8, W/8]\n",
    "        d3 = self.up3(bottleneck)\n",
    "        d3 = torch.cat([d3, e3_center], dim=1)  # [B, c3+c3, H/8, W/8]\n",
    "        d3 = self.dec3(d3)  # [B, c3, H/8, W/8]\n",
    "        \n",
    "        # Decoder Level 2: Upsample + skip from e2_center\n",
    "        # [B, c3, H/8, W/8] -> [B, c2, H/4, W/4]\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2_center], dim=1)  # [B, c2+c2, H/4, W/4]\n",
    "        d2 = self.dec2(d2)  # [B, c2, H/4, W/4]\n",
    "        \n",
    "        # Decoder Level 1: Upsample + skip from e1_center\n",
    "        # [B, c2, H/4, W/4] -> [B, c1, H/2, W/2]\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1_center], dim=1)  # [B, c1+c1, H/2, W/2]\n",
    "        d1 = self.dec1(d1)  # [B, c1, H/2, W/2]\n",
    "        \n",
    "        # Final upsample to original resolution\n",
    "        # [B, c1, H/2, W/2] -> [B, c1, H, W]\n",
    "        dec_out = self.final_up(d1)\n",
    "        dec_out = self.final_conv(dec_out)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # OUTPUT - Coarse logits + INR refinement\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Coarse segmentation logits\n",
    "        coarse_logits = self.seg_head(dec_out)  # [B, 1, H, W]\n",
    "        \n",
    "        # INR boundary refinement\n",
    "        refine_logits = self.inr_branch(dec_out)  # [B, 1, H, W]\n",
    "        \n",
    "        # Combine coarse and refined predictions\n",
    "        final_logits = coarse_logits + refine_logits\n",
    "        \n",
    "        return final_logits\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Test HANSNet\n",
    "# =============================================================================\n",
    "\n",
    "def test_hansnet():\n",
    "    \"\"\"Test the complete HANS-Net model.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HANS-Net Complete Model Test\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nUsing device: {device}\\n\")\n",
    "    \n",
    "    # Instantiate model\n",
    "    model = HANSNet(base_channels=32, num_classes=1).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print()\n",
    "    \n",
    "    # Create dummy input: [B=2, T=3, C=1, H=128, W=128]\n",
    "    x_dummy = torch.randn(2, 3, 1, 128, 128, device=device)\n",
    "    print(f\"Input shape:  {list(x_dummy.shape)} [B, T, C, H, W]\")\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x_dummy)\n",
    "    \n",
    "    print(f\"Output shape: {list(output.shape)} [B, num_classes, H, W]\")\n",
    "    print(f\"Expected:     [2, 1, 128, 128]\")\n",
    "    \n",
    "    assert output.shape == (2, 1, 128, 128), f\"Shape mismatch! Got {output.shape}\"\n",
    "    print(\"\\n✓ HANS-Net forward pass successful!\")\n",
    "    \n",
    "    # Test with different spatial sizes\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Testing with different spatial sizes...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for size in [64, 128, 256]:\n",
    "        x_test = torch.randn(1, 3, 1, size, size, device=device)\n",
    "        with torch.no_grad():\n",
    "            out_test = model(x_test)\n",
    "        print(f\"   Input [{1}, 3, 1, {size}, {size}] -> Output {list(out_test.shape)} ✓\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ALL TESTS PASSED!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Run the test\n",
    "hansnet_model = test_hansnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8aefb",
   "metadata": {},
   "source": [
    "### **DropOut Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HANSNet().cpu()\n",
    "model.train()  # VERY IMPORTANT — dropout active\n",
    "\n",
    "x = torch.randn(1, 3, 1, 128, 128)\n",
    "\n",
    "preds = []\n",
    "for _ in range(4):\n",
    "    out = model(x)\n",
    "    preds.append(out.detach().numpy())\n",
    "\n",
    "print(\"Outputs equal?\", \n",
    "      (preds[0] == preds[1]).all(),\n",
    "      (preds[1] == preds[2]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HANSNet().cpu()\n",
    "x = torch.randn(1, 3, 1, 128, 128)\n",
    "\n",
    "from torch.nn.functional import sigmoid\n",
    "\n",
    "model.train()  # activate dropout\n",
    "\n",
    "samples = []\n",
    "for _ in range(8):\n",
    "    samples.append(sigmoid(model(x)))\n",
    "\n",
    "samples = torch.stack(samples)  # [8,1,128,128]\n",
    "\n",
    "mean_map = samples.mean(dim=0)\n",
    "var_map  = samples.var(dim=0)\n",
    "\n",
    "print(\"Mean:\", mean_map.shape)\n",
    "print(\"Variance:\", var_map.shape)\n",
    "print(\"Mean-variance sum:\", var_map.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4addf",
   "metadata": {},
   "source": [
    "### **Sanity Check (For NaNs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HANSNet().cpu()\n",
    "model.train()\n",
    "\n",
    "x = torch.randn(1, 3, 1, 128, 128)\n",
    "\n",
    "logits = model(x)\n",
    "print(\"Any NaNs in logits?\", torch.isnan(logits).any().item())\n",
    "print(\"Any Infs in logits?\", torch.isinf(logits).any().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HANSNet().cpu()\n",
    "model.train()  # activate dropout\n",
    "\n",
    "# Simulate normalized CT-like input (0–1)\n",
    "x = torch.rand(1, 3, 1, 128, 128)\n",
    "\n",
    "samples = []\n",
    "for _ in range(8):\n",
    "    samples.append(torch.sigmoid(model(x)))\n",
    "\n",
    "samples = torch.stack(samples, dim=0)  # [8,1,1,128,128]\n",
    "mean_map = samples.mean(dim=0)\n",
    "var_map  = samples.var(dim=0)\n",
    "\n",
    "print(\"Mean:\", mean_map.shape)\n",
    "print(\"Variance:\", var_map.shape)\n",
    "print(\"Any NaNs in mean?\", torch.isnan(mean_map).any().item())\n",
    "print(\"Any NaNs in var?\", torch.isnan(var_map).any().item())\n",
    "print(\"Mean-variance sum:\", var_map.sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Comprehensive NaN/Stability Test After Fix\n",
    "# =============================================================================\n",
    "\n",
    "def test_stability():\n",
    "    \"\"\"Test model stability after fixing hyperbolic operations.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Stability Test - Checking for NaN/Inf values\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    # Test 1: HyperbolicConvBlock in isolation\n",
    "    print(\"\\n1. Testing HyperbolicConvBlock isolation...\")\n",
    "    hyp = HyperbolicConvBlock(128, 256).to(device)\n",
    "    x_hyp = torch.randn(2, 128, 8, 8, device=device)\n",
    "    \n",
    "    out_hyp = hyp(x_hyp)\n",
    "    print(f\"   Input range: [{x_hyp.min():.3f}, {x_hyp.max():.3f}]\")\n",
    "    print(f\"   Output range: [{out_hyp.min():.3f}, {out_hyp.max():.3f}]\")\n",
    "    print(f\"   NaN in output: {torch.isnan(out_hyp).any().item()}\")\n",
    "    print(f\"   Inf in output: {torch.isinf(out_hyp).any().item()}\")\n",
    "    assert not torch.isnan(out_hyp).any(), \"NaN in HyperbolicConvBlock!\"\n",
    "    print(\"   ✓ PASSED\")\n",
    "    \n",
    "    # Test 2: Full HANSNet forward pass\n",
    "    print(\"\\n2. Testing HANSNet single forward pass...\")\n",
    "    model = HANSNet(base_channels=32).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.randn(1, 3, 1, 128, 128, device=device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    \n",
    "    print(f\"   Input range: [{x.min():.3f}, {x.max():.3f}]\")\n",
    "    print(f\"   Output range: [{out.min():.3f}, {out.max():.3f}]\")\n",
    "    print(f\"   NaN in output: {torch.isnan(out).any().item()}\")\n",
    "    print(f\"   Inf in output: {torch.isinf(out).any().item()}\")\n",
    "    assert not torch.isnan(out).any(), \"NaN in HANSNet output!\"\n",
    "    print(\"   ✓ PASSED\")\n",
    "    \n",
    "    # Test 3: Multiple forward passes with dropout (train mode)\n",
    "    print(\"\\n3. Testing HANSNet with dropout (train mode)...\")\n",
    "    model.train()\n",
    "    \n",
    "    nan_found = False\n",
    "    for i in range(10):\n",
    "        x = torch.randn(1, 3, 1, 128, 128, device=device)\n",
    "        out = model(x)\n",
    "        if torch.isnan(out).any():\n",
    "            print(f\"   NaN found at iteration {i+1}!\")\n",
    "            nan_found = True\n",
    "            break\n",
    "    \n",
    "    if not nan_found:\n",
    "        print(\"   No NaN in 10 forward passes\")\n",
    "        print(\"   ✓ PASSED\")\n",
    "    \n",
    "    # Test 4: MC Dropout uncertainty estimation\n",
    "    print(\"\\n4. Testing MC Dropout uncertainty estimation...\")\n",
    "    model.train()\n",
    "    \n",
    "    x = torch.rand(1, 3, 1, 128, 128, device=device)  # Normalized input [0, 1]\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(8):\n",
    "        with torch.no_grad():\n",
    "            out = torch.sigmoid(model(x))\n",
    "        samples.append(out)\n",
    "        \n",
    "    samples = torch.stack(samples, dim=0)\n",
    "    mean_map = samples.mean(dim=0)\n",
    "    var_map = samples.var(dim=0)\n",
    "    \n",
    "    print(f\"   Mean range: [{mean_map.min():.4f}, {mean_map.max():.4f}]\")\n",
    "    print(f\"   Variance range: [{var_map.min():.6f}, {var_map.max():.6f}]\")\n",
    "    print(f\"   NaN in mean: {torch.isnan(mean_map).any().item()}\")\n",
    "    print(f\"   NaN in variance: {torch.isnan(var_map).any().item()}\")\n",
    "    print(f\"   Variance sum: {var_map.sum().item():.4f}\")\n",
    "    \n",
    "    assert not torch.isnan(mean_map).any(), \"NaN in mean!\"\n",
    "    assert not torch.isnan(var_map).any(), \"NaN in variance!\"\n",
    "    print(\"   ✓ PASSED\")\n",
    "    \n",
    "    # Test 5: Gradient flow check\n",
    "    print(\"\\n5. Testing gradient flow...\")\n",
    "    model.train()\n",
    "    x = torch.randn(1, 3, 1, 64, 64, device=device, requires_grad=False)\n",
    "    target = torch.randint(0, 2, (1, 1, 64, 64), device=device).float()\n",
    "    \n",
    "    out = model(x)\n",
    "    loss = F.binary_cross_entropy_with_logits(out, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    grad_ok = True\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(f\"   NaN gradient in {name}!\")\n",
    "                grad_ok = False\n",
    "                break\n",
    "    \n",
    "    if grad_ok:\n",
    "        print(\"   No NaN gradients found\")\n",
    "        print(\"   ✓ PASSED\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ALL STABILITY TESTS PASSED!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "test_stability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2e37a",
   "metadata": {},
   "source": [
    "## 8. Loss Functions & MC-Dropout Inference Utilities\n",
    "\n",
    "Helper functions for training and uncertainty-aware inference:\n",
    "- **dice_loss**: Soft Dice loss for segmentation training\n",
    "- **mc_predict**: Monte Carlo Dropout for uncertainty estimation\n",
    "- **visualize_uncertainty**: Simple visualization of predictions and uncertainty maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(probs: torch.Tensor, targets: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute soft Dice loss for binary segmentation.\n",
    "    \n",
    "    Dice coefficient measures the overlap between predicted probabilities\n",
    "    and ground truth masks. Dice loss = 1 - Dice coefficient.\n",
    "    \n",
    "    Args:\n",
    "        probs: Model output after sigmoid, shape [B, 1, H, W], values in [0, 1]\n",
    "        targets: Ground-truth binary mask, shape [B, 1, H, W], values in {0, 1}\n",
    "        eps: Small constant for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Scalar tensor: mean Dice loss over the batch\n",
    "    \n",
    "    Formula:\n",
    "        dice_coeff = (2 * intersection + eps) / (sum_probs + sum_targets + eps)\n",
    "        dice_loss = 1 - dice_coeff\n",
    "    \"\"\"\n",
    "    # Ensure same shape\n",
    "    assert probs.shape == targets.shape, f\"Shape mismatch: {probs.shape} vs {targets.shape}\"\n",
    "    \n",
    "    B = probs.shape[0]\n",
    "    \n",
    "    # Flatten spatial dimensions: [B, 1, H, W] -> [B, H*W]\n",
    "    probs_flat = probs.view(B, -1)\n",
    "    targets_flat = targets.view(B, -1)\n",
    "    \n",
    "    # Compute intersection and sums per sample\n",
    "    intersection = (probs_flat * targets_flat).sum(dim=1)  # [B]\n",
    "    sum_probs = probs_flat.sum(dim=1)                       # [B]\n",
    "    sum_targets = targets_flat.sum(dim=1)                   # [B]\n",
    "    \n",
    "    # Dice coefficient per sample\n",
    "    dice_coeff = (2.0 * intersection + eps) / (sum_probs + sum_targets + eps)  # [B]\n",
    "    \n",
    "    # Dice loss = 1 - dice_coeff, averaged over batch\n",
    "    loss = 1.0 - dice_coeff.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def combined_loss(logits: torch.Tensor, targets: torch.Tensor, \n",
    "                  bce_weight: float = 0.5, dice_weight: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Combined BCE + Dice loss for segmentation.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw model output (before sigmoid), shape [B, 1, H, W]\n",
    "        targets: Ground-truth binary mask, shape [B, 1, H, W]\n",
    "        bce_weight: Weight for BCE loss\n",
    "        dice_weight: Weight for Dice loss\n",
    "    \n",
    "    Returns:\n",
    "        Scalar tensor: weighted sum of BCE and Dice losses\n",
    "    \"\"\"\n",
    "    # BCE with logits (numerically stable)\n",
    "    bce = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "    \n",
    "    # Dice loss (needs probabilities)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    dice = dice_loss(probs, targets)\n",
    "    \n",
    "    return bce_weight * bce + dice_weight * dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa998c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_predict(model: nn.Module, x: torch.Tensor, n_samples: int = 10) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Monte Carlo Dropout prediction for uncertainty estimation.\n",
    "    \n",
    "    Performs multiple stochastic forward passes with dropout enabled\n",
    "    to estimate prediction uncertainty (epistemic uncertainty).\n",
    "    \n",
    "    Args:\n",
    "        model: HANSNet instance (or any model with Dropout layers)\n",
    "        x: Input tensor [B, T=3, 1, H, W] (3 consecutive CT slices)\n",
    "        n_samples: Number of stochastic forward passes (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        mean_probs: Mean predicted probabilities [B, 1, H, W]\n",
    "        var_probs: Variance of predictions (uncertainty) [B, 1, H, W]\n",
    "    \n",
    "    Usage:\n",
    "        >>> model = HANSNet(base_channels=32)\n",
    "        >>> x = torch.rand(1, 3, 1, 128, 128)\n",
    "        >>> mean_probs, var_probs = mc_predict(model, x, n_samples=10)\n",
    "        >>> # High variance regions indicate model uncertainty\n",
    "    \n",
    "    Note:\n",
    "        - Model is set to train() mode to activate Dropout layers\n",
    "        - Gradients are disabled during sampling for efficiency\n",
    "        - Higher n_samples gives more accurate uncertainty estimates but takes longer\n",
    "    \"\"\"\n",
    "    # Store original training state\n",
    "    was_training = model.training\n",
    "    \n",
    "    # Enable dropout by setting model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Collect samples\n",
    "    samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            samples.append(probs)\n",
    "    \n",
    "    # Stack samples: [n_samples, B, 1, H, W]\n",
    "    samples = torch.stack(samples, dim=0)\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean_probs = samples.mean(dim=0)  # [B, 1, H, W]\n",
    "    var_probs = samples.var(dim=0)    # [B, 1, H, W]\n",
    "    \n",
    "    # Restore original training state\n",
    "    if not was_training:\n",
    "        model.eval()\n",
    "    \n",
    "    return mean_probs, var_probs\n",
    "\n",
    "\n",
    "def mc_predict_with_samples(model: nn.Module, x: torch.Tensor, n_samples: int = 10) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Extended MC Dropout that also returns all individual samples.\n",
    "    \n",
    "    Useful for more detailed uncertainty analysis or custom aggregation.\n",
    "    \n",
    "    Args:\n",
    "        model: HANSNet instance\n",
    "        x: Input tensor [B, T=3, 1, H, W]\n",
    "        n_samples: Number of stochastic forward passes\n",
    "    \n",
    "    Returns:\n",
    "        mean_probs: Mean predicted probabilities [B, 1, H, W]\n",
    "        var_probs: Variance of predictions [B, 1, H, W]\n",
    "        all_samples: All prediction samples [n_samples, B, 1, H, W]\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            logits = model(x)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            samples.append(probs)\n",
    "    \n",
    "    samples = torch.stack(samples, dim=0)\n",
    "    mean_probs = samples.mean(dim=0)\n",
    "    var_probs = samples.var(dim=0)\n",
    "    \n",
    "    if not was_training:\n",
    "        model.eval()\n",
    "    \n",
    "    return mean_probs, var_probs, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_uncertainty(ct_slice: torch.Tensor, \n",
    "                          mean_probs: torch.Tensor, \n",
    "                          var_probs: torch.Tensor,\n",
    "                          threshold: float = 0.5,\n",
    "                          figsize: Tuple[int, int] = (15, 4)) -> None:\n",
    "    \"\"\"\n",
    "    Visualize CT slice, predicted segmentation, and uncertainty map.\n",
    "    \n",
    "    Creates a 1x4 subplot showing:\n",
    "    - Original CT slice (grayscale)\n",
    "    - Mean prediction probability (segmentation)\n",
    "    - Binary prediction (thresholded)\n",
    "    - Variance map (uncertainty heatmap)\n",
    "    \n",
    "    Args:\n",
    "        ct_slice: CT image, shape [H, W] or [1, H, W] or [1, 1, H, W]\n",
    "        mean_probs: Mean predicted probabilities [1, H, W] or [1, 1, H, W]\n",
    "        var_probs: Variance (uncertainty) [1, H, W] or [1, 1, H, W]\n",
    "        threshold: Threshold for binary prediction (default: 0.5)\n",
    "        figsize: Figure size (width, height)\n",
    "    \n",
    "    Note:\n",
    "        - All tensors are moved to CPU and converted to numpy\n",
    "        - Requires matplotlib (imported inside function)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Convert tensors to numpy and squeeze extra dimensions\n",
    "    def to_numpy(t):\n",
    "        if isinstance(t, torch.Tensor):\n",
    "            t = t.detach().cpu()\n",
    "        return t.squeeze().numpy()\n",
    "    \n",
    "    ct_np = to_numpy(ct_slice)\n",
    "    mean_np = to_numpy(mean_probs)\n",
    "    var_np = to_numpy(var_probs)\n",
    "    \n",
    "    # Create binary prediction\n",
    "    binary_pred = (mean_np > threshold).astype(float)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 4, figsize=figsize)\n",
    "    \n",
    "    # CT slice\n",
    "    axes[0].imshow(ct_np, cmap='gray')\n",
    "    axes[0].set_title('CT Slice (Input)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Mean prediction (probability)\n",
    "    im1 = axes[1].imshow(mean_np, cmap='hot', vmin=0, vmax=1)\n",
    "    axes[1].set_title('Mean Probability')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Binary prediction\n",
    "    axes[2].imshow(binary_pred, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[2].set_title(f'Binary Pred (τ={threshold})')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Variance (uncertainty)\n",
    "    im3 = axes[3].imshow(var_np, cmap='viridis')\n",
    "    axes[3].set_title('Uncertainty (Variance)')\n",
    "    axes[3].axis('off')\n",
    "    plt.colorbar(im3, ax=axes[3], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_mc_samples(ct_slice: torch.Tensor,\n",
    "                         samples: torch.Tensor,\n",
    "                         n_show: int = 4,\n",
    "                         figsize: Tuple[int, int] = (16, 4)) -> None:\n",
    "    \"\"\"\n",
    "    Visualize individual MC Dropout samples alongside the CT slice.\n",
    "    \n",
    "    Args:\n",
    "        ct_slice: CT image, shape [H, W] or [1, H, W]\n",
    "        samples: MC samples [n_samples, 1, 1, H, W] or [n_samples, 1, H, W]\n",
    "        n_show: Number of samples to display (default: 4)\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    def to_numpy(t):\n",
    "        if isinstance(t, torch.Tensor):\n",
    "            t = t.detach().cpu()\n",
    "        return t.squeeze().numpy()\n",
    "    \n",
    "    ct_np = to_numpy(ct_slice)\n",
    "    n_samples = min(n_show, samples.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_samples + 1, figsize=figsize)\n",
    "    \n",
    "    # CT slice\n",
    "    axes[0].imshow(ct_np, cmap='gray')\n",
    "    axes[0].set_title('CT Slice')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Individual samples\n",
    "    for i in range(n_samples):\n",
    "        sample_np = to_numpy(samples[i])\n",
    "        axes[i + 1].imshow(sample_np, cmap='hot', vmin=0, vmax=1)\n",
    "        axes[i + 1].set_title(f'Sample {i + 1}')\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d20800",
   "metadata": {},
   "source": [
    "### MC-Dropout Demo & Validation\n",
    "\n",
    "Test the MC-Dropout inference pipeline with a dummy input to verify:\n",
    "- Correct output shapes\n",
    "- No NaN/Inf values\n",
    "- Variance is non-zero (dropout is working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02279e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MC-Dropout Demo & Validation\n",
    "# =============================================================================\n",
    "\n",
    "def test_mc_dropout():\n",
    "    \"\"\"Test MC-Dropout inference pipeline.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MC-Dropout Inference Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nUsing device: {device}\\n\")\n",
    "    \n",
    "    # Create model\n",
    "    model = HANSNet(base_channels=32).to(device)\n",
    "    print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    # Create dummy CT-like input (normalized to [0, 1])\n",
    "    x = torch.rand(1, 3, 1, 128, 128, device=device)\n",
    "    print(f\"\\nInput shape: {list(x.shape)} [B, T, C, H, W]\")\n",
    "    print(f\"Input range: [{x.min():.3f}, {x.max():.3f}]\")\n",
    "    \n",
    "    # Run MC-Dropout prediction\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Running MC-Dropout with n_samples=8...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    n_samples = 8\n",
    "    mean_probs, var_probs = mc_predict(model, x, n_samples=n_samples)\n",
    "    \n",
    "    # Check shapes\n",
    "    print(f\"\\nMean probs shape: {list(mean_probs.shape)}\")\n",
    "    print(f\"Var probs shape:  {list(var_probs.shape)}\")\n",
    "    \n",
    "    expected_shape = (1, 1, 128, 128)\n",
    "    assert mean_probs.shape == expected_shape, f\"Mean shape mismatch: {mean_probs.shape}\"\n",
    "    assert var_probs.shape == expected_shape, f\"Var shape mismatch: {var_probs.shape}\"\n",
    "    print(\"✓ Shapes correct\")\n",
    "    \n",
    "    # Check for NaN/Inf\n",
    "    print(f\"\\nNaN in mean_probs: {torch.isnan(mean_probs).any().item()}\")\n",
    "    print(f\"NaN in var_probs:  {torch.isnan(var_probs).any().item()}\")\n",
    "    print(f\"Inf in mean_probs: {torch.isinf(mean_probs).any().item()}\")\n",
    "    print(f\"Inf in var_probs:  {torch.isinf(var_probs).any().item()}\")\n",
    "    \n",
    "    assert not torch.isnan(mean_probs).any(), \"NaN in mean_probs!\"\n",
    "    assert not torch.isnan(var_probs).any(), \"NaN in var_probs!\"\n",
    "    assert not torch.isinf(mean_probs).any(), \"Inf in mean_probs!\"\n",
    "    assert not torch.isinf(var_probs).any(), \"Inf in var_probs!\"\n",
    "    print(\"✓ No NaN/Inf values\")\n",
    "    \n",
    "    # Check value ranges\n",
    "    print(f\"\\nMean probs range: [{mean_probs.min():.4f}, {mean_probs.max():.4f}]\")\n",
    "    print(f\"Var probs range:  [{var_probs.min():.6f}, {var_probs.max():.6f}]\")\n",
    "    print(f\"Var probs sum:    {var_probs.sum().item():.4f}\")\n",
    "    \n",
    "    # Variance should be > 0 (dropout is introducing randomness)\n",
    "    if var_probs.sum() > 0:\n",
    "        print(\"✓ Variance > 0 (dropout is working)\")\n",
    "    else:\n",
    "        print(\"⚠ Warning: Variance is 0 - dropout might not be active\")\n",
    "    \n",
    "    # Test dice_loss\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Testing dice_loss...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create dummy target\n",
    "    target = (torch.rand(1, 1, 128, 128, device=device) > 0.7).float()\n",
    "    \n",
    "    loss = dice_loss(mean_probs, target)\n",
    "    print(f\"Dice loss: {loss.item():.4f}\")\n",
    "    assert not torch.isnan(loss), \"NaN in dice_loss!\"\n",
    "    assert 0 <= loss.item() <= 1, f\"Dice loss out of range: {loss.item()}\"\n",
    "    print(\"✓ Dice loss computed correctly\")\n",
    "    \n",
    "    # Test combined_loss\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Testing combined_loss...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Need logits for combined loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "    \n",
    "    comb_loss = combined_loss(logits, target)\n",
    "    print(f\"Combined loss (BCE + Dice): {comb_loss.item():.4f}\")\n",
    "    assert not torch.isnan(comb_loss), \"NaN in combined_loss!\"\n",
    "    print(\"✓ Combined loss computed correctly\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ALL MC-DROPOUT TESTS PASSED!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model, x, mean_probs, var_probs\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "mc_model, mc_input, mc_mean, mc_var = test_mc_dropout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
