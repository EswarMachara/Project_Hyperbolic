{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f73108e",
   "metadata": {},
   "source": [
    "# HANS-Net Standalone (Kaggle-ready)\n",
    "- Self-contained pipeline: data loader, model, training loop in this notebook.\n",
    "- Set `KAGGLE_DATASET_NAME` to your Kaggle dataset slug (folder under `/kaggle/input`).\n",
    "- Run all cells to validate folders, smoke-test the model, and optionally train.\n",
    "- Checkpoints write to `/kaggle/working/checkpoints` on Kaggle or `./checkpoints` locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Kaggle-aware configuration\n",
    "KAGGLE_DATASET_NAME = \"replace-with-your-dataset\"  # e.g., \"liver-tumor-segmentation\"\n",
    "KAGGLE_INPUT_ROOT = \"/kaggle/input\"\n",
    "KAGGLE_WORK_ROOT = \"/kaggle/working\"\n",
    "\n",
    "AUTO_KAGGLE = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is not None\n",
    "DEFAULT_DATA_ROOT = \"Dataset\"\n",
    "DATA_ROOT = DEFAULT_DATA_ROOT\n",
    "\n",
    "if AUTO_KAGGLE:\n",
    "    candidate = os.path.join(KAGGLE_INPUT_ROOT, KAGGLE_DATASET_NAME)\n",
    "    if os.path.isdir(candidate):\n",
    "        DATA_ROOT = candidate\n",
    "        print(f\"Detected Kaggle; using dataset at {DATA_ROOT}\")\n",
    "    else:\n",
    "        print(f\"[WARN] Kaggle detected but dataset folder not found: {candidate}\")\n",
    "else:\n",
    "    print(\"Kaggle not detected; using local paths.\")\n",
    "\n",
    "IMG_SIZE = (128, 128)\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 2\n",
    "LR = 1e-4\n",
    "CHECKPOINT_DIR = os.path.join(KAGGLE_WORK_ROOT if AUTO_KAGGLE else \".\", \"checkpoints\")\n",
    "RESUME = None  # path to checkpoint if resuming\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Dataset root: {DATA_ROOT}\")\n",
    "print(f\"Checkpoints: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd611d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset helpers and loaders\n",
    "def get_case_id_from_filename(fname: str) -> str:\n",
    "    basename = os.path.basename(fname)\n",
    "    name_no_ext = os.path.splitext(basename)[0]\n",
    "    if '_slice_' not in name_no_ext:\n",
    "        raise ValueError(f\"Invalid filename format: '{fname}'\")\n",
    "    parts = name_no_ext.split('_slice_')\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(f\"Invalid filename format: '{fname}'\")\n",
    "    case_id = parts[0]\n",
    "    if not re.match(r'^volume-\\d+$', case_id):\n",
    "        raise ValueError(f\"Invalid case ID format: '{case_id}'\")\n",
    "    return case_id\n",
    "\n",
    "def get_slice_index(fname: str) -> int:\n",
    "    basename = os.path.basename(fname)\n",
    "    name_no_ext = os.path.splitext(basename)[0]\n",
    "    if '_slice_' not in name_no_ext:\n",
    "        raise ValueError(f\"Invalid filename format: '{fname}'\")\n",
    "    parts = name_no_ext.split('_slice_')\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(f\"Invalid filename format: '{fname}'\")\n",
    "    slice_str = parts[1]\n",
    "    return int(slice_str)\n",
    "\n",
    "@dataclass\n",
    "class SliceMeta:\n",
    "    img_path: str\n",
    "    mask_path: str\n",
    "    case_id: str\n",
    "    slice_idx: int\n",
    "\n",
    "def build_slice_metadata(img_root: str, mask_root: str) -> List[SliceMeta]:\n",
    "    if not os.path.isdir(img_root):\n",
    "        raise FileNotFoundError(f\"Image root not found: {img_root}\")\n",
    "    if not os.path.isdir(mask_root):\n",
    "        raise FileNotFoundError(f\"Mask root not found: {mask_root}\")\n",
    "    img_root = os.path.abspath(img_root)\n",
    "    mask_root = os.path.abspath(mask_root)\n",
    "    img_files = [f for f in os.listdir(img_root) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    mask_files_set = set(f for f in os.listdir(mask_root) if f.lower().endswith(('.jpg', '.jpeg', '.png')))\n",
    "    metadata_list: List[SliceMeta] = []\n",
    "    for img_fname in img_files:\n",
    "        if img_fname not in mask_files_set:\n",
    "            raise ValueError(f\"No corresponding mask for {img_fname}\")\n",
    "        img_path = os.path.join(img_root, img_fname)\n",
    "        mask_path = os.path.join(mask_root, img_fname)\n",
    "        case_id = get_case_id_from_filename(img_fname)\n",
    "        slice_idx = get_slice_index(img_fname)\n",
    "        metadata_list.append(SliceMeta(img_path, mask_path, case_id, slice_idx))\n",
    "    return metadata_list\n",
    "\n",
    "class LITSSliceDataset(Dataset):\n",
    "    def __init__(self, img_root: str, mask_root: str, img_size: Tuple[int, int] = (128, 128)) -> None:\n",
    "        self.img_size = img_size\n",
    "        metadata_list = build_slice_metadata(img_root, mask_root)\n",
    "        case_to_slices: Dict[str, List[SliceMeta]] = {}\n",
    "        for meta in metadata_list:\n",
    "            case_to_slices.setdefault(meta.case_id, []).append(meta)\n",
    "        for cid in case_to_slices:\n",
    "            case_to_slices[cid].sort(key=lambda m: m.slice_idx)\n",
    "        self.case_to_slices = case_to_slices\n",
    "        self.samples: List[Tuple[str, int]] = []\n",
    "        for case_id, slices in self.case_to_slices.items():\n",
    "            for center_idx in range(len(slices)):\n",
    "                self.samples.append((case_id, center_idx))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _get_triplet_indices(self, num_slices: int, center_idx: int) -> Tuple[int, int, int]:\n",
    "        prev_idx = max(0, center_idx - 1)\n",
    "        next_idx = min(num_slices - 1, center_idx + 1)\n",
    "        return (prev_idx, center_idx, next_idx)\n",
    "\n",
    "    def _load_slice(self, path: str) -> torch.Tensor:\n",
    "        img = Image.open(path).convert('L')\n",
    "        img = img.resize((self.img_size[1], self.img_size[0]), resample=Image.BILINEAR)\n",
    "        return TF.to_tensor(img)\n",
    "\n",
    "    def _load_mask(self, path: str) -> torch.Tensor:\n",
    "        mask = Image.open(path).convert('L')\n",
    "        mask = mask.resize((self.img_size[1], self.img_size[0]), resample=Image.NEAREST)\n",
    "        mask_tensor = TF.to_tensor(mask)\n",
    "        return (mask_tensor > 0.5).float()\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        case_id, center_idx = self.samples[idx]\n",
    "        slices = self.case_to_slices[case_id]\n",
    "        i_prev, i_center, i_next = self._get_triplet_indices(len(slices), center_idx)\n",
    "        img_prev = self._load_slice(slices[i_prev].img_path)\n",
    "        img_center = self._load_slice(slices[i_center].img_path)\n",
    "        img_next = self._load_slice(slices[i_next].img_path)\n",
    "        imgs_3 = torch.stack([img_prev, img_center, img_next], dim=0)\n",
    "        mask_center = self._load_mask(slices[i_center].mask_path)\n",
    "        return imgs_3, mask_center\n",
    "\n",
    "def build_dataloaders(dataset_root: str, img_size: Tuple[int, int] = (128, 128), batch_size: int = 4, train_ratio: float = 0.8, num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:\n",
    "    img_root = os.path.join(dataset_root, \"train_images\", \"train_images\")\n",
    "    mask_root = os.path.join(dataset_root, \"train_masks\", \"train_masks\")\n",
    "    full_dataset = LITSSliceDataset(img_root=img_root, mask_root=mask_root, img_size=img_size)\n",
    "    n_total = len(full_dataset)\n",
    "    n_train = int(train_ratio * n_total)\n",
    "    n_val = n_total - n_train\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [n_train, n_val])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def demo_dataset_structure(dataset_root: str) -> None:\n",
    "    img_root = os.path.join(dataset_root, \"train_images\", \"train_images\")\n",
    "    mask_root = os.path.join(dataset_root, \"train_masks\", \"train_masks\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Dataset Structure Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Dataset root: {dataset_root}\")\n",
    "    print(f\"Image folder: {img_root}\")\n",
    "    print(f\"Mask folder:  {mask_root}\")\n",
    "    img_exists = os.path.isdir(img_root)\n",
    "    mask_exists = os.path.isdir(mask_root)\n",
    "    if not img_exists or not mask_exists:\n",
    "        print(\"Folders missing; cannot extract metadata.\")\n",
    "        return\n",
    "    metadata_list = build_slice_metadata(img_root, mask_root)\n",
    "    total_slices = len(metadata_list)\n",
    "    print(f\"Total slices: {total_slices}\")\n",
    "    case_to_slices: dict = {}\n",
    "    for meta in metadata_list:\n",
    "        case_to_slices.setdefault(meta.case_id, []).append(meta.slice_idx)\n",
    "    print(f\"Cases: {len(case_to_slices)}\")\n",
    "    case_ids = sorted(case_to_slices.keys())[:2]\n",
    "    for case_id in case_ids:\n",
    "        slice_indices = sorted(case_to_slices[case_id])\n",
    "        print(f\"  Case {case_id}: {len(slice_indices)} slices; first 6: {slice_indices[:6]}\")\n",
    "    print(\"Validation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANS-Net model (standalone)\n",
    "class WaveletDecomposition(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ll = torch.tensor([[1, 1], [1, 1]], dtype=torch.float32) / 2.0\n",
    "        lh = torch.tensor([[1, 1], [-1, -1]], dtype=torch.float32) / 2.0\n",
    "        hl = torch.tensor([[1, -1], [1, -1]], dtype=torch.float32) / 2.0\n",
    "        hh = torch.tensor([[1, -1], [-1, 1]], dtype=torch.float32) / 2.0\n",
    "        filters = torch.stack([ll, lh, hl, hh], dim=0).unsqueeze(1)\n",
    "        self.register_buffer('filters', filters)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        x_reshape = x.view(B * C, 1, H, W)\n",
    "        coeffs = F.conv2d(x_reshape, self.filters, stride=2, padding=0)\n",
    "        _, _, H_out, W_out = coeffs.shape\n",
    "        return coeffs.view(B, C * 4, H_out, W_out)\n",
    "\n",
    "class WaveletReconstruction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ll = torch.tensor([[1, 1], [1, 1]], dtype=torch.float32) / 2.0\n",
    "        lh = torch.tensor([[1, 1], [-1, -1]], dtype=torch.float32) / 2.0\n",
    "        hl = torch.tensor([[1, -1], [1, -1]], dtype=torch.float32) / 2.0\n",
    "        hh = torch.tensor([[1, -1], [-1, 1]], dtype=torch.float32) / 2.0\n",
    "        filters = torch.stack([ll, lh, hl, hh], dim=0).unsqueeze(0)\n",
    "        self.register_buffer('filters', filters)\n",
    "    def forward(self, coeffs: torch.Tensor) -> torch.Tensor:\n",
    "        B, C4, H, W = coeffs.shape\n",
    "        C = C4 // 4\n",
    "        coeffs = coeffs.view(B * C, 4, H, W)\n",
    "        x = F.conv_transpose2d(coeffs, self.filters, stride=2, padding=0)\n",
    "        return x.view(B, C, H * 2, W * 2)\n",
    "\n",
    "class SynapticPlasticity(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(channels))\n",
    "        self.threshold = nn.Parameter(torch.zeros(channels))\n",
    "        self.plasticity_rate = nn.Parameter(torch.ones(1) * 0.1)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, _, _ = x.shape\n",
    "        channel_mean = x.mean(dim=(2, 3))\n",
    "        modulation = torch.sigmoid(self.plasticity_rate * (channel_mean - self.threshold.view(1, -1)))\n",
    "        effective_scale = self.gain.view(1, -1, 1, 1) * (1 + modulation.view(B, C, 1, 1))\n",
    "        return x * effective_scale\n",
    "\n",
    "class PlasticConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, stride: int = 1, use_plasticity: bool = True, use_residual: bool = False, dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.use_plasticity = use_plasticity\n",
    "        self.use_residual = use_residual and (in_channels == out_channels) and (stride == 1)\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        if use_plasticity:\n",
    "            self.plasticity = SynapticPlasticity(out_channels)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout2d(dropout_p) if dropout_p > 0.0 else nn.Identity()\n",
    "        if self.use_residual and (in_channels != out_channels or stride != 1):\n",
    "            self.residual_proj = nn.Conv2d(in_channels, out_channels, 1, stride, bias=False)\n",
    "        else:\n",
    "            self.residual_proj = None\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.use_plasticity:\n",
    "            out = self.plasticity(out)\n",
    "        if self.use_residual:\n",
    "            if self.residual_proj is not None:\n",
    "                identity = self.residual_proj(identity)\n",
    "            out = out + identity\n",
    "        out = self.dropout(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "def exp_map_zero(v: torch.Tensor, c: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "    sqrt_c = torch.sqrt(c)\n",
    "    v_norm = v.norm(dim=-1, keepdim=True).clamp(min=eps)\n",
    "    tanh_arg = (sqrt_c * v_norm).clamp(max=15.0)\n",
    "    return torch.tanh(tanh_arg) * v / (sqrt_c * v_norm + eps)\n",
    "\n",
    "def log_map_zero(y: torch.Tensor, c: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "    sqrt_c = torch.sqrt(c)\n",
    "    y_norm = y.norm(dim=-1, keepdim=True).clamp(min=eps)\n",
    "    y_norm_scaled = (sqrt_c * y_norm).clamp(min=eps, max=1.0 - eps)\n",
    "    return torch.arctanh(y_norm_scaled) * y / (sqrt_c * y_norm + eps)\n",
    "\n",
    "def project_to_ball(x: torch.Tensor, c: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "    max_norm = (1.0 - eps) / torch.sqrt(c)\n",
    "    x_norm = x.norm(dim=-1, keepdim=True).clamp(min=eps)\n",
    "    scale = torch.clamp(max_norm / x_norm, max=1.0)\n",
    "    return x * scale\n",
    "\n",
    "def mobius_add(x: torch.Tensor, y: torch.Tensor, c: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "    x_sq = (x * x).sum(dim=-1, keepdim=True)\n",
    "    y_sq = (y * y).sum(dim=-1, keepdim=True)\n",
    "    xy = (x * y).sum(dim=-1, keepdim=True)\n",
    "    num = (1 + 2 * c * xy + c * y_sq) * x + (1 - c * x_sq) * y\n",
    "    denom = (1 + 2 * c * xy + c * c * x_sq * y_sq).clamp(min=eps)\n",
    "    return project_to_ball(num / denom, c, eps)\n",
    "\n",
    "class HyperbolicConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, curvature: float = 1.0, learnable_curvature: bool = True):\n",
    "        super().__init__()\n",
    "        if learnable_curvature:\n",
    "            init_val = math.log(math.exp(curvature) - 1)\n",
    "            self.curvature = nn.Parameter(torch.tensor(init_val))\n",
    "        else:\n",
    "            self.register_buffer('curvature', torch.tensor(curvature))\n",
    "        self.input_norm = nn.GroupNorm(min(8, in_channels), in_channels)\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        self.output_norm = nn.GroupNorm(min(8, out_channels), out_channels)\n",
    "        self.act = nn.GELU()\n",
    "        self.scale = nn.Parameter(torch.ones(1) * 0.1)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        c = F.softplus(self.curvature).clamp(min=0.1, max=10.0)\n",
    "        x = self.input_norm(x)\n",
    "        x_scaled = x * torch.abs(self.scale)\n",
    "        x_bhwc = x_scaled.permute(0, 2, 3, 1).contiguous()\n",
    "        x_hyp = exp_map_zero(x_bhwc, c)\n",
    "        x_hyp = project_to_ball(x_hyp, c)\n",
    "        x_tangent = log_map_zero(x_hyp, c)\n",
    "        x_tangent = x_tangent.permute(0, 3, 1, 2).contiguous()\n",
    "        out = self.conv(x_tangent)\n",
    "        out = out + self.bias.view(1, -1, 1, 1)\n",
    "        out = self.output_norm(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.q_proj = nn.Conv2d(embed_dim, embed_dim, 1, bias=False)\n",
    "        self.k_proj = nn.Conv2d(embed_dim, embed_dim, 1, bias=False)\n",
    "        self.v_proj = nn.Conv2d(embed_dim, embed_dim, 1, bias=False)\n",
    "        self.out_proj = nn.Conv2d(embed_dim, embed_dim, 1, bias=False)\n",
    "        self.temporal_pos = nn.Parameter(torch.randn(1, 3, embed_dim, 1, 1) * 0.02)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x + self.temporal_pos[:, :T]\n",
    "        center_idx = T // 2\n",
    "        q_input = x[:, center_idx]\n",
    "        q = self.q_proj(q_input)\n",
    "        x_flat = x.view(B * T, C, H, W)\n",
    "        k = self.k_proj(x_flat)\n",
    "        v = self.v_proj(x_flat)\n",
    "        q = q.view(B, self.num_heads, self.head_dim, H * W)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim, H * W)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim, H * W)\n",
    "        q = q.permute(0, 1, 3, 2)\n",
    "        k = k.permute(0, 2, 3, 4, 1)\n",
    "        v = v.permute(0, 2, 3, 4, 1)\n",
    "        attn = torch.einsum('bnsd,bndst->bnst', q, k) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.einsum('bnst,bndst->bnsd', attn, v)\n",
    "        out = out.permute(0, 1, 3, 2).contiguous()\n",
    "        out = out.view(B, C, H, W)\n",
    "        out = self.out_proj(out)\n",
    "        out = out + q_input\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        out = self.norm(out)\n",
    "        out = out.permute(0, 3, 1, 2)\n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_frequencies: int = 10, include_input: bool = True):\n",
    "        super().__init__()\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.include_input = include_input\n",
    "        freq_bands = 2.0 ** torch.linspace(0, num_frequencies - 1, num_frequencies)\n",
    "        self.register_buffer('freq_bands', freq_bands)\n",
    "        self.out_dim = 2 * num_frequencies * 2\n",
    "        if include_input:\n",
    "            self.out_dim += 2\n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        scaled = coords.unsqueeze(-1) * self.freq_bands * math.pi\n",
    "        encoded = torch.stack([torch.sin(scaled), torch.cos(scaled)], dim=-1)\n",
    "        encoded = encoded.view(*coords.shape[:-1], -1)\n",
    "        if self.include_input:\n",
    "            encoded = torch.cat([coords, encoded], dim=-1)\n",
    "        return encoded\n",
    "\n",
    "def make_coord_grid(H: int, W: int, device: torch.device = None) -> torch.Tensor:\n",
    "    y = torch.linspace(-1, 1, H, device=device)\n",
    "    x = torch.linspace(-1, 1, W, device=device)\n",
    "    yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "    return torch.stack([xx, yy], dim=-1)\n",
    "\n",
    "class INRBranch(nn.Module):\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int = 256, num_frequencies: int = 10, num_layers: int = 3):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(num_frequencies, include_input=True)\n",
    "        coord_dim = self.pos_encoder.out_dim\n",
    "        self.feature_proj = nn.Conv2d(feature_dim, hidden_dim // 2, 1)\n",
    "        input_dim = coord_dim + hidden_dim // 2\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_dim if i == 0 else hidden_dim\n",
    "            out_dim = hidden_dim if i < num_layers - 1 else 1\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if i < num_layers - 1:\n",
    "                layers.append(nn.GELU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    def forward(self, features: torch.Tensor, coords: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        B, C, H, W = features.shape\n",
    "        device = features.device\n",
    "        if coords is None:\n",
    "            coords = make_coord_grid(H, W, device)\n",
    "        coord_enc = self.pos_encoder(coords).unsqueeze(0).expand(B, -1, -1, -1)\n",
    "        feat_proj = self.feature_proj(features).permute(0, 2, 3, 1)\n",
    "        combined = torch.cat([coord_enc, feat_proj], dim=-1)\n",
    "        out = self.mlp(combined)\n",
    "        return out.permute(0, 3, 1, 2)\n",
    "\n",
    "class HANSNet(nn.Module):\n",
    "    def __init__(self, base_channels: int = 32, num_classes: int = 1):\n",
    "        super().__init__()\n",
    "        c1 = base_channels\n",
    "        c2 = base_channels * 2\n",
    "        c3 = base_channels * 4\n",
    "        c4 = base_channels * 8\n",
    "        self.wavelet = WaveletDecomposition()\n",
    "        self.enc1 = PlasticConvBlock(4, c1, use_plasticity=True)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = PlasticConvBlock(c1, c2, use_plasticity=True)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = PlasticConvBlock(c2, c3, use_plasticity=True)\n",
    "        self.temporal_attn = TemporalAttention(embed_dim=c3, num_heads=4)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.bottleneck = HyperbolicConvBlock(c3, c4, curvature=1.0, learnable_curvature=True)\n",
    "        self.up3 = nn.ConvTranspose2d(c4, c3, kernel_size=2, stride=2)\n",
    "        self.dec3 = PlasticConvBlock(c3 + c3, c3, use_plasticity=True, dropout_p=0.3)\n",
    "        self.up2 = nn.ConvTranspose2d(c3, c2, kernel_size=2, stride=2)\n",
    "        self.dec2 = PlasticConvBlock(c2 + c2, c2, use_plasticity=True, dropout_p=0.3)\n",
    "        self.up1 = nn.ConvTranspose2d(c2, c1, kernel_size=2, stride=2)\n",
    "        self.dec1 = PlasticConvBlock(c1 + c1, c1, use_plasticity=True, dropout_p=0.3)\n",
    "        self.final_up = nn.ConvTranspose2d(c1, c1, kernel_size=2, stride=2)\n",
    "        self.final_conv = PlasticConvBlock(c1, c1, use_plasticity=True, dropout_p=0.3)\n",
    "        self.seg_head = nn.Conv2d(c1, num_classes, kernel_size=1)\n",
    "        self.inr_branch = INRBranch(feature_dim=c1, hidden_dim=128, num_frequencies=10, num_layers=3)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C, H, W = x.shape\n",
    "        assert T == 3, f\"Expected T=3 slices, got {T}\"\n",
    "        assert C == 1, f\"Expected C=1 channel, got {C}\"\n",
    "        x_flat = x.view(B * T, C, H, W)\n",
    "        e1 = self.wavelet(x_flat)\n",
    "        e1 = self.enc1(e1)\n",
    "        e2 = self.pool1(e1)\n",
    "        e2 = self.enc2(e2)\n",
    "        e3 = self.pool2(e2)\n",
    "        e3 = self.enc3(e3)\n",
    "        _, c3_ch, h3, w3 = e3.shape\n",
    "        e3_temporal = e3.view(B, T, c3_ch, h3, w3)\n",
    "        f_center = self.temporal_attn(e3_temporal)\n",
    "        bottleneck = self.pool3(f_center)\n",
    "        bottleneck = self.bottleneck(bottleneck)\n",
    "        center_idx = T // 2\n",
    "        _, c1_ch, h1, w1 = e1.shape\n",
    "        e1_center = e1.view(B, T, c1_ch, h1, w1)[:, center_idx]\n",
    "        _, c2_ch, h2, w2 = e2.shape\n",
    "        e2_center = e2.view(B, T, c2_ch, h2, w2)[:, center_idx]\n",
    "        e3_center = f_center\n",
    "        d3 = self.up3(bottleneck)\n",
    "        d3 = torch.cat([d3, e3_center], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2_center], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1_center], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        dec_out = self.final_up(d1)\n",
    "        dec_out = self.final_conv(dec_out)\n",
    "        coarse_logits = self.seg_head(dec_out)\n",
    "        refine_logits = self.inr_branch(dec_out)\n",
    "        return coarse_logits + refine_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities\n",
    "def save_checkpoint(model, optimizer, epoch: int, path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    checkpoint = {\"model_state\": model.state_dict(), \"optim_state\": optimizer.state_dict(), \"epoch\": epoch}\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "def load_checkpoint(path: str, model, optimizer=None, device: str = \"cuda\") -> int:\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    if optimizer is not None and \"optim_state\" in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint[\"optim_state\"])\n",
    "    return checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "def dice_coeff(pred_probs: torch.Tensor, target_mask: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    pred_flat = pred_probs.view(pred_probs.size(0), -1)\n",
    "    target_flat = target_mask.view(target_mask.size(0), -1)\n",
    "    intersection = (pred_flat * target_flat).sum(dim=1)\n",
    "    union = pred_flat.sum(dim=1) + target_flat.sum(dim=1)\n",
    "    dice = (2 * intersection + eps) / (union + eps)\n",
    "    return dice.mean()\n",
    "\n",
    "def iou_score(pred_probs: torch.Tensor, target_mask: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    pred_flat = pred_probs.view(pred_probs.size(0), -1)\n",
    "    target_flat = target_mask.view(target_mask.size(0), -1)\n",
    "    intersection = (pred_flat * target_flat).sum(dim=1)\n",
    "    union = pred_flat.sum(dim=1) + target_flat.sum(dim=1) - intersection\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    return iou.mean()\n",
    "\n",
    "def dice_loss(pred_logits: torch.Tensor, target_mask: torch.Tensor) -> torch.Tensor:\n",
    "    pred_probs = torch.sigmoid(pred_logits)\n",
    "    return 1.0 - dice_coeff(pred_probs, target_mask)\n",
    "\n",
    "def combined_loss(pred_logits: torch.Tensor, target_mask: torch.Tensor) -> torch.Tensor:\n",
    "    bce = F.binary_cross_entropy_with_logits(pred_logits, target_mask)\n",
    "    dloss = dice_loss(pred_logits, target_mask)\n",
    "    return bce + dloss\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_dice = 0.0\n",
    "    total_iou = 0.0\n",
    "    for imgs_3, mask_center in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs_3 = imgs_3.to(device)\n",
    "        mask_center = mask_center.to(device)\n",
    "        pred = model(imgs_3)\n",
    "        loss = combined_loss(pred, mask_center)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred_probs = torch.sigmoid(pred)\n",
    "        total_dice += dice_coeff(pred_probs, mask_center).item()\n",
    "        total_iou += iou_score(pred_probs, mask_center).item()\n",
    "    n = len(loader)\n",
    "    return total_loss / n, total_dice / n, total_iou / n\n",
    "\n",
    "def validate_one_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_dice = 0.0\n",
    "    total_iou = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs_3, mask_center in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            imgs_3 = imgs_3.to(device)\n",
    "            mask_center = mask_center.to(device)\n",
    "            pred = model(imgs_3)\n",
    "            loss = combined_loss(pred, mask_center)\n",
    "            total_loss += loss.item()\n",
    "            pred_probs = torch.sigmoid(pred)\n",
    "            total_dice += dice_coeff(pred_probs, mask_center).item()\n",
    "            total_iou += iou_score(pred_probs, mask_center).item()\n",
    "    n = len(loader)\n",
    "    return total_loss / n, total_dice / n, total_iou / n\n",
    "\n",
    "def train_pipeline(dataset_root: str, epochs: int = 3, lr: float = 1e-4, batch_size: int = 4, device: str = \"cuda\", checkpoint_dir: str = \"checkpoints\", resume_path: str | None = None):\n",
    "    model = HANSNet().to(device)\n",
    "    train_loader, val_loader = build_dataloaders(dataset_root, batch_size=batch_size)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    start_epoch = 0\n",
    "    if resume_path is not None and os.path.isfile(resume_path):\n",
    "        print(f\"Resuming from checkpoint: {resume_path}\")\n",
    "        start_epoch = load_checkpoint(resume_path, model, optimizer, device) + 1\n",
    "    best_val_loss = float(\"inf\")\n",
    "    print(f\"Starting training for {epochs} epochs (from epoch {start_epoch + 1})...\")\n",
    "    print(f\"Train samples: {len(train_loader.dataset)}, Val samples: {len(val_loader.dataset)}\")\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        train_loss, train_dice, train_iou = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_dice, val_iou = validate_one_epoch(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | train_loss={train_loss:.4f} | train_dice={train_dice:.4f} | train_iou={train_iou:.4f} | val_loss={val_loss:.4f} | val_dice={val_dice:.4f} | val_iou={val_iou:.4f}\")\n",
    "        last_ckpt_path = os.path.join(checkpoint_dir, \"last.pt\")\n",
    "        save_checkpoint(model, optimizer, epoch, last_ckpt_path)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_ckpt_path = os.path.join(checkpoint_dir, \"best.pt\")\n",
    "            save_checkpoint(model, optimizer, epoch, best_ckpt_path)\n",
    "            print(f\"  [checkpoint] best  -> {best_ckpt_path} (val improved)\")\n",
    "    print(f\"Training complete. Best val_loss: {best_val_loss:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aba4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate folders, smoke test, and train (optional)\n",
    "IMG_ROOT = os.path.join(DATA_ROOT, \"train_images\", \"train_images\")\n",
    "MASK_ROOT = os.path.join(DATA_ROOT, \"train_masks\", \"train_masks\")\n",
    "data_available = os.path.isdir(IMG_ROOT) and os.path.isdir(MASK_ROOT)\n",
    "\n",
    "print(f\"Image folder exists: {data_available and os.path.isdir(IMG_ROOT)} -> {IMG_ROOT}\")\n",
    "print(f\"Mask folder exists:  {data_available and os.path.isdir(MASK_ROOT)} -> {MASK_ROOT}\")\n",
    "\n",
    "if data_available:\n",
    "    demo_dataset_structure(DATA_ROOT)\n",
    "else:\n",
    "    print(\"[WARN] Dataset folders missing; training will be skipped.\")\n",
    "\n",
    "def smoke_test(model_cls, device=DEVICE, img_size=IMG_SIZE):\n",
    "    model = model_cls().to(device)\n",
    "    x = torch.randn(1, 3, 1, img_size[0], img_size[1], device=device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    print(f\"Input: {tuple(x.shape)}, Output: {tuple(out.shape)}\")\n",
    "    return model\n",
    "\n",
    "_ = smoke_test(HANSNet)\n",
    "\n",
    "if not data_available:\n",
    "    print(\"Dataset not found; skipping training. Set KAGGLE_DATASET_NAME or DATA_ROOT and rerun.\")\n",
    "else:\n",
    "    _ = train_pipeline(dataset_root=DATA_ROOT, epochs=EPOCHS, lr=LR, batch_size=BATCH_SIZE, device=DEVICE, checkpoint_dir=CHECKPOINT_DIR, resume_path=RESUME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2048bcd",
   "metadata": {},
   "source": [
    "### **Simple visual Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba78920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visual evaluation sampling directly from masks with foreground\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ckpt_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n",
    "if not os.path.isfile(ckpt_path):\n",
    "    alt = os.path.join(CHECKPOINT_DIR, \"last.pt\")\n",
    "    ckpt_path = alt if os.path.isfile(alt) else None\n",
    "\n",
    "def _load_slice(path: str, img_size=IMG_SIZE):\n",
    "    img = Image.open(path).convert(\"L\")\n",
    "    img = img.resize((img_size[1], img_size[0]), resample=Image.BILINEAR)\n",
    "    return TF.to_tensor(img)\n",
    "\n",
    "def _load_mask(path: str, img_size=IMG_SIZE):\n",
    "    mask = Image.open(path).convert(\"L\")\n",
    "    mask = mask.resize((img_size[1], img_size[0]), resample=Image.NEAREST)\n",
    "    mask_tensor = TF.to_tensor(mask)\n",
    "    return (mask_tensor > 0.5).float()\n",
    "\n",
    "if ckpt_path is None:\n",
    "    print(\"No checkpoint found in CHECKPOINT_DIR; run training first.\")\n",
    "else:\n",
    "    print(f\"Loading checkpoint: {ckpt_path}\")\n",
    "    model = HANSNet().to(DEVICE)\n",
    "    ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    img_root = os.path.join(DATA_ROOT, \"train_images\", \"train_images\")\n",
    "    mask_root = os.path.join(DATA_ROOT, \"train_masks\", \"train_masks\")\n",
    "    if not (os.path.isdir(img_root) and os.path.isdir(mask_root)):\n",
    "        print(\"Image/mask folders missing; cannot sample.\")\n",
    "    else:\n",
    "        metadata_list = build_slice_metadata(img_root, mask_root)\n",
    "        case_to_slices = {}\n",
    "        for meta in metadata_list:\n",
    "            case_to_slices.setdefault(meta.case_id, []).append(meta)\n",
    "        for cid in case_to_slices:\n",
    "            case_to_slices[cid].sort(key=lambda m: m.slice_idx)\n",
    "\n",
    "        # collect slices whose mask has foreground\n",
    "        pool = []\n",
    "        for case_id, slices in case_to_slices.items():\n",
    "            for idx, meta in enumerate(slices):\n",
    "                arr = np.array(Image.open(meta.mask_path).convert(\"L\"))\n",
    "                if np.count_nonzero(arr) > 0:\n",
    "                    pool.append((case_id, idx, meta))\n",
    "\n",
    "        num_samples = 8\n",
    "        if len(pool) == 0:\n",
    "            print(\"No non-empty masks found in dataset; cannot plot.\")\n",
    "        else:\n",
    "            chosen = random.sample(pool, k=min(num_samples, len(pool)))\n",
    "            plt.figure(figsize=(12, 4 * len(chosen)))\n",
    "            for plot_idx, (case_id, center_idx, meta) in enumerate(chosen):\n",
    "                slices = case_to_slices[case_id]\n",
    "                prev_idx = max(0, center_idx - 1)\n",
    "                next_idx = min(len(slices) - 1, center_idx + 1)\n",
    "                img_prev = _load_slice(slices[prev_idx].img_path)\n",
    "                img_center = _load_slice(slices[center_idx].img_path)\n",
    "                img_next = _load_slice(slices[next_idx].img_path)\n",
    "                imgs_3 = torch.stack([img_prev, img_center, img_next], dim=0)\n",
    "                mask_center = _load_mask(slices[center_idx].mask_path)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred_logits = model(imgs_3.unsqueeze(0).to(DEVICE))\n",
    "                    pred_probs = torch.sigmoid(pred_logits).detach().cpu().squeeze(0).squeeze(0)\n",
    "\n",
    "                img_show = imgs_3[1].cpu().squeeze(0)\n",
    "                mask_np = mask_center.cpu().squeeze(0)\n",
    "                pred_np = (pred_probs > 0.5).float()\n",
    "\n",
    "                ax1 = plt.subplot(len(chosen), 3, plot_idx * 3 + 1)\n",
    "                ax1.imshow(img_show.numpy(), cmap=\"gray\")\n",
    "                ax1.set_title(f\"Image {plot_idx + 1}\")\n",
    "                ax1.axis(\"off\")\n",
    "\n",
    "                ax2 = plt.subplot(len(chosen), 3, plot_idx * 3 + 2)\n",
    "                ax2.imshow(mask_np.numpy(), cmap=\"gray\")\n",
    "                ax2.set_title(\"Ground Truth (non-zero)\")\n",
    "                ax2.axis(\"off\")\n",
    "\n",
    "                ax3 = plt.subplot(len(chosen), 3, plot_idx * 3 + 3)\n",
    "                ax3.imshow(pred_np.numpy(), cmap=\"gray\")\n",
    "                ax3.set_title(\"Prediction\")\n",
    "                ax3.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f41a53",
   "metadata": {},
   "source": [
    "### **Masks check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask coverage summary: counts black vs non-black masks\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "mask_root = Path(os.path.join(DATA_ROOT, \"train_masks\", \"train_masks\"))\n",
    "if not mask_root.is_dir():\n",
    "    print(f\"Mask folder not found: {mask_root}\")\n",
    "else:\n",
    "    mask_files = [p for p in mask_root.iterdir() if p.suffix.lower() in [\".png\", \".jpg\", \".jpeg\"]]\n",
    "    total = len(mask_files)\n",
    "    zero_count = 0\n",
    "    nonzero_count = 0\n",
    "    for p in tqdm(mask_files, desc=\"Scanning masks\", leave=False):\n",
    "        arr = np.array(Image.open(p).convert(\"L\"))\n",
    "        if np.count_nonzero(arr) == 0:\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            nonzero_count += 1\n",
    "    print(f\"Total masks:    {total}\")\n",
    "    print(f\"All-zero masks: {zero_count}\")\n",
    "    print(f\"Non-zero masks: {nonzero_count}\")\n",
    "    if total > 0:\n",
    "        print(f\"Percent zero:   {zero_count / total * 100:.2f}%\")\n",
    "        print(f\"Percent nonzero:{nonzero_count / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559c207",
   "metadata": {},
   "source": [
    "### **Summary of Today Training (11-12-2025)**\n",
    "- Ran the model for upto 5 Epochs with Learning Rate: 1e-4, Batch size: 64 and got unrealistically good train results\n",
    "- Found zero foreground containing masks in the Validation split\n",
    "- This is the main reason for getting Dice Score 1 for the Val split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd7ac8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
