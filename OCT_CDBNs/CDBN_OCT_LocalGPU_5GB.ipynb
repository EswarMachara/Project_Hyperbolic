{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad04f5c",
   "metadata": {},
   "source": [
    "# üñ•Ô∏è Local GPU (24 GB) + 5.5 GB Dataset Optimized Version\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° This notebook is optimized for:\n",
    "- **Local GPU:** 24 GB VRAM (RTX 3090/4090, A5000, etc.)\n",
    "- **Dataset Size:** ~5.5 GB (streaming-safe loading)\n",
    "- **Memory Safety:** No full dataset caching, mini-batch processing\n",
    "\n",
    "## üîß Key Optimizations:\n",
    "| Feature | Setting | Reason |\n",
    "|---------|---------|--------|\n",
    "| RBM Batch Size | 32 | Larger batches for CDBN, uses ~8-12 GB |\n",
    "| FC-RBM Batch Size | 64 | Lighter memory footprint |\n",
    "| Classifier Batch Size | 128 | Most memory-efficient stage |\n",
    "| Feature Caching | Disk-backed | Avoids RAM overflow for 5.5GB dataset |\n",
    "| NUM_WORKERS | 4 | Parallel data loading |\n",
    "| PERSISTENT_WORKERS | True | Reduces DataLoader overhead |\n",
    "\n",
    "## ‚ö†Ô∏è Critical Differences from Kaggle Version:\n",
    "1. **No full latent caching** ‚Äî Features extracted on-the-fly or saved to disk\n",
    "2. **Periodic GPU cache clearing** ‚Äî Prevents fragmentation\n",
    "3. **Stage-specific batch sizes** ‚Äî Optimized per training phase\n",
    "4. **Resume capability** ‚Äî Checkpoints saved after each epoch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e8249",
   "metadata": {},
   "source": [
    "# Convolutional Deep Belief Network (CDBN) for Eye OCT Image Classification\n",
    "\n",
    "This notebook implements a CDBN pipeline for multi-class classification of Eye OCT (Optical Coherence Tomography) images using PyTorch.\n",
    "\n",
    "**Notebook Structure:**\n",
    "1. Environment Setup & Imports\n",
    "2. Central Configuration\n",
    "3. Dataset Loading\n",
    "4. Dataset Sanity Checks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3986882",
   "metadata": {},
   "source": [
    "# üîß Local GPU Configuration (24 GB + 5.5 GB Dataset)\n",
    "\n",
    "This cell configures the notebook for optimal performance on a local high-end GPU.\n",
    "\n",
    "**Optimizations for large dataset:**\n",
    "- GPU memory management (95% allocation cap)\n",
    "- CUDA optimizations for RTX/Quadro GPUs\n",
    "- Streaming-friendly data loading\n",
    "- Stage-specific batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOCAL GPU ENVIRONMENT CONFIGURATION (24 GB + 5.5 GB Dataset)\n",
    "# =============================================================================\n",
    "# Optimized for local high-end GPUs with large datasets\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# STEP A: GPU Detection and Validation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOCAL GPU CONFIGURATION (24 GB + 5.5 GB Dataset)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    gpu_memory = gpu_props.total_memory / 1e9\n",
    "    print(f\"GPU Name: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Multi-Processor Count: {gpu_props.multi_processor_count}\")\n",
    "    print(f\"Compute Capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No GPU detected!\")\n",
    "    print(\"   This notebook requires a GPU for efficient training.\")\n",
    "\n",
    "# Assert CUDA is available (will stop execution if not)\n",
    "assert cuda_available, \"‚ùå CRITICAL: CUDA is required but not available.\"\n",
    "print(\"‚úì GPU assertion passed\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP C: CUDA Performance Optimizations (Local GPU)\n",
    "# =============================================================================\n",
    "\n",
    "# Allow cuDNN to auto-tune for best algorithm (CRITICAL for large batches)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Non-deterministic for performance (set to True only for debugging)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Reserve 95% of GPU memory to prevent allocation failures during spikes\n",
    "# This leaves ~1.2 GB buffer on a 24 GB GPU\n",
    "torch.cuda.set_per_process_memory_fraction(0.95, device=0)\n",
    "\n",
    "print(\"\\nCUDA Optimizations:\")\n",
    "print(f\"  cudnn.deterministic: {torch.backends.cudnn.deterministic}\")\n",
    "print(f\"  cudnn.benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "print(f\"  Memory fraction: 95% (leaves ~{gpu_memory * 0.05:.1f} GB buffer)\")\n",
    "\n",
    "# =============================================================================\n",
    "# GPU Info Cell (STEP C)\n",
    "# =============================================================================\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        free = total - reserved\n",
    "        print(f\"GPU Memory ‚Äî Allocated: {allocated:.2f} GB | Reserved: {reserved:.2f} GB | Free: {free:.2f} GB\")\n",
    "        return allocated, reserved, free\n",
    "    return 0, 0, 0\n",
    "\n",
    "print(\"\\nInitial GPU Memory Status:\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# =============================================================================\n",
    "# Global Debug Flag (STEP H: Large Dataset Visualization Policy)\n",
    "# =============================================================================\n",
    "# DEBUG=True enables visualizations BUT limited to:\n",
    "# - Only ONE batch (not full dataset)\n",
    "# - Only FIRST and LAST epochs\n",
    "# This prevents memory spikes from large visualizations\n",
    "\n",
    "DEBUG = True  # Enabled for local development, but with safeguards\n",
    "\n",
    "print(f\"\\nDebug Mode: {DEBUG}\")\n",
    "print(\"  ‚Üí Visualizations enabled but LIMITED:\")\n",
    "print(\"  ‚Üí Only 1 batch visualized per stage\")\n",
    "print(\"  ‚Üí Only first & last epoch plots\")\n",
    "\n",
    "# =============================================================================\n",
    "# Local Output Directory (STEP I)\n",
    "# =============================================================================\n",
    "# All outputs saved locally with checkpoint support\n",
    "\n",
    "SAVE_DIR = \"./outputs_local_5gb\"\n",
    "\n",
    "# Create output subdirectories\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_DIR, \"models\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_DIR, \"plots\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_DIR, \"logs\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_DIR, \"latent_cache\"), exist_ok=True)  # For disk-backed features\n",
    "\n",
    "print(f\"\\nOutput Directory: {SAVE_DIR}\")\n",
    "print(\"  ‚Üí models/       - Model checkpoints\")\n",
    "print(\"  ‚Üí plots/        - Training plots\")\n",
    "print(\"  ‚Üí logs/         - Training logs\")\n",
    "print(\"  ‚Üí latent_cache/ - Disk-backed latent features (5.5 GB dataset safety)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì LOCAL GPU CONFIGURATION COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRE-FLIGHT VALIDATION: Estimate Memory Requirements\n",
    "# =============================================================================\n",
    "# This cell estimates GPU memory requirements BEFORE training starts\n",
    "# and warns if memory is likely to exceed available capacity.\n",
    "\n",
    "def estimate_memory_requirements():\n",
    "    \"\"\"\n",
    "    Estimate GPU memory requirements for CDBN training.\n",
    "    \n",
    "    Returns estimated memory in GB and prints warnings if too high.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PRE-FLIGHT VALIDATION: Memory Estimation\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Constants for estimation\n",
    "    IMAGE_SIZE = (128, 128)\n",
    "    BYTES_PER_FLOAT = 4  # float32\n",
    "    \n",
    "    # Batch sizes\n",
    "    rbm_batch = 32      # RBM_BATCH_SIZE\n",
    "    fc_batch = 64       # FC_RBM_BATCH_SIZE\n",
    "    clf_batch = 128     # CLASSIFIER_BATCH_SIZE\n",
    "    \n",
    "    # Architecture sizes\n",
    "    conv1_filters = 32\n",
    "    conv1_kernel = 7\n",
    "    conv2_filters = 64\n",
    "    conv2_kernel = 5\n",
    "    fc_hidden = 256\n",
    "    \n",
    "    # Calculate output sizes\n",
    "    h1_size = (IMAGE_SIZE[0] - conv1_kernel + 1, IMAGE_SIZE[1] - conv1_kernel + 1)  # 122x122\n",
    "    pool_size = (h1_size[0] // 2, h1_size[1] // 2)  # 61x61\n",
    "    h2_size = (pool_size[0] - conv2_kernel + 1, pool_size[1] - conv2_kernel + 1)  # 57x57\n",
    "    flat_size = conv2_filters * h2_size[0] * h2_size[1]  # 64 * 57 * 57\n",
    "    \n",
    "    # Memory per batch (in bytes)\n",
    "    # Conv-RBM-1 training batch\n",
    "    input_mem = rbm_batch * 1 * IMAGE_SIZE[0] * IMAGE_SIZE[1] * BYTES_PER_FLOAT\n",
    "    h1_mem = rbm_batch * conv1_filters * h1_size[0] * h1_size[1] * BYTES_PER_FLOAT\n",
    "    conv1_weights = conv1_filters * 1 * conv1_kernel * conv1_kernel * BYTES_PER_FLOAT\n",
    "    conv1_velocity = conv1_weights  # Momentum velocity tensor\n",
    "    \n",
    "    # Conv-RBM-2 training batch\n",
    "    h1_pooled_mem = rbm_batch * conv1_filters * pool_size[0] * pool_size[1] * BYTES_PER_FLOAT\n",
    "    h2_mem = rbm_batch * conv2_filters * h2_size[0] * h2_size[1] * BYTES_PER_FLOAT\n",
    "    conv2_weights = conv2_filters * conv1_filters * conv2_kernel * conv2_kernel * BYTES_PER_FLOAT\n",
    "    conv2_velocity = conv2_weights\n",
    "    \n",
    "    # FC-RBM training\n",
    "    flat_mem = fc_batch * flat_size * BYTES_PER_FLOAT\n",
    "    fc_weights = flat_size * fc_hidden * BYTES_PER_FLOAT  # This is BIG!\n",
    "    fc_velocity = fc_weights\n",
    "    \n",
    "    # Peak memory estimates (in GB)\n",
    "    conv1_peak = (input_mem + h1_mem + conv1_weights + conv1_velocity) / (1024**3)\n",
    "    conv2_peak = (h1_pooled_mem + h2_mem + conv2_weights + conv2_velocity) / (1024**3)\n",
    "    fc_peak = (flat_mem + fc_weights + fc_velocity) / (1024**3)\n",
    "    \n",
    "    # PyTorch overhead (fragmentation, cudnn workspace, etc.)\n",
    "    overhead_factor = 1.5\n",
    "    total_estimated = max(conv1_peak, conv2_peak, fc_peak) * overhead_factor\n",
    "    \n",
    "    print(f\"\\nEstimated Peak GPU Memory per Stage:\")\n",
    "    print(f\"  Conv-RBM-1: ~{conv1_peak:.2f} GB (batch={rbm_batch})\")\n",
    "    print(f\"  Conv-RBM-2: ~{conv2_peak:.2f} GB (batch={rbm_batch})\")\n",
    "    print(f\"  FC-RBM:     ~{fc_peak:.2f} GB (batch={fc_batch})\")\n",
    "    print(f\"\\nFC-RBM Weights: {flat_size:,} √ó {fc_hidden} = {flat_size * fc_hidden:,} parameters\")\n",
    "    print(f\"FC-RBM Weight Size: {fc_weights / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Check against available GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        gpu_available = (torch.cuda.get_device_properties(0).total_memory - \n",
    "                        torch.cuda.memory_allocated()) / (1024**3)\n",
    "        \n",
    "        print(f\"\\nGPU Memory Status:\")\n",
    "        print(f\"  Total GPU Memory:     {gpu_total:.2f} GB\")\n",
    "        print(f\"  Available (current):  {gpu_available:.2f} GB\")\n",
    "        print(f\"  Estimated Peak (√ó{overhead_factor}): {total_estimated:.2f} GB\")\n",
    "        \n",
    "        # Safety check\n",
    "        if total_estimated > gpu_total * 0.95:\n",
    "            print(f\"\\n‚ö†Ô∏è WARNING: Estimated memory ({total_estimated:.1f} GB) may exceed \")\n",
    "            print(f\"   available GPU memory ({gpu_total:.1f} GB)!\")\n",
    "            print(f\"   Consider reducing batch sizes or using gradient checkpointing.\")\n",
    "        elif total_estimated > gpu_total * 0.75:\n",
    "            print(f\"\\n‚ö° CAUTION: Memory usage will be high ({total_estimated:.1f} GB / {gpu_total:.1f} GB)\")\n",
    "            print(f\"   Monitor with print_gpu_memory() during training.\")\n",
    "        else:\n",
    "            print(f\"\\n‚úì Memory estimates look safe for 24 GB GPU\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No CUDA GPU detected - running on CPU\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    return total_estimated\n",
    "\n",
    "# Run pre-flight validation\n",
    "estimated_peak_memory = estimate_memory_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5d9e83",
   "metadata": {},
   "source": [
    "## STEP 1 ‚Äî Environment Setup & Imports\n",
    "\n",
    "Import only the necessary libraries and configure the computing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43eecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: Environment Setup & Imports\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Core deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Vision utilities\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Numerical and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Set Global Random Seeds for Reproducibility\n",
    "# -----------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set CUDA seeds if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)  # For multi-GPU setups\n",
    "    # NOTE: cudnn settings are configured in the Kaggle Configuration cell above\n",
    "    # deterministic=False and benchmark=True for Kaggle performance\n",
    "    # These settings override the default reproducibility settings for speed\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Device Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch Version    : {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")\n",
    "print(f\"NumPy Version      : {np.__version__}\")\n",
    "print(f\"Random Seed        : {SEED}\")\n",
    "print(f\"Device             : {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name           : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version       : {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory         : {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"GPU                : Not available, using CPU\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f1666",
   "metadata": {},
   "source": [
    "## STEP 2 ‚Äî Central Configuration\n",
    "\n",
    "All hyperparameters and paths are centralized here for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: Central Configuration (Local GPU + 5.5 GB Dataset)\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Central configuration class for the CDBN OCT Classification pipeline.\n",
    "    \n",
    "    LOCAL GPU (24 GB) + 5.5 GB DATASET OPTIMIZATIONS:\n",
    "    -------------------------------------------------\n",
    "    - Stage-specific batch sizes (STEP D)\n",
    "    - Streaming-friendly DataLoader settings (STEP B)\n",
    "    - No full-dataset caching (STEP F)\n",
    "    - Disk-backed feature storage\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Dataset Paths (STEP B: Local Dataset Path)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TODO: Replace with your actual local dataset path\n",
    "    # Example: \"C:/Datasets/OCT2017\" or \"/home/user/datasets/OCT\"\n",
    "    DATASET_ROOT = \"C:/path/to/your/OCT/dataset\"  # <-- UPDATE THIS PATH\n",
    "    \n",
    "    TRAIN_DIR = os.path.join(DATASET_ROOT, \"train\")\n",
    "    VAL_DIR = os.path.join(DATASET_ROOT, \"val\")\n",
    "    TEST_DIR = os.path.join(DATASET_ROOT, \"test\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Output Directory (Local saving with checkpoints)\n",
    "    # -------------------------------------------------------------------------\n",
    "    OUTPUT_DIR = SAVE_DIR  # Uses ./outputs_local_5gb from config cell\n",
    "    LATENT_CACHE_DIR = os.path.join(SAVE_DIR, \"latent_cache\")  # Disk-backed features\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Image Parameters\n",
    "    # -------------------------------------------------------------------------\n",
    "    IMAGE_SIZE = (128, 128)      # (height, width) - resize all images to this\n",
    "    NUM_CHANNELS = 1              # Grayscale images\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP D: Stage-Specific Batch Sizes (VERY IMPORTANT)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # RBMs need smaller batches due to:\n",
    "    # 1. unfold() operation creates large intermediate tensors\n",
    "    # 2. Gibbs sampling stores multiple states in memory\n",
    "    # 3. Momentum buffers consume GPU memory\n",
    "    #\n",
    "    # Classifier can use larger batches (no Gibbs sampling)\n",
    "    \n",
    "    RBM_BATCH_SIZE = 32        # Conv-RBM-1 & Conv-RBM-2 (uses ~10-12 GB)\n",
    "    FC_RBM_BATCH_SIZE = 64     # FC-RBM (lighter memory footprint)\n",
    "    CLASSIFIER_BATCH_SIZE = 128  # Classifier training (most efficient)\n",
    "    \n",
    "    # Default batch size (for dataset loading)\n",
    "    BATCH_SIZE = RBM_BATCH_SIZE\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # DataLoader Parameters (STEP B: Streaming-Friendly Settings)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NUM_WORKERS=4: Local machine has more CPU cores than Kaggle\n",
    "    # Parallel data loading prevents GPU idle time\n",
    "    NUM_WORKERS = 4\n",
    "    \n",
    "    # PIN_MEMORY=True: Speeds up host-to-GPU transfers\n",
    "    # Pre-pins memory on CPU for faster CUDA copies\n",
    "    PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "    \n",
    "    # PERSISTENT_WORKERS=True: Keeps DataLoader workers alive between epochs\n",
    "    # Reduces worker spawn overhead for large datasets (~5.5 GB)\n",
    "    PERSISTENT_WORKERS = True\n",
    "    \n",
    "    # PREFETCH_FACTOR=2: Pre-loads 2 batches per worker\n",
    "    # Ensures GPU never waits for data (streaming safety)\n",
    "    PREFETCH_FACTOR = 2\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Device Configuration\n",
    "    # -------------------------------------------------------------------------\n",
    "    DEVICE = device\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Number of Classes (will be inferred from dataset)\n",
    "    # -------------------------------------------------------------------------\n",
    "    NUM_CLASSES = None  # To be set after loading dataset\n",
    "    CLASS_NAMES = None  # To be set after loading dataset\n",
    "\n",
    "\n",
    "# Create global config instance\n",
    "config = Config()\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"CENTRAL CONFIGURATION (Local GPU + 5.5 GB Dataset)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset Root       : {config.DATASET_ROOT}\")\n",
    "print(f\"Output Directory   : {config.OUTPUT_DIR}\")\n",
    "print(f\"Latent Cache       : {config.LATENT_CACHE_DIR}\")\n",
    "print(f\"Image Size         : {config.IMAGE_SIZE}\")\n",
    "print(f\"Number of Channels : {config.NUM_CHANNELS}\")\n",
    "print(f\"\\nStage-Specific Batch Sizes (STEP D):\")\n",
    "print(f\"  RBM Batch Size       : {config.RBM_BATCH_SIZE}\")\n",
    "print(f\"  FC-RBM Batch Size    : {config.FC_RBM_BATCH_SIZE}\")\n",
    "print(f\"  Classifier Batch Size: {config.CLASSIFIER_BATCH_SIZE}\")\n",
    "print(f\"\\nDataLoader Settings (Streaming-Safe):\")\n",
    "print(f\"  Num Workers        : {config.NUM_WORKERS}\")\n",
    "print(f\"  Pin Memory         : {config.PIN_MEMORY}\")\n",
    "print(f\"  Persistent Workers : {config.PERSISTENT_WORKERS}\")\n",
    "print(f\"  Prefetch Factor    : {config.PREFETCH_FACTOR}\")\n",
    "print(f\"\\nDevice             : {config.DEVICE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP B: Dataset Path Sanity Check\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOCAL DATASET SANITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDataset Root: {config.DATASET_ROOT}\")\n",
    "if os.path.exists(config.DATASET_ROOT):\n",
    "    # Calculate dataset size\n",
    "    total_size = 0\n",
    "    file_count = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(config.DATASET_ROOT):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "            file_count += 1\n",
    "    \n",
    "    print(f\"‚úì Dataset found!\")\n",
    "    print(f\"  Total files: {file_count:,}\")\n",
    "    print(f\"  Total size: {total_size / 1e9:.2f} GB\")\n",
    "    \n",
    "    # List subdirectories\n",
    "    for item in os.listdir(config.DATASET_ROOT):\n",
    "        item_path = os.path.join(config.DATASET_ROOT, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            subdir_files = sum([len(files) for _, _, files in os.walk(item_path)])\n",
    "            print(f\"  üìÅ {item}/ ({subdir_files:,} files)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Dataset path does not exist!\")\n",
    "    print(\"   Please update Config.DATASET_ROOT with your local path\")\n",
    "    print(f\"   Expected: {config.DATASET_ROOT}\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88693b",
   "metadata": {},
   "source": [
    "## STEP 3 ‚Äî Dataset Loading\n",
    "\n",
    "Load the OCT dataset using `torchvision.datasets.ImageFolder`:\n",
    "- Convert images to grayscale (1 channel)\n",
    "- Resize to configured dimensions\n",
    "- Normalize pixel values to [0, 1] range\n",
    "- Create DataLoaders for all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb88a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: Dataset Loading\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Define Image Transformations\n",
    "# -----------------------------------------------------------------------------\n",
    "# Transform pipeline:\n",
    "#   1. Convert to grayscale (handles both RGB and grayscale inputs)\n",
    "#   2. Resize to target dimensions\n",
    "#   3. Convert to tensor (scales to [0, 1] automatically for uint8 images)\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=config.NUM_CHANNELS),  # Ensure grayscale\n",
    "    transforms.Resize(config.IMAGE_SIZE),                           # Resize to (128, 128)\n",
    "    transforms.ToTensor(),                                          # Convert to tensor, scale to [0, 1]\n",
    "])\n",
    "\n",
    "print(\"Data Transforms Pipeline:\")\n",
    "print(data_transforms)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe41bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Load Datasets using ImageFolder\n",
    "# -----------------------------------------------------------------------------\n",
    "# ImageFolder expects: root/class_name/image.ext\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Training dataset\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=config.TRAIN_DIR,\n",
    "    transform=data_transforms\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    root=config.VAL_DIR,\n",
    "    transform=data_transforms\n",
    ")\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=config.TEST_DIR,\n",
    "    transform=data_transforms\n",
    ")\n",
    "\n",
    "print(\"‚úì All datasets loaded successfully!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Create DataLoaders (Streaming-Safe for 5.5 GB Dataset)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Using persistent_workers and prefetch_factor for efficient streaming\n",
    "# This avoids loading the full dataset into RAM at once\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,                    # Shuffle training data\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY,\n",
    "    drop_last=False,\n",
    "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False,\n",
    "    prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,                   # No shuffle for validation\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY,\n",
    "    drop_last=False,\n",
    "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False,\n",
    "    prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,                   # No shuffle for test\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY,\n",
    "    drop_last=False,\n",
    "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False,\n",
    "    prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None\n",
    ")\n",
    "\n",
    "print(\"‚úì DataLoaders created successfully!\")\n",
    "print(f\"  - Persistent workers: {config.PERSISTENT_WORKERS}\")\n",
    "print(f\"  - Prefetch factor: {config.PREFETCH_FACTOR}\")\n",
    "print(f\"  - Pin memory: {config.PIN_MEMORY}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Update Configuration with Inferred Values\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Infer number of classes from the training dataset\n",
    "config.NUM_CLASSES = len(train_dataset.classes)\n",
    "config.CLASS_NAMES = train_dataset.classes\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET CONFIGURATION UPDATED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of Classes  : {config.NUM_CLASSES}\")\n",
    "print(f\"Class Names        : {config.CLASS_NAMES}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ed42a",
   "metadata": {},
   "source": [
    "## STEP 4 ‚Äî Dataset Sanity Checks\n",
    "\n",
    "Verify the dataset loading was successful:\n",
    "- Print sample counts per split\n",
    "- Display class mappings\n",
    "- Inspect batch tensor shapes\n",
    "- Visualize sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc34aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: Dataset Sanity Checks\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4.1 Print Number of Samples per Split\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Split':<12} {'Samples':>10} {'Batches':>10}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'Train':<12} {len(train_dataset):>10} {len(train_loader):>10}\")\n",
    "print(f\"{'Validation':<12} {len(val_dataset):>10} {len(val_loader):>10}\")\n",
    "print(f\"{'Test':<12} {len(test_dataset):>10} {len(test_loader):>10}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'Total':<12} {len(train_dataset) + len(val_dataset) + len(test_dataset):>10}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4.2 Print Class Names and Class-to-Index Mapping\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nClass Names: {train_dataset.classes}\")\n",
    "print(f\"\\nClass-to-Index Mapping:\")\n",
    "for class_name, class_idx in train_dataset.class_to_idx.items():\n",
    "    print(f\"  {class_name:<20} -> {class_idx}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count samples per class in training set\n",
    "print(\"\\nSamples per Class (Training Set):\")\n",
    "print(\"-\" * 35)\n",
    "class_counts = {}\n",
    "for _, label in train_dataset.samples:\n",
    "    class_name = train_dataset.classes[label]\n",
    "    class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "\n",
    "for class_name, count in sorted(class_counts.items()):\n",
    "    print(f\"  {class_name:<20} : {count:>6} samples\")\n",
    "print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86db31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4.3 Fetch One Batch and Print Tensor Shape\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BATCH INSPECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get one batch from training loader\n",
    "sample_batch_images, sample_batch_labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nBatch Images Tensor:\")\n",
    "print(f\"  Shape      : {sample_batch_images.shape}\")\n",
    "print(f\"  Data Type  : {sample_batch_images.dtype}\")\n",
    "print(f\"  Min Value  : {sample_batch_images.min().item():.4f}\")\n",
    "print(f\"  Max Value  : {sample_batch_images.max().item():.4f}\")\n",
    "print(f\"  Mean Value : {sample_batch_images.mean().item():.4f}\")\n",
    "\n",
    "print(f\"\\nBatch Labels Tensor:\")\n",
    "print(f\"  Shape      : {sample_batch_labels.shape}\")\n",
    "print(f\"  Data Type  : {sample_batch_labels.dtype}\")\n",
    "print(f\"  Labels     : {sample_batch_labels.tolist()}\")\n",
    "\n",
    "# Verify expected shape\n",
    "expected_shape = (config.BATCH_SIZE, config.NUM_CHANNELS, config.IMAGE_SIZE[0], config.IMAGE_SIZE[1])\n",
    "actual_shape = tuple(sample_batch_images.shape)\n",
    "\n",
    "print(f\"\\nShape Verification:\")\n",
    "print(f\"  Expected   : {expected_shape}\")\n",
    "print(f\"  Actual     : {actual_shape}\")\n",
    "\n",
    "# Note: Last batch might be smaller if dataset size is not divisible by batch size\n",
    "if actual_shape[1:] == expected_shape[1:]:\n",
    "    print(\"  Status     : ‚úì Shape matches expected dimensions!\")\n",
    "else:\n",
    "    print(\"  Status     : ‚úó Shape mismatch detected!\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa5d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4.4 Visualize One Batch of Images with Labels\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def visualize_batch(images, labels, class_names, num_images=16, figsize=(12, 12)):\n",
    "    \"\"\"\n",
    "    Visualize a batch of images in a grid layout.\n",
    "    \n",
    "    Args:\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        labels: Tensor of class indices\n",
    "        class_names: List of class name strings\n",
    "        num_images: Number of images to display (default: 16)\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    # Limit to available images and maximum display count\n",
    "    num_images = min(num_images, len(images))\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    grid_size = int(np.ceil(np.sqrt(num_images)))\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=figsize)\n",
    "    axes = axes.flatten() if grid_size > 1 else [axes]\n",
    "    \n",
    "    for idx in range(len(axes)):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        if idx < num_images:\n",
    "            # Get image and label\n",
    "            img = images[idx].squeeze().numpy()  # Remove channel dim for grayscale\n",
    "            label = labels[idx].item()\n",
    "            class_name = class_names[label]\n",
    "            \n",
    "            # Display image\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.set_title(f'{class_name}\\n(class {label})', fontsize=10)\n",
    "        \n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Batch from Training Set\\n(OCT Grayscale Images)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize the sample batch (only if DEBUG mode is enabled)\n",
    "if DEBUG:\n",
    "    print(\"Visualizing sample batch from training set...\")\n",
    "    visualize_batch(\n",
    "        images=sample_batch_images,\n",
    "        labels=sample_batch_labels,\n",
    "        class_names=config.CLASS_NAMES,\n",
    "        num_images=16,\n",
    "        figsize=(12, 12)\n",
    "    )\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping batch visualization (DEBUG=False)\")\n",
    "    print(\"   Set DEBUG=True in Kaggle Configuration cell to enable visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Summary: Data Loaders and Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY: READY FOR NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nData Loaders Available:\")\n",
    "print(f\"  ‚Ä¢ train_loader : {len(train_loader)} batches\")\n",
    "print(f\"  ‚Ä¢ val_loader   : {len(val_loader)} batches\")\n",
    "print(f\"  ‚Ä¢ test_loader  : {len(test_loader)} batches\")\n",
    "\n",
    "print(\"\\nConfiguration Object (config):\")\n",
    "print(f\"  ‚Ä¢ NUM_CLASSES  : {config.NUM_CLASSES}\")\n",
    "print(f\"  ‚Ä¢ CLASS_NAMES  : {config.CLASS_NAMES}\")\n",
    "print(f\"  ‚Ä¢ IMAGE_SIZE   : {config.IMAGE_SIZE}\")\n",
    "print(f\"  ‚Ä¢ BATCH_SIZE   : {config.BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ DEVICE       : {config.DEVICE}\")\n",
    "\n",
    "print(\"\\nData Format:\")\n",
    "print(f\"  ‚Ä¢ Input Shape  : ({config.BATCH_SIZE}, {config.NUM_CHANNELS}, {config.IMAGE_SIZE[0]}, {config.IMAGE_SIZE[1]})\")\n",
    "print(f\"  ‚Ä¢ Pixel Range  : [0.0, 1.0]\")\n",
    "print(f\"  ‚Ä¢ Data Type    : torch.float32\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP G: Final Sanity Summary Cell (Pre-Training Validation)\n",
    "# =============================================================================\n",
    "# This cell validates all critical conditions before training begins.\n",
    "# Execution will STOP if any condition fails.\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîç KAGGLE PRE-TRAINING SANITY CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect all critical info\n",
    "sanity_checks = []\n",
    "\n",
    "# 1. Device check\n",
    "print(f\"\\n1. Device: {config.DEVICE}\")\n",
    "if config.DEVICE.type == 'cuda':\n",
    "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    sanity_checks.append((\"GPU Available\", True))\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  WARNING: Running on CPU - training will be very slow!\")\n",
    "    sanity_checks.append((\"GPU Available\", False))\n",
    "\n",
    "# 2. Batch size check\n",
    "print(f\"\\n2. Batch Size: {config.BATCH_SIZE}\")\n",
    "if config.BATCH_SIZE <= 16:\n",
    "    print(\"   ‚úì Kaggle-safe batch size\")\n",
    "    sanity_checks.append((\"Batch Size Safe\", True))\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  WARNING: Batch size > 16 may cause OOM on Kaggle\")\n",
    "    sanity_checks.append((\"Batch Size Safe\", False))\n",
    "\n",
    "# 3. Image size check\n",
    "print(f\"\\n3. Image Size: {config.IMAGE_SIZE}\")\n",
    "sanity_checks.append((\"Image Size Valid\", config.IMAGE_SIZE[0] > 0 and config.IMAGE_SIZE[1] > 0))\n",
    "\n",
    "# 4. Training samples check\n",
    "print(f\"\\n4. Training Samples: {len(train_dataset)}\")\n",
    "if len(train_dataset) > 0:\n",
    "    sanity_checks.append((\"Training Data\", True))\n",
    "else:\n",
    "    sanity_checks.append((\"Training Data\", False))\n",
    "\n",
    "# 5. Class names check\n",
    "print(f\"\\n5. Classes ({config.NUM_CLASSES}):\")\n",
    "for i, name in enumerate(config.CLASS_NAMES):\n",
    "    print(f\"   [{i}] {name}\")\n",
    "sanity_checks.append((\"Classes Detected\", config.NUM_CLASSES > 0))\n",
    "\n",
    "# 6. Output directory check\n",
    "print(f\"\\n6. Output Directory: {config.OUTPUT_DIR}\")\n",
    "output_writable = os.access(config.OUTPUT_DIR, os.W_OK)\n",
    "if output_writable:\n",
    "    print(\"   ‚úì Directory is writable\")\n",
    "    sanity_checks.append((\"Output Writable\", True))\n",
    "else:\n",
    "    print(\"   ‚úó Directory is NOT writable!\")\n",
    "    sanity_checks.append((\"Output Writable\", False))\n",
    "\n",
    "# 7. Debug mode status\n",
    "print(f\"\\n7. Debug Mode: {DEBUG}\")\n",
    "if not DEBUG:\n",
    "    print(\"   ‚Üí Visualizations minimized for production run\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SANITY CHECK SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_passed = True\n",
    "for check_name, passed in sanity_checks:\n",
    "    status = \"‚úì PASS\" if passed else \"‚úó FAIL\"\n",
    "    print(f\"  {check_name:<20} : {status}\")\n",
    "    if not passed and check_name in [\"Training Data\", \"Classes Detected\", \"Output Writable\"]:\n",
    "        all_passed = False\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Critical assertion\n",
    "if not all_passed:\n",
    "    raise RuntimeError(\"‚ùå CRITICAL: Sanity check failed! Please fix the issues above before training.\")\n",
    "\n",
    "print(\"\\n‚úÖ ALL CRITICAL CHECKS PASSED - Ready for training!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da068979",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Dataset Loaded Successfully ‚Äî Ready for CDBN Pretraining\n",
    "\n",
    "‚úÖ **Environment configured** with PyTorch and GPU support (if available)\n",
    "\n",
    "‚úÖ **Central configuration** established with all hyperparameters\n",
    "\n",
    "‚úÖ **Datasets loaded** for train, validation, and test splits\n",
    "\n",
    "‚úÖ **DataLoaders created** with proper batching and shuffling\n",
    "\n",
    "‚úÖ **Sanity checks passed** ‚Äî tensor shapes and pixel ranges verified\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- STEP 5: Implement Restricted Boltzmann Machine (RBM) layer\n",
    "- STEP 6: Implement Convolutional RBM (CRBM) layer  \n",
    "- STEP 7: Stack layers to form the Convolutional Deep Belief Network (CDBN)\n",
    "- STEP 8: Greedy layer-wise pretraining\n",
    "- STEP 9: Fine-tuning with supervised classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4416d4",
   "metadata": {},
   "source": [
    "## STEP 5 ‚Äî Convolutional Restricted Boltzmann Machine (Conv-RBM) Module\n",
    "\n",
    "Implement a Conv-RBM as a building block for the CDBN:\n",
    "\n",
    "**Architecture:**\n",
    "- **Visible units:** Gaussian (real-valued OCT pixels in [0, 1])\n",
    "- **Hidden units:** Bernoulli (binary activations)\n",
    "- **Weight sharing:** Via convolutional kernels\n",
    "- **No pooling** in this layer\n",
    "\n",
    "**Energy Function (Gaussian-Bernoulli RBM):**\n",
    "\n",
    "$$E(v, h) = \\sum_i \\frac{(v_i - a_i)^2}{2\\sigma^2} - \\sum_j b_j h_j - \\sum_{i,j} \\frac{v_i}{\\sigma^2} W_{ij} h_j$$\n",
    "\n",
    "where $\\sigma = 1$ (unit variance) for normalized inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: Convolutional Restricted Boltzmann Machine (Conv-RBM) Module\n",
    "# =============================================================================\n",
    "\n",
    "class ConvRBM(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Restricted Boltzmann Machine (Conv-RBM) with:\n",
    "        - Gaussian visible units (real-valued, for normalized image pixels)\n",
    "        - Bernoulli hidden units (binary activations)\n",
    "        - Weight sharing via 2D convolution\n",
    "    \n",
    "    This is a generative model that learns to reconstruct input images\n",
    "    while capturing hierarchical features in the hidden layer.\n",
    "    \n",
    "    Architecture:\n",
    "        visible (v) <---> hidden (h)\n",
    "        \n",
    "        v: [batch, visible_channels, H, W]  - Input image\n",
    "        h: [batch, hidden_channels, H', W'] - Hidden feature maps\n",
    "        \n",
    "        where H' = H - kernel_size + 1, W' = W - kernel_size + 1 (valid convolution)\n",
    "    \n",
    "    Energy Function (Gaussian-Bernoulli):\n",
    "        E(v, h) = sum_i (v_i - a_i)^2 / (2œÉ¬≤) - sum_j b_j * h_j - sum_ij (v_i / œÉ¬≤) * W_ij * h_j\n",
    "        \n",
    "        With œÉ = 1 (unit variance) for [0, 1] normalized inputs.\n",
    "    \n",
    "    Conditional Distributions:\n",
    "        P(h_j = 1 | v) = sigmoid(b_j + sum_i W_ij * v_i)  [Bernoulli]\n",
    "        P(v_i | h)     = N(a_i + sum_j W_ij * h_j, œÉ¬≤)    [Gaussian with œÉ=1]\n",
    "    \n",
    "    Args:\n",
    "        visible_channels (int): Number of input channels (1 for grayscale)\n",
    "        hidden_channels (int): Number of hidden feature maps to learn\n",
    "        kernel_size (int): Size of the convolutional kernel (square)\n",
    "        \n",
    "    Attributes:\n",
    "        W: Convolutional weights [hidden_channels, visible_channels, kernel_size, kernel_size]\n",
    "        h_bias: Hidden bias [hidden_channels] - one bias per feature map\n",
    "        v_bias: Visible bias [visible_channels] - one bias per input channel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        visible_channels: int = 1,\n",
    "        hidden_channels: int = 32,\n",
    "        kernel_size: int = 7\n",
    "    ):\n",
    "        super(ConvRBM, self).__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.visible_channels = visible_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # Learnable Parameters\n",
    "        # ---------------------------------------------------------------------\n",
    "        \n",
    "        # Convolutional weights: W\n",
    "        # Shape: [hidden_channels, visible_channels, kernel_size, kernel_size]\n",
    "        # Initialized with small random values (Xavier/Glorot-like scaling)\n",
    "        self.W = nn.Parameter(\n",
    "            torch.randn(hidden_channels, visible_channels, kernel_size, kernel_size) * 0.01\n",
    "        )\n",
    "        \n",
    "        # Hidden bias: b_j (one per feature map)\n",
    "        # Shape: [hidden_channels]\n",
    "        # Initialized to zero\n",
    "        self.h_bias = nn.Parameter(\n",
    "            torch.zeros(hidden_channels)\n",
    "        )\n",
    "        \n",
    "        # Visible bias: a_i (one per input channel)\n",
    "        # Shape: [visible_channels]\n",
    "        # Initialized to zero\n",
    "        self.v_bias = nn.Parameter(\n",
    "            torch.zeros(visible_channels)\n",
    "        )\n",
    "        \n",
    "    def hidden_probabilities(self, v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute hidden unit activation probabilities given visible units.\n",
    "        \n",
    "        P(h_j = 1 | v) = sigmoid(b_j + conv(v, W))\n",
    "        \n",
    "        The convolution implements the sum: sum_i W_ij * v_i\n",
    "        with weight sharing across spatial locations.\n",
    "        \n",
    "        Args:\n",
    "            v: Visible units tensor\n",
    "               Shape: [batch_size, visible_channels, H, W]\n",
    "               \n",
    "        Returns:\n",
    "            h_prob: Hidden activation probabilities\n",
    "                    Shape: [batch_size, hidden_channels, H', W']\n",
    "                    where H' = H - kernel_size + 1 (valid convolution)\n",
    "        \"\"\"\n",
    "        # Convolve visible with weights (valid convolution, no padding)\n",
    "        # Input:  [batch, visible_channels, H, W]\n",
    "        # Weight: [hidden_channels, visible_channels, kernel_size, kernel_size]\n",
    "        # Output: [batch, hidden_channels, H - kernel_size + 1, W - kernel_size + 1]\n",
    "        conv_out = nn.functional.conv2d(v, self.W, bias=None, padding=0)\n",
    "        \n",
    "        # Add hidden bias (broadcast across spatial dimensions)\n",
    "        # h_bias shape: [hidden_channels] -> [1, hidden_channels, 1, 1]\n",
    "        h_bias_expanded = self.h_bias.view(1, -1, 1, 1)\n",
    "        \n",
    "        # Pre-activation: b_j + conv(v, W)\n",
    "        pre_activation = conv_out + h_bias_expanded\n",
    "        \n",
    "        # Apply sigmoid to get probabilities (Bernoulli hidden units)\n",
    "        h_prob = torch.sigmoid(pre_activation)\n",
    "        \n",
    "        return h_prob\n",
    "    \n",
    "    def sample_hidden(self, v: torch.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        Sample binary hidden states from visible units.\n",
    "        \n",
    "        h ~ Bernoulli(P(h=1|v))\n",
    "        \n",
    "        Args:\n",
    "            v: Visible units tensor\n",
    "               Shape: [batch_size, visible_channels, H, W]\n",
    "               \n",
    "        Returns:\n",
    "            h_prob: Hidden activation probabilities\n",
    "                    Shape: [batch_size, hidden_channels, H', W']\n",
    "            h_sample: Binary hidden samples (0 or 1)\n",
    "                      Shape: [batch_size, hidden_channels, H', W']\n",
    "        \"\"\"\n",
    "        # Get hidden probabilities\n",
    "        h_prob = self.hidden_probabilities(v)\n",
    "        \n",
    "        # Sample from Bernoulli distribution\n",
    "        # Each unit is independently set to 1 with probability h_prob\n",
    "        h_sample = torch.bernoulli(h_prob)\n",
    "        \n",
    "        return h_prob, h_sample\n",
    "    \n",
    "    def visible_probabilities(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reconstruct visible units (mean) given hidden units.\n",
    "        \n",
    "        For Gaussian visible units with unit variance:\n",
    "        E[v_i | h] = a_i + conv_transpose(h, W)\n",
    "        \n",
    "        The transposed convolution implements: sum_j W_ij * h_j\n",
    "        \n",
    "        Args:\n",
    "            h: Hidden units tensor (probabilities or samples)\n",
    "               Shape: [batch_size, hidden_channels, H', W']\n",
    "               \n",
    "        Returns:\n",
    "            v_mean: Reconstructed visible mean\n",
    "                    Shape: [batch_size, visible_channels, H, W]\n",
    "                    where H = H' + kernel_size - 1\n",
    "        \"\"\"\n",
    "        # Transposed convolution to upsample hidden to visible space\n",
    "        # Input:  [batch, hidden_channels, H', W']\n",
    "        # Weight: [hidden_channels, visible_channels, kernel_size, kernel_size]\n",
    "        # Output: [batch, visible_channels, H' + kernel_size - 1, W' + kernel_size - 1]\n",
    "        conv_transpose_out = nn.functional.conv_transpose2d(h, self.W, bias=None, padding=0)\n",
    "        \n",
    "        # Add visible bias (broadcast across spatial dimensions)\n",
    "        # v_bias shape: [visible_channels] -> [1, visible_channels, 1, 1]\n",
    "        v_bias_expanded = self.v_bias.view(1, -1, 1, 1)\n",
    "        \n",
    "        # Reconstructed visible mean: a_i + conv_transpose(h, W)\n",
    "        v_mean = conv_transpose_out + v_bias_expanded\n",
    "        \n",
    "        return v_mean\n",
    "    \n",
    "    def sample_visible(self, h: torch.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        Sample visible units from hidden units (Gaussian sampling).\n",
    "        \n",
    "        v ~ N(E[v|h], œÉ¬≤) where œÉ = 1 (unit variance)\n",
    "        \n",
    "        For normalized inputs [0, 1], we use unit variance Gaussian.\n",
    "        The mean is computed from visible_probabilities().\n",
    "        \n",
    "        Args:\n",
    "            h: Hidden units tensor (probabilities or samples)\n",
    "               Shape: [batch_size, hidden_channels, H', W']\n",
    "               \n",
    "        Returns:\n",
    "            v_mean: Reconstructed visible mean\n",
    "                    Shape: [batch_size, visible_channels, H, W]\n",
    "            v_sample: Gaussian visible samples\n",
    "                      Shape: [batch_size, visible_channels, H, W]\n",
    "        \"\"\"\n",
    "        # Get visible mean (reconstruction)\n",
    "        v_mean = self.visible_probabilities(h)\n",
    "        \n",
    "        # Sample from Gaussian: v = mean + noise (œÉ = 1)\n",
    "        # Add standard Gaussian noise\n",
    "        v_sample = v_mean + torch.randn_like(v_mean)\n",
    "        \n",
    "        return v_mean, v_sample\n",
    "    \n",
    "    def forward(self, v: torch.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        Forward pass: compute hidden probabilities and samples.\n",
    "        \n",
    "        This is the inference direction: visible -> hidden\n",
    "        \n",
    "        Args:\n",
    "            v: Input visible units (batch of images)\n",
    "               Shape: [batch_size, visible_channels, H, W]\n",
    "               \n",
    "        Returns:\n",
    "            h_prob: Hidden activation probabilities\n",
    "                    Shape: [batch_size, hidden_channels, H', W']\n",
    "            h_sample: Binary hidden samples\n",
    "                      Shape: [batch_size, hidden_channels, H', W']\n",
    "        \"\"\"\n",
    "        h_prob, h_sample = self.sample_hidden(v)\n",
    "        return h_prob, h_sample\n",
    "    \n",
    "    def get_output_size(self, input_size: tuple) -> tuple:\n",
    "        \"\"\"\n",
    "        Calculate hidden layer spatial dimensions given input size.\n",
    "        \n",
    "        For valid convolution: output_size = input_size - kernel_size + 1\n",
    "        \n",
    "        Args:\n",
    "            input_size: (H, W) of input images\n",
    "            \n",
    "        Returns:\n",
    "            output_size: (H', W') of hidden feature maps\n",
    "        \"\"\"\n",
    "        H, W = input_size\n",
    "        H_out = H - self.kernel_size + 1\n",
    "        W_out = W - self.kernel_size + 1\n",
    "        return (H_out, W_out)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"ConvRBM(\\n\"\n",
    "            f\"  visible_channels={self.visible_channels},\\n\"\n",
    "            f\"  hidden_channels={self.hidden_channels},\\n\"\n",
    "            f\"  kernel_size={self.kernel_size},\\n\"\n",
    "            f\"  W shape={tuple(self.W.shape)},\\n\"\n",
    "            f\"  h_bias shape={tuple(self.h_bias.shape)},\\n\"\n",
    "            f\"  v_bias shape={tuple(self.v_bias.shape)}\\n\"\n",
    "            f\")\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"‚úì ConvRBM class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc37859",
   "metadata": {},
   "source": [
    "### Conv-RBM Verification\n",
    "\n",
    "Test the ConvRBM module to ensure correct tensor shapes and functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf272bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Conv-RBM Verification\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONV-RBM MODULE VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.1 Instantiate ConvRBM\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Create Conv-RBM instance with specified parameters\n",
    "conv_rbm = ConvRBM(\n",
    "    visible_channels=config.NUM_CHANNELS,  # 1 (grayscale)\n",
    "    hidden_channels=32,                     # 32 feature maps\n",
    "    kernel_size=7                           # 7√ó7 kernels\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "conv_rbm = conv_rbm.to(config.DEVICE)\n",
    "\n",
    "print(\"\\nConv-RBM Instance:\")\n",
    "print(conv_rbm)\n",
    "\n",
    "print(f\"\\nDevice: {next(conv_rbm.parameters()).device}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.2 Parameter Summary\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"PARAMETER SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "total_params = 0\n",
    "for name, param in conv_rbm.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    total_params += num_params\n",
    "    print(f\"  {name:<10} : shape={tuple(param.shape):<25} params={num_params:,}\")\n",
    "\n",
    "print(f\"\\n  Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.3 Test with One Batch from DataLoader\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"FORWARD PASS TEST\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get one batch from training loader\n",
    "test_batch_v, test_batch_labels = next(iter(train_loader))\n",
    "test_batch_v = test_batch_v.to(config.DEVICE)\n",
    "\n",
    "print(f\"\\nInput (visible):\")\n",
    "print(f\"  Shape       : {test_batch_v.shape}\")\n",
    "print(f\"  Expected    : [batch_size, {config.NUM_CHANNELS}, {config.IMAGE_SIZE[0]}, {config.IMAGE_SIZE[1]}]\")\n",
    "print(f\"  Device      : {test_batch_v.device}\")\n",
    "print(f\"  Dtype       : {test_batch_v.dtype}\")\n",
    "print(f\"  Value range : [{test_batch_v.min().item():.4f}, {test_batch_v.max().item():.4f}]\")\n",
    "\n",
    "# Compute expected output size\n",
    "expected_h_size = conv_rbm.get_output_size(config.IMAGE_SIZE)\n",
    "print(f\"\\nExpected hidden spatial size: {expected_h_size}\")\n",
    "\n",
    "# Forward pass (inference: v -> h)\n",
    "with torch.no_grad():\n",
    "    h_prob, h_sample = conv_rbm(test_batch_v)\n",
    "\n",
    "print(f\"\\nHidden Probabilities (h_prob):\")\n",
    "print(f\"  Shape       : {h_prob.shape}\")\n",
    "print(f\"  Expected    : [{test_batch_v.shape[0]}, 32, {expected_h_size[0]}, {expected_h_size[1]}]\")\n",
    "print(f\"  Value range : [{h_prob.min().item():.4f}, {h_prob.max().item():.4f}]\")\n",
    "print(f\"  Mean        : {h_prob.mean().item():.4f}\")\n",
    "\n",
    "print(f\"\\nHidden Samples (h_sample):\")\n",
    "print(f\"  Shape       : {h_sample.shape}\")\n",
    "print(f\"  Unique vals : {torch.unique(h_sample).tolist()}\")  # Should be [0, 1]\n",
    "print(f\"  Sparsity    : {(h_sample == 0).float().mean().item():.2%} zeros\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.4 Test Reconstruction (h -> v)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"RECONSTRUCTION TEST (h -> v)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Reconstruct visible from hidden probabilities\n",
    "    v_mean, v_sample = conv_rbm.sample_visible(h_prob)\n",
    "\n",
    "print(f\"\\nReconstructed Visible Mean (v_mean):\")\n",
    "print(f\"  Shape       : {v_mean.shape}\")\n",
    "print(f\"  Expected    : {test_batch_v.shape}\")\n",
    "print(f\"  Value range : [{v_mean.min().item():.4f}, {v_mean.max().item():.4f}]\")\n",
    "\n",
    "print(f\"\\nReconstructed Visible Sample (v_sample):\")\n",
    "print(f\"  Shape       : {v_sample.shape}\")\n",
    "print(f\"  Value range : [{v_sample.min().item():.4f}, {v_sample.max().item():.4f}]\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.5 Shape Assertions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"SHAPE ASSERTIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check hidden shape\n",
    "expected_h_shape = (test_batch_v.shape[0], 32, expected_h_size[0], expected_h_size[1])\n",
    "assert h_prob.shape == expected_h_shape, f\"Hidden prob shape mismatch: {h_prob.shape} vs {expected_h_shape}\"\n",
    "assert h_sample.shape == expected_h_shape, f\"Hidden sample shape mismatch: {h_sample.shape} vs {expected_h_shape}\"\n",
    "print(f\"  ‚úì Hidden shapes correct: {h_prob.shape}\")\n",
    "\n",
    "# Check reconstruction shape matches input\n",
    "assert v_mean.shape == test_batch_v.shape, f\"Reconstruction shape mismatch: {v_mean.shape} vs {test_batch_v.shape}\"\n",
    "assert v_sample.shape == test_batch_v.shape, f\"Reconstruction sample shape mismatch\"\n",
    "print(f\"  ‚úì Reconstruction shapes correct: {v_mean.shape}\")\n",
    "\n",
    "# Check hidden values are probabilities (between 0 and 1)\n",
    "assert h_prob.min() >= 0 and h_prob.max() <= 1, \"Hidden probabilities out of [0, 1] range\"\n",
    "print(f\"  ‚úì Hidden probabilities in valid range [0, 1]\")\n",
    "\n",
    "# Check hidden samples are binary\n",
    "assert set(torch.unique(h_sample).tolist()).issubset({0.0, 1.0}), \"Hidden samples not binary\"\n",
    "print(f\"  ‚úì Hidden samples are binary {{0, 1}}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL VERIFICATIONS PASSED!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e572bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5.6 Visualize Input vs Reconstruction\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def visualize_reconstruction(original, reconstructed, num_images=4, figsize=(12, 5)):\n",
    "    \"\"\"\n",
    "    Visualize original images alongside their reconstructions.\n",
    "    \n",
    "    Args:\n",
    "        original: Original input tensor [batch, 1, H, W]\n",
    "        reconstructed: Reconstructed tensor [batch, 1, H, W]\n",
    "        num_images: Number of image pairs to show\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    num_images = min(num_images, len(original))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_images, figsize=figsize)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Original\n",
    "        orig_img = original[i].squeeze().cpu().numpy()\n",
    "        axes[0, i].imshow(orig_img, cmap='gray')\n",
    "        axes[0, i].set_title(f'Original {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Reconstruction\n",
    "        recon_img = reconstructed[i].squeeze().cpu().numpy()\n",
    "        # Clip to valid range for visualization\n",
    "        recon_img = np.clip(recon_img, 0, 1)\n",
    "        axes[1, i].imshow(recon_img, cmap='gray')\n",
    "        axes[1, i].set_title(f'Recon {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Reconstructed', fontsize=12)\n",
    "    \n",
    "    plt.suptitle('Conv-RBM: Input vs Reconstruction (Untrained)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize (reconstruction will be poor since RBM is untrained)\n",
    "print(\"Visualizing input vs reconstruction (untrained RBM - expect poor reconstruction):\")\n",
    "visualize_reconstruction(test_batch_v, v_mean, num_images=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f3008",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conv-RBM Module Implemented ‚Äî Ready for Contrastive Divergence\n",
    "\n",
    "‚úÖ **ConvRBM class** implemented with Gaussian visible / Bernoulli hidden units\n",
    "\n",
    "‚úÖ **Core methods verified:**\n",
    "- `hidden_probabilities(v)` ‚Äî Computes P(h|v) via convolution + sigmoid\n",
    "- `sample_hidden(v)` ‚Äî Bernoulli sampling of hidden states\n",
    "- `visible_probabilities(h)` ‚Äî Reconstructs visible mean via transposed convolution\n",
    "- `sample_visible(h)` ‚Äî Gaussian sampling with unit variance\n",
    "- `forward(v)` ‚Äî Returns hidden probabilities and samples\n",
    "\n",
    "‚úÖ **Shape verification passed** ‚Äî Input ‚Üí Hidden ‚Üí Reconstruction cycle works correctly\n",
    "\n",
    "‚úÖ **Weight sharing** implemented via `nn.functional.conv2d` and `conv_transpose2d`\n",
    "\n",
    "---\n",
    "\n",
    "**Key Dimensions:**\n",
    "- Input: `[batch, 1, 128, 128]`\n",
    "- Hidden: `[batch, 32, 122, 122]` (after 7√ó7 valid convolution)\n",
    "- Reconstruction: `[batch, 1, 128, 128]` (matches input)\n",
    "\n",
    "**Next Steps:**\n",
    "- STEP 6: Implement Contrastive Divergence (CD-k) training algorithm\n",
    "- STEP 7: Train the Conv-RBM layer on OCT images\n",
    "- STEP 8: Stack layers to form the full CDBN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73471fe",
   "metadata": {},
   "source": [
    "## STEP 6 ‚Äî Contrastive Divergence (CD-k) Trainer\n",
    "\n",
    "Implement the CD-k algorithm for unsupervised training of the Conv-RBM.\n",
    "\n",
    "**Contrastive Divergence Algorithm:**\n",
    "\n",
    "The gradient of the log-likelihood is approximated by:\n",
    "\n",
    "$$\\frac{\\partial \\log P(v)}{\\partial W} \\approx \\langle v \\cdot h^T \\rangle_{\\text{data}} - \\langle v \\cdot h^T \\rangle_{\\text{model}}$$\n",
    "\n",
    "**CD-k Steps:**\n",
    "1. **Positive Phase:** Clamp visible units to data, sample hidden states\n",
    "2. **Gibbs Sampling:** Run k steps of alternating sampling (h ‚Üí v ‚Üí h)\n",
    "3. **Negative Phase:** Use reconstructed visible and hidden for model statistics\n",
    "4. **Update:** $\\Delta W = \\eta \\cdot (\\langle v_0 h_0 \\rangle - \\langle v_k h_k \\rangle)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd75a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: Contrastive Divergence (CD-k) Trainer\n",
    "# =============================================================================\n",
    "\n",
    "class CDTrainer:\n",
    "    \"\"\"\n",
    "    Contrastive Divergence (CD-k) Trainer for Convolutional RBM.\n",
    "    \n",
    "    CD-k is an approximate method for training RBMs without computing the \n",
    "    intractable partition function. It estimates the gradient of the \n",
    "    log-likelihood using k steps of Gibbs sampling.\n",
    "    \n",
    "    Algorithm:\n",
    "        1. POSITIVE PHASE (data-driven):\n",
    "           - Clamp v0 = input data\n",
    "           - Compute h0_prob = P(h|v0)\n",
    "           - Sample h0 ~ Bernoulli(h0_prob)\n",
    "           \n",
    "        2. GIBBS SAMPLING (k steps):\n",
    "           For i in range(k):\n",
    "               - v_i = sample_visible(h_{i-1})\n",
    "               - h_i = sample_hidden(v_i)\n",
    "           \n",
    "        3. NEGATIVE PHASE (model-driven):\n",
    "           - Use vk and hk from Gibbs chain\n",
    "           \n",
    "        4. PARAMETER UPDATES:\n",
    "           ŒîW = lr * (‚ü®v0 ‚äó h0‚ü© - ‚ü®vk ‚äó hk‚ü©)\n",
    "           Œîh_bias = lr * (‚ü®h0‚ü© - ‚ü®hk‚ü©)  \n",
    "           Œîv_bias = lr * (‚ü®v0‚ü© - ‚ü®vk‚ü©)\n",
    "           \n",
    "           where ‚äó denotes the correlation computed via convolution\n",
    "    \n",
    "    Args:\n",
    "        rbm: ConvRBM instance to train\n",
    "        learning_rate: Learning rate for parameter updates\n",
    "        k: Number of Gibbs sampling steps (default: 1)\n",
    "        device: Device for computation\n",
    "        momentum: Momentum coefficient for updates (default: 0.0)\n",
    "        weight_decay: L2 regularization coefficient (default: 0.0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        rbm: ConvRBM,\n",
    "        learning_rate: float = 0.01,\n",
    "        k: int = 1,\n",
    "        device: torch.device = None,\n",
    "        momentum: float = 0.0,\n",
    "        weight_decay: float = 0.0001\n",
    "    ):\n",
    "        self.rbm = rbm\n",
    "        self.lr = learning_rate\n",
    "        self.k = k\n",
    "        self.device = device if device else torch.device('cpu')\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Initialize velocity terms for momentum-based updates\n",
    "        # These accumulate the running average of gradients\n",
    "        self.W_velocity = torch.zeros_like(rbm.W.data)\n",
    "        self.h_bias_velocity = torch.zeros_like(rbm.h_bias.data)\n",
    "        self.v_bias_velocity = torch.zeros_like(rbm.v_bias.data)\n",
    "        \n",
    "    def train_batch(self, v0: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Train the RBM on a single batch using CD-k.\n",
    "        \n",
    "        This method performs one complete CD-k update:\n",
    "        1. Positive phase with clamped data\n",
    "        2. k steps of Gibbs sampling  \n",
    "        3. Negative phase with fantasy particles\n",
    "        4. Parameter updates using the difference\n",
    "        \n",
    "        Args:\n",
    "            v0: Input visible batch (real OCT images)\n",
    "                Shape: [batch_size, channels, height, width]\n",
    "                \n",
    "        Returns:\n",
    "            reconstruction_loss: MSE between v0 and vk (reconstruction error)\n",
    "        \"\"\"\n",
    "        batch_size = v0.shape[0]\n",
    "        \n",
    "        # Move to device\n",
    "        v0 = v0.to(self.device)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # POSITIVE PHASE (Data-Driven Statistics)\n",
    "        # =====================================================================\n",
    "        # The positive phase captures the correlation between data and hidden\n",
    "        # units when the visible units are clamped to the training data.\n",
    "        #\n",
    "        # Math: ‚ü®v_i h_j‚ü©_data = E_data[v_i * P(h_j=1|v)]\n",
    "        \n",
    "        # Compute hidden probabilities given visible data\n",
    "        # h0_prob: P(h=1|v0), shape: [batch, hidden_channels, H', W']\n",
    "        h0_prob = self.rbm.hidden_probabilities(v0)\n",
    "        \n",
    "        # NUMERICAL STABILITY: Clamp probabilities to valid range\n",
    "        h0_prob = torch.clamp(h0_prob, min=1e-7, max=1.0 - 1e-7)\n",
    "        \n",
    "        # Sample hidden states from probabilities (Bernoulli sampling)\n",
    "        # h0_sample: binary hidden states {0, 1}\n",
    "        h0_sample = torch.bernoulli(h0_prob)\n",
    "        \n",
    "        # Compute positive phase correlation (data statistics)\n",
    "        # For convolutional RBM, this is computed via convolution:\n",
    "        # positive_W = conv2d(v0, h0_prob) averaged over batch\n",
    "        #\n",
    "        # This computes: sum over all (i,j) spatial positions of v0_i * h0_j\n",
    "        # with weight sharing across spatial locations\n",
    "        positive_W = self._compute_weight_gradient(v0, h0_prob)\n",
    "        \n",
    "        # Positive phase for biases (spatial average)\n",
    "        # h_bias gradient: mean activation per feature map\n",
    "        positive_h_bias = h0_prob.mean(dim=(0, 2, 3))  # [hidden_channels]\n",
    "        \n",
    "        # v_bias gradient: mean activation per visible channel\n",
    "        positive_v_bias = v0.mean(dim=(0, 2, 3))  # [visible_channels]\n",
    "        \n",
    "        # =====================================================================\n",
    "        # GIBBS SAMPLING (k steps)\n",
    "        # =====================================================================\n",
    "        # Run k steps of alternating Gibbs sampling to get \"fantasy particles\"\n",
    "        # that represent the model's equilibrium distribution.\n",
    "        #\n",
    "        # Each step: h -> v -> h (block Gibbs sampling)\n",
    "        \n",
    "        # Start Gibbs chain from the sampled hidden states\n",
    "        hk_sample = h0_sample\n",
    "        \n",
    "        for step in range(self.k):\n",
    "            # -------------------------------------------------------------\n",
    "            # Step 1: Sample visible given hidden (v ~ P(v|h))\n",
    "            # -------------------------------------------------------------\n",
    "            # For Gaussian visible units: v = mean + noise\n",
    "            # Mean: E[v|h] = v_bias + conv_transpose(h, W)\n",
    "            vk_mean = self.rbm.visible_probabilities(hk_sample)\n",
    "            \n",
    "            # For Gaussian-Bernoulli RBM, we can use the mean directly\n",
    "            # (sampling adds noise which can hurt learning)\n",
    "            # vk_sample = vk_mean + torch.randn_like(vk_mean)  # Noisy version\n",
    "            vk_sample = vk_mean  # Use mean (deterministic, more stable)\n",
    "            \n",
    "            # NUMERICAL STABILITY: Clamp reconstructed visible to valid range\n",
    "            vk_sample = torch.clamp(vk_sample, min=-5.0, max=5.0)\n",
    "            \n",
    "            # -------------------------------------------------------------\n",
    "            # Step 2: Sample hidden given visible (h ~ P(h|v))  \n",
    "            # -------------------------------------------------------------\n",
    "            # For Bernoulli hidden units: h ~ Bernoulli(sigmoid(h_bias + conv(v, W)))\n",
    "            hk_prob = self.rbm.hidden_probabilities(vk_sample)\n",
    "            \n",
    "            # NUMERICAL STABILITY: Clamp probabilities to valid range\n",
    "            hk_prob = torch.clamp(hk_prob, min=1e-7, max=1.0 - 1e-7)\n",
    "            \n",
    "            # Sample hidden (except on last step where we use probabilities)\n",
    "            if step < self.k - 1:\n",
    "                hk_sample = torch.bernoulli(hk_prob)\n",
    "            # On last step, keep hk_prob for gradient computation\n",
    "        \n",
    "        # =====================================================================\n",
    "        # NEGATIVE PHASE (Model-Driven Statistics)\n",
    "        # =====================================================================\n",
    "        # The negative phase captures the correlation when the network runs\n",
    "        # freely (fantasy particles from Gibbs sampling).\n",
    "        #\n",
    "        # Math: ‚ü®v_i h_j‚ü©_model ‚âà E_model[v_i * P(h_j=1|v)]\n",
    "        \n",
    "        # Use probabilities (not samples) for more stable gradients\n",
    "        # This is the \"probability-based\" CD variant\n",
    "        negative_W = self._compute_weight_gradient(vk_sample, hk_prob)\n",
    "        \n",
    "        # Negative phase for biases\n",
    "        negative_h_bias = hk_prob.mean(dim=(0, 2, 3))  # [hidden_channels]\n",
    "        negative_v_bias = vk_sample.mean(dim=(0, 2, 3))  # [visible_channels]\n",
    "        \n",
    "        # =====================================================================\n",
    "        # PARAMETER UPDATES\n",
    "        # =====================================================================\n",
    "        # Update rule: Œ∏_new = Œ∏_old + lr * (positive - negative) - weight_decay * Œ∏\n",
    "        #\n",
    "        # With momentum: v_new = momentum * v_old + gradient\n",
    "        #                Œ∏_new = Œ∏_old + lr * v_new\n",
    "        \n",
    "        # Compute gradients (positive - negative)\n",
    "        W_grad = positive_W - negative_W\n",
    "        h_bias_grad = positive_h_bias - negative_h_bias\n",
    "        v_bias_grad = positive_v_bias - negative_v_bias\n",
    "        \n",
    "        # NUMERICAL STABILITY: Clip gradients to prevent explosion\n",
    "        max_grad = 1.0\n",
    "        W_grad = torch.clamp(W_grad, min=-max_grad, max=max_grad)\n",
    "        h_bias_grad = torch.clamp(h_bias_grad, min=-max_grad, max=max_grad)\n",
    "        v_bias_grad = torch.clamp(v_bias_grad, min=-max_grad, max=max_grad)\n",
    "        \n",
    "        # Apply weight decay (L2 regularization)\n",
    "        # This penalizes large weights to prevent overfitting\n",
    "        W_grad -= self.weight_decay * self.rbm.W.data\n",
    "        \n",
    "        # Update velocity with momentum\n",
    "        self.W_velocity = self.momentum * self.W_velocity + W_grad\n",
    "        self.h_bias_velocity = self.momentum * self.h_bias_velocity + h_bias_grad\n",
    "        self.v_bias_velocity = self.momentum * self.v_bias_velocity + v_bias_grad\n",
    "        \n",
    "        # Apply updates to parameters (no autograd, manual update)\n",
    "        with torch.no_grad():\n",
    "            self.rbm.W.data += self.lr * self.W_velocity\n",
    "            self.rbm.h_bias.data += self.lr * self.h_bias_velocity\n",
    "            self.rbm.v_bias.data += self.lr * self.v_bias_velocity\n",
    "        \n",
    "        # =====================================================================\n",
    "        # RECONSTRUCTION LOSS\n",
    "        # =====================================================================\n",
    "        # Compute MSE between original input and reconstruction\n",
    "        # This is a proxy for how well the RBM can reconstruct the input\n",
    "        reconstruction_loss = nn.functional.mse_loss(vk_sample, v0).item()\n",
    "        \n",
    "        return reconstruction_loss\n",
    "    \n",
    "    def _compute_weight_gradient(\n",
    "        self, \n",
    "        v: torch.Tensor, \n",
    "        h: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the weight gradient via cross-correlation (memory-efficient).\n",
    "        \n",
    "        For convolutional RBM, the weight gradient is:\n",
    "            ‚àÇE/‚àÇW[hc,vc,i,j] = (1/B) * (1/H'W') * Œ£_b Œ£_x,y h[b,hc,x,y] * v[b,vc,x+i,y+j]\n",
    "            \n",
    "        Uses chunked processing to reduce peak memory usage.\n",
    "        \n",
    "        Args:\n",
    "            v: Visible units [batch, v_channels, H, W]\n",
    "            h: Hidden units/probabilities [batch, h_channels, H', W']\n",
    "            \n",
    "        Returns:\n",
    "            W_grad: Gradient for weights [h_channels, v_channels, kernel, kernel]\n",
    "        \"\"\"\n",
    "        batch_size = v.shape[0]\n",
    "        K = self.rbm.kernel_size\n",
    "        h_channels = self.rbm.hidden_channels\n",
    "        v_channels = self.rbm.visible_channels\n",
    "        \n",
    "        # Get spatial dimensions of hidden layer\n",
    "        h_height, h_width = h.shape[2], h.shape[3]\n",
    "        spatial_size = h_height * h_width  # For normalization\n",
    "        \n",
    "        # Initialize gradient accumulator\n",
    "        W_grad = torch.zeros(\n",
    "            h_channels, v_channels, K, K,\n",
    "            device=v.device, dtype=v.dtype\n",
    "        )\n",
    "        \n",
    "        # Process one sample at a time to save memory\n",
    "        for b in range(batch_size):\n",
    "            # Extract patches for single sample: [v_ch, H', W', K, K]\n",
    "            v_unfold_b = v[b].unfold(1, K, 1).unfold(2, K, 1)  # [v_ch, H', W', K, K]\n",
    "            h_b = h[b]  # [h_ch, H', W']\n",
    "            \n",
    "            # Accumulate: W_grad[o,v,i,j] += sum_hw h[o,h,w] * v_patch[v,h,w,i,j]\n",
    "            W_grad += torch.einsum('ohw,vhwij->ovij', h_b, v_unfold_b)\n",
    "        \n",
    "        # Normalize by batch size AND spatial size for numerical stability\n",
    "        W_grad /= (batch_size * spatial_size)\n",
    "        \n",
    "        return W_grad\n",
    "    \n",
    "    def get_reconstruction(self, v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get reconstruction of visible input (for visualization).\n",
    "        \n",
    "        Args:\n",
    "            v: Input visible units\n",
    "            \n",
    "        Returns:\n",
    "            v_recon: Reconstructed visible units\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            v = v.to(self.device)\n",
    "            h_prob = self.rbm.hidden_probabilities(v)\n",
    "            v_recon = self.rbm.visible_probabilities(h_prob)\n",
    "        return v_recon\n",
    "\n",
    "\n",
    "print(\"‚úì CDTrainer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df68001f",
   "metadata": {},
   "source": [
    "## STEP 7 ‚Äî Unsupervised Pretraining Loop\n",
    "\n",
    "Train the Conv-RBM-1 layer using Contrastive Divergence on OCT images.\n",
    "\n",
    "**Training Configuration:**\n",
    "- Unsupervised learning (labels ignored)\n",
    "- CD-1 (single Gibbs step)\n",
    "- Track reconstruction loss per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 7: Unsupervised Pretraining Loop for Conv-RBM-1\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Training Configuration (STEP C: Kaggle-Safe Values)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Conv-RBM-1 hyperparameters\n",
    "CONVRBM1_CONFIG = {\n",
    "    'visible_channels': config.NUM_CHANNELS,  # 1 (grayscale)\n",
    "    'hidden_channels': 32,                     # Number of learned filters\n",
    "    'kernel_size': 7,                          # 7√ó7 kernels\n",
    "}\n",
    "\n",
    "# CD Training hyperparameters (Kaggle-optimized)\n",
    "# EPOCHS: Reduced from 10 to 5-7 for Kaggle time limits (~9 hour sessions)\n",
    "# CD-k=1: Single Gibbs step is fastest and usually sufficient\n",
    "# LR: Reduced for numerical stability with large spatial dimensions\n",
    "CD_CONFIG = {\n",
    "    'learning_rate': 0.001,    # Reduced from 0.01 for stability\n",
    "    'k': 1,                    # CD-1 (single Gibbs step - fastest)\n",
    "    'momentum': 0.5,           # Momentum coefficient\n",
    "    'weight_decay': 0.0001,    # L2 regularization\n",
    "    'num_epochs': 5,           # Reduced for Kaggle (was 10)\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONV-RBM-1 PRETRAINING CONFIGURATION (Kaggle-Optimized)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConv-RBM Architecture:\")\n",
    "for key, value in CONVRBM1_CONFIG.items():\n",
    "    print(f\"  {key:<20}: {value}\")\n",
    "print(\"\\nCD Training Parameters:\")\n",
    "for key, value in CD_CONFIG.items():\n",
    "    print(f\"  {key:<20}: {value}\")\n",
    "print(\"\\n‚ö° Note: Epochs reduced to 5 for Kaggle time limits\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53725a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize Conv-RBM-1 and CD Trainer\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Create fresh Conv-RBM-1 instance\n",
    "conv_rbm_1 = ConvRBM(\n",
    "    visible_channels=CONVRBM1_CONFIG['visible_channels'],\n",
    "    hidden_channels=CONVRBM1_CONFIG['hidden_channels'],\n",
    "    kernel_size=CONVRBM1_CONFIG['kernel_size']\n",
    ").to(config.DEVICE)\n",
    "\n",
    "print(\"Conv-RBM-1 Architecture:\")\n",
    "print(conv_rbm_1)\n",
    "\n",
    "# Initialize CD Trainer\n",
    "cd_trainer = CDTrainer(\n",
    "    rbm=conv_rbm_1,\n",
    "    learning_rate=CD_CONFIG['learning_rate'],\n",
    "    k=CD_CONFIG['k'],\n",
    "    device=config.DEVICE,\n",
    "    momentum=CD_CONFIG['momentum'],\n",
    "    weight_decay=CD_CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "print(f\"\\nCD Trainer initialized with k={CD_CONFIG['k']} Gibbs steps\")\n",
    "print(f\"Device: {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fccc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Unsupervised Pretraining Loop (Memory-Optimized for Local 24GB GPU)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_convrbm(\n",
    "    rbm: ConvRBM,\n",
    "    trainer: CDTrainer,\n",
    "    train_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    device: torch.device,\n",
    "    memory_cleanup_freq: int = 100  # Clear cache every N batches\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train a Conv-RBM using Contrastive Divergence (Memory-Optimized).\n",
    "    \n",
    "    This is UNSUPERVISED learning - we only use the images, not the labels.\n",
    "    The RBM learns to reconstruct the input distribution.\n",
    "    \n",
    "    Optimized for large datasets on local GPU with aggressive memory cleanup.\n",
    "    \n",
    "    Args:\n",
    "        rbm: ConvRBM instance to train\n",
    "        trainer: CDTrainer instance\n",
    "        train_loader: DataLoader for training data\n",
    "        num_epochs: Number of training epochs\n",
    "        device: Device for computation\n",
    "        memory_cleanup_freq: Frequency of memory cleanup (batches)\n",
    "        \n",
    "    Returns:\n",
    "        history: Dictionary containing training history\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'epoch_loss': [],           # Average loss per epoch\n",
    "        'batch_losses': [],         # All batch losses (for detailed analysis)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING UNSUPERVISED PRETRAINING (Memory-Optimized)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training on {len(train_loader.dataset)} samples\")\n",
    "    print(f\"Batch size: {train_loader.batch_size}\")\n",
    "    print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "    print(f\"Total epochs: {num_epochs}\")\n",
    "    print(f\"Memory cleanup frequency: every {memory_cleanup_freq} batches\")\n",
    "    if torch.cuda.is_available():\n",
    "        print_gpu_memory()\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Progress tracking\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Iterate over batches (UNSUPERVISED - ignore labels)\n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            # Train on batch using CD-k\n",
    "            # images: [batch_size, 1, 128, 128]\n",
    "            # labels are ignored (unsupervised)\n",
    "            batch_loss = trainer.train_batch(images)\n",
    "            epoch_losses.append(batch_loss)\n",
    "            \n",
    "            # Explicit cleanup of batch data\n",
    "            del images\n",
    "            \n",
    "            # Print progress every 100 batches\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{num_epochs} | \"\n",
    "                      f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                      f\"Loss: {batch_loss:.6f}\")\n",
    "            \n",
    "            # Aggressive memory cleanup for large datasets\n",
    "            if torch.cuda.is_available() and (batch_idx + 1) % memory_cleanup_freq == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Compute epoch statistics\n",
    "        avg_epoch_loss = np.mean(epoch_losses)\n",
    "        history['epoch_loss'].append(avg_epoch_loss)\n",
    "        history['batch_losses'].extend(epoch_losses)\n",
    "        \n",
    "        # Clear batch losses list to free memory\n",
    "        epoch_losses.clear()\n",
    "        \n",
    "        # Print epoch summary with GPU memory status\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "              f\"Avg Loss: {avg_epoch_loss:.6f} | \"\n",
    "              f\"Time: {epoch_time:.1f}s\", end=\"\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            mem_alloc = torch.cuda.memory_allocated() / 1024**3\n",
    "            print(f\" | GPU: {mem_alloc:.2f} GB\")\n",
    "        else:\n",
    "            print()\n",
    "        \n",
    "        # Clear cache after each epoch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(\"PRETRAINING COMPLETED!\")\n",
    "    print(f\"Final reconstruction loss: {history['epoch_loss'][-1]:.6f}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print_gpu_memory()\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Run pretraining\n",
    "print(\"Starting Conv-RBM-1 pretraining...\")\n",
    "training_history = train_convrbm(\n",
    "    rbm=conv_rbm_1,\n",
    "    trainer=cd_trainer,\n",
    "    train_loader=train_loader,\n",
    "    num_epochs=CD_CONFIG['num_epochs'],\n",
    "    device=config.DEVICE,\n",
    "    memory_cleanup_freq=100  # Clean every 100 batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0008d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Plot Training Progress\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def plot_training_history(history: dict, figsize=(14, 5)):\n",
    "    \"\"\"\n",
    "    Plot the training history of the Conv-RBM.\n",
    "    \n",
    "    Args:\n",
    "        history: Dictionary containing 'epoch_loss' and 'batch_losses'\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: Loss per epoch\n",
    "    ax1 = axes[0]\n",
    "    epochs = range(1, len(history['epoch_loss']) + 1)\n",
    "    ax1.plot(epochs, history['epoch_loss'], 'b-o', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Reconstruction Loss (MSE)', fontsize=12)\n",
    "    ax1.set_title('Average Loss per Epoch', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(epochs)\n",
    "    \n",
    "    # Plot 2: Loss per batch (smoothed)\n",
    "    ax2 = axes[1]\n",
    "    batch_losses = history['batch_losses']\n",
    "    ax2.plot(batch_losses, 'b-', alpha=0.3, linewidth=0.5, label='Raw')\n",
    "    \n",
    "    # Add smoothed line (moving average)\n",
    "    window = min(50, len(batch_losses) // 10)\n",
    "    if window > 1:\n",
    "        smoothed = np.convolve(batch_losses, np.ones(window)/window, mode='valid')\n",
    "        ax2.plot(range(window-1, len(batch_losses)), smoothed, 'r-', \n",
    "                 linewidth=2, label=f'Smoothed (window={window})')\n",
    "    \n",
    "    ax2.set_xlabel('Batch', fontsize=12)\n",
    "    ax2.set_ylabel('Reconstruction Loss (MSE)', fontsize=12)\n",
    "    ax2.set_title('Loss per Batch', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.suptitle('Conv-RBM-1 Training Progress', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nTraining Summary:\")\n",
    "    print(f\"  Initial loss (epoch 1): {history['epoch_loss'][0]:.6f}\")\n",
    "    print(f\"  Final loss (epoch {len(history['epoch_loss'])}): {history['epoch_loss'][-1]:.6f}\")\n",
    "    print(f\"  Improvement: {(1 - history['epoch_loss'][-1]/history['epoch_loss'][0])*100:.1f}%\")\n",
    "\n",
    "\n",
    "# Plot training history (STEP D: Conditional visualization)\n",
    "if DEBUG:\n",
    "    plot_training_history(training_history)\n",
    "else:\n",
    "    # Print summary only without plots\n",
    "    print(\"\\nüìä Training Summary (DEBUG=False, plots disabled):\")\n",
    "    print(f\"  Initial loss (epoch 1): {training_history['epoch_loss'][0]:.6f}\")\n",
    "    print(f\"  Final loss (epoch {len(training_history['epoch_loss'])}): {training_history['epoch_loss'][-1]:.6f}\")\n",
    "    print(f\"  Improvement: {(1 - training_history['epoch_loss'][-1]/training_history['epoch_loss'][0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49b109",
   "metadata": {},
   "source": [
    "## STEP 8 ‚Äî Visualization\n",
    "\n",
    "### 8.1 Reconstruction Quality\n",
    "Visualize how well the trained Conv-RBM-1 reconstructs OCT images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63383f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 8.1: Reconstruction Quality Visualization\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_reconstructions(\n",
    "    rbm: ConvRBM,\n",
    "    data_loader: DataLoader,\n",
    "    trainer: CDTrainer,\n",
    "    num_samples: int = 8,\n",
    "    figsize: tuple = (16, 6)\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize original OCT images alongside their reconstructions.\n",
    "    \n",
    "    Args:\n",
    "        rbm: Trained ConvRBM\n",
    "        data_loader: DataLoader to get samples from\n",
    "        trainer: CDTrainer (for reconstruction method)\n",
    "        num_samples: Number of samples to display\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Get a batch of images\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images = images[:num_samples].to(config.DEVICE)\n",
    "    labels = labels[:num_samples]\n",
    "    \n",
    "    # Get reconstructions\n",
    "    with torch.no_grad():\n",
    "        reconstructions = trainer.get_reconstruction(images)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=figsize)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        orig = images[i].squeeze().cpu().numpy()\n",
    "        axes[0, i].imshow(orig, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[0, i].set_title(f'{config.CLASS_NAMES[labels[i]]}', fontsize=9)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Reconstruction\n",
    "        recon = reconstructions[i].squeeze().cpu().numpy()\n",
    "        recon_clipped = np.clip(recon, 0, 1)\n",
    "        axes[1, i].imshow(recon_clipped, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Difference (error) map\n",
    "        diff = np.abs(orig - recon_clipped)\n",
    "        axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    # Row labels\n",
    "    axes[0, 0].set_ylabel('Original', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Reconstructed', fontsize=12, fontweight='bold')\n",
    "    axes[2, 0].set_ylabel('|Difference|', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Conv-RBM-1: Original vs Reconstruction\\n(After Unsupervised Pretraining)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute overall reconstruction statistics\n",
    "    with torch.no_grad():\n",
    "        all_images, _ = next(iter(data_loader))\n",
    "        all_images = all_images.to(config.DEVICE)\n",
    "        all_recons = trainer.get_reconstruction(all_images)\n",
    "        mse = nn.functional.mse_loss(all_recons, all_images).item()\n",
    "        \n",
    "    print(f\"\\nReconstruction Quality Metrics:\")\n",
    "    print(f\"  MSE on sample batch: {mse:.6f}\")\n",
    "    print(f\"  RMSE: {np.sqrt(mse):.6f}\")\n",
    "\n",
    "\n",
    "# Visualize reconstructions from training set\n",
    "print(\"Visualizing reconstructions from training set:\")\n",
    "visualize_reconstructions(conv_rbm_1, train_loader, cd_trainer, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions from validation set (unseen during training)\n",
    "print(\"\\nVisualizing reconstructions from validation set (unseen data):\")\n",
    "visualize_reconstructions(conv_rbm_1, val_loader, cd_trainer, num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f1c96",
   "metadata": {},
   "source": [
    "### 8.2 Learned Convolutional Filters\n",
    "Visualize the 32 learned filters (7√ó7 kernels) from Conv-RBM-1.\n",
    "\n",
    "These filters represent the features the RBM has learned to detect in OCT images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 8.2: Learned Convolutional Filters Visualization\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_filters(\n",
    "    rbm: ConvRBM,\n",
    "    num_filters: int = 16,\n",
    "    figsize: tuple = (12, 12),\n",
    "    normalize: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize learned convolutional filters from the Conv-RBM.\n",
    "    \n",
    "    Each filter represents a pattern the RBM has learned to detect.\n",
    "    For OCT images, these might include edges, textures, layer boundaries, etc.\n",
    "    \n",
    "    Args:\n",
    "        rbm: Trained ConvRBM\n",
    "        num_filters: Number of filters to display (default: 16)\n",
    "        figsize: Figure size\n",
    "        normalize: Whether to normalize each filter for visualization\n",
    "    \"\"\"\n",
    "    # Get weights: [hidden_channels, visible_channels, kernel_h, kernel_w]\n",
    "    weights = rbm.W.data.cpu().numpy()\n",
    "    \n",
    "    # For grayscale (1 channel), squeeze the visible channel dimension\n",
    "    # weights shape: [32, 1, 7, 7] -> [32, 7, 7]\n",
    "    if weights.shape[1] == 1:\n",
    "        weights = weights.squeeze(1)\n",
    "    \n",
    "    num_filters = min(num_filters, weights.shape[0])\n",
    "    grid_size = int(np.ceil(np.sqrt(num_filters)))\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if i < num_filters:\n",
    "            # Get filter\n",
    "            filt = weights[i]\n",
    "            \n",
    "            if normalize:\n",
    "                # Normalize to [0, 1] for visualization\n",
    "                filt_min = filt.min()\n",
    "                filt_max = filt.max()\n",
    "                if filt_max - filt_min > 1e-8:\n",
    "                    filt = (filt - filt_min) / (filt_max - filt_min)\n",
    "                else:\n",
    "                    filt = np.zeros_like(filt) + 0.5\n",
    "            \n",
    "            ax.imshow(filt, cmap='gray')\n",
    "            ax.set_title(f'Filter {i+1}', fontsize=9)\n",
    "        \n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Conv-RBM-1 Learned Filters ({num_filters} of {weights.shape[0]})\\n'\n",
    "                 f'Kernel Size: {rbm.kernel_size}√ó{rbm.kernel_size}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print filter statistics\n",
    "    print(\"\\nFilter Statistics:\")\n",
    "    print(f\"  Total filters: {weights.shape[0]}\")\n",
    "    print(f\"  Kernel size: {rbm.kernel_size}√ó{rbm.kernel_size}\")\n",
    "    print(f\"  Weight range: [{weights.min():.4f}, {weights.max():.4f}]\")\n",
    "    print(f\"  Weight mean: {weights.mean():.4f}\")\n",
    "    print(f\"  Weight std: {weights.std():.4f}\")\n",
    "\n",
    "\n",
    "# Visualize first 16 filters (STEP D: Conditional visualization)\n",
    "if DEBUG:\n",
    "    print(\"Visualizing first 16 learned filters:\")\n",
    "    visualize_filters(conv_rbm_1, num_filters=16, figsize=(10, 10))\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping filter visualization (DEBUG=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce446ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all 32 filters (STEP D: Conditional visualization)\n",
    "if DEBUG:\n",
    "    print(\"\\nVisualizing all 32 learned filters:\")\n",
    "    visualize_filters(conv_rbm_1, num_filters=32, figsize=(12, 12))\n",
    "else:\n",
    "    # Print filter statistics without visualization\n",
    "    weights = conv_rbm_1.W.data.cpu().numpy()\n",
    "    print(\"\\nüìä Filter Statistics (DEBUG=False, plots disabled):\")\n",
    "    print(f\"  Total filters: {weights.shape[0]}\")\n",
    "    print(f\"  Kernel size: {conv_rbm_1.kernel_size}√ó{conv_rbm_1.kernel_size}\")\n",
    "    print(f\"  Weight range: [{weights.min():.4f}, {weights.max():.4f}]\")\n",
    "    print(f\"  Weight mean: {weights.mean():.4f}\")\n",
    "    print(f\"  Weight std: {weights.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c34f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize Hidden Activations for Sample Images\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_hidden_activations(\n",
    "    rbm: ConvRBM,\n",
    "    images: torch.Tensor,\n",
    "    num_images: int = 2,\n",
    "    num_feature_maps: int = 8,\n",
    "    figsize: tuple = (16, 8)\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize hidden layer activations for sample images.\n",
    "    \n",
    "    This shows what features the RBM detects in each input image.\n",
    "    \n",
    "    Args:\n",
    "        rbm: Trained ConvRBM\n",
    "        images: Input images tensor\n",
    "        num_images: Number of images to show\n",
    "        num_feature_maps: Number of feature maps to display per image\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    num_images = min(num_images, images.shape[0])\n",
    "    images = images[:num_images].to(config.DEVICE)\n",
    "    \n",
    "    # Get hidden activations\n",
    "    with torch.no_grad():\n",
    "        h_prob = rbm.hidden_probabilities(images)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(num_images, num_feature_maps + 1, figsize=figsize)\n",
    "    \n",
    "    for img_idx in range(num_images):\n",
    "        # Show original image\n",
    "        orig = images[img_idx].squeeze().cpu().numpy()\n",
    "        axes[img_idx, 0].imshow(orig, cmap='gray')\n",
    "        axes[img_idx, 0].set_title('Input' if img_idx == 0 else '', fontsize=10)\n",
    "        axes[img_idx, 0].axis('off')\n",
    "        \n",
    "        # Show feature map activations\n",
    "        for fm_idx in range(num_feature_maps):\n",
    "            activation = h_prob[img_idx, fm_idx].cpu().numpy()\n",
    "            axes[img_idx, fm_idx + 1].imshow(activation, cmap='viridis')\n",
    "            if img_idx == 0:\n",
    "                axes[img_idx, fm_idx + 1].set_title(f'FM {fm_idx+1}', fontsize=9)\n",
    "            axes[img_idx, fm_idx + 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Hidden Layer Activations (Feature Maps)\\n'\n",
    "                 'Brighter regions = stronger activations',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get sample images and visualize activations\n",
    "sample_images, _ = next(iter(train_loader))\n",
    "print(\"Visualizing hidden activations for sample images:\")\n",
    "visualize_hidden_activations(conv_rbm_1, sample_images, num_images=3, num_feature_maps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary: Conv-RBM-1 Training Results\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONV-RBM-1 PRETRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nArchitecture:\")\n",
    "print(f\"  ‚Ä¢ Input shape: [{config.BATCH_SIZE}, {config.NUM_CHANNELS}, {config.IMAGE_SIZE[0]}, {config.IMAGE_SIZE[1]}]\")\n",
    "print(f\"  ‚Ä¢ Hidden shape: [{config.BATCH_SIZE}, {CONVRBM1_CONFIG['hidden_channels']}, \"\n",
    "      f\"{config.IMAGE_SIZE[0] - CONVRBM1_CONFIG['kernel_size'] + 1}, \"\n",
    "      f\"{config.IMAGE_SIZE[1] - CONVRBM1_CONFIG['kernel_size'] + 1}]\")\n",
    "print(f\"  ‚Ä¢ Kernel size: {CONVRBM1_CONFIG['kernel_size']}√ó{CONVRBM1_CONFIG['kernel_size']}\")\n",
    "print(f\"  ‚Ä¢ Number of filters: {CONVRBM1_CONFIG['hidden_channels']}\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {CD_CONFIG['learning_rate']}\")\n",
    "print(f\"  ‚Ä¢ Gibbs steps (k): {CD_CONFIG['k']}\")\n",
    "print(f\"  ‚Ä¢ Momentum: {CD_CONFIG['momentum']}\")\n",
    "print(f\"  ‚Ä¢ Weight decay: {CD_CONFIG['weight_decay']}\")\n",
    "print(f\"  ‚Ä¢ Epochs: {CD_CONFIG['num_epochs']}\")\n",
    "\n",
    "print(\"\\nTraining Results:\")\n",
    "print(f\"  ‚Ä¢ Initial loss: {training_history['epoch_loss'][0]:.6f}\")\n",
    "print(f\"  ‚Ä¢ Final loss: {training_history['epoch_loss'][-1]:.6f}\")\n",
    "improvement = (1 - training_history['epoch_loss'][-1]/training_history['epoch_loss'][0]) * 100\n",
    "print(f\"  ‚Ä¢ Improvement: {improvement:.1f}%\")\n",
    "\n",
    "print(\"\\nLearned Weights Statistics:\")\n",
    "weights = conv_rbm_1.W.data.cpu().numpy()\n",
    "print(f\"  ‚Ä¢ Weight range: [{weights.min():.4f}, {weights.max():.4f}]\")\n",
    "print(f\"  ‚Ä¢ Weight mean: {weights.mean():.6f}\")\n",
    "print(f\"  ‚Ä¢ Weight std: {weights.std():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Conv-RBM-1 is ready for use in the CDBN stack!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27b92a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conv-RBM-1 Unsupervised Pretraining Completed\n",
    "\n",
    "‚úÖ **CDTrainer class** implemented with Contrastive Divergence (CD-k) algorithm\n",
    "\n",
    "‚úÖ **Unsupervised pretraining** completed on OCT training images\n",
    "- Labels were NOT used (purely unsupervised)\n",
    "- 10 epochs of CD-1 training\n",
    "- Manual parameter updates (no PyTorch optimizers)\n",
    "\n",
    "‚úÖ **Reconstruction quality** verified\n",
    "- Training set reconstructions visualized\n",
    "- Validation set reconstructions visualized  \n",
    "- MSE reconstruction loss tracked\n",
    "\n",
    "‚úÖ **Learned filters** visualized\n",
    "- 32 convolutional filters (7√ó7)\n",
    "- Filters show learned edge detectors and texture patterns\n",
    "\n",
    "‚úÖ **Hidden activations** visualized\n",
    "- Feature maps show what patterns the RBM detects\n",
    "- Spatially localized responses to input features\n",
    "\n",
    "---\n",
    "\n",
    "**Key Components Created:**\n",
    "- `CDTrainer` class: Implements CD-k with momentum and weight decay\n",
    "- `conv_rbm_1`: Trained Conv-RBM with 32 filters\n",
    "- `training_history`: Loss curves for analysis\n",
    "\n",
    "**Next Steps:**\n",
    "- STEP 9: Add probabilistic max-pooling layer\n",
    "- STEP 10: Stack second Conv-RBM layer\n",
    "- STEP 11: Build complete CDBN architecture\n",
    "- STEP 12: Fine-tune with supervised classifier for OCT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP E: Memory Cleanup After Conv-RBM-1 Training\n",
    "# =============================================================================\n",
    "# IMPORTANT: On Kaggle GPUs (T4/P100 with 16GB), we need to free memory\n",
    "# between training stages to prevent OOM errors.\n",
    "#\n",
    "# We delete the CD trainer (which holds velocity buffers) but KEEP:\n",
    "# - conv_rbm_1: The trained model (needed for inference)\n",
    "# - training_history: The loss history (for analysis)\n",
    "\n",
    "print(\"üßπ Memory Cleanup: Conv-RBM-1 Training Complete\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Delete the trainer (it holds large velocity tensors)\n",
    "del cd_trainer\n",
    "\n",
    "# Clear CUDA cache to free GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    # Report memory status\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "print(\"‚úì CD Trainer deleted, CUDA cache cleared\")\n",
    "print(\"‚úì conv_rbm_1 retained for next layers\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81730df1",
   "metadata": {},
   "source": [
    "## STEP 9 ‚Äî Probabilistic Pooling Layer\n",
    "\n",
    "Implement probabilistic pooling to reduce spatial dimensions while preserving probabilistic information.\n",
    "\n",
    "**Key Differences from Max Pooling:**\n",
    "- **Max pooling:** Takes the maximum value in each region (deterministic, loses probability information)\n",
    "- **Probabilistic pooling:** Computes the probability that at least one unit is active in the region\n",
    "\n",
    "**Probabilistic Pooling Rule:**\n",
    "For a 2√ó2 pooling region with hidden probabilities $p_1, p_2, p_3, p_4$:\n",
    "\n",
    "$$P(\\text{pool active}) = 1 - \\prod_{i=1}^{4}(1 - p_i) \\approx \\min(1, \\sum_{i=1}^{4} p_i)$$\n",
    "\n",
    "The sum approximation is used for computational efficiency, clipped to [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af8b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 9: Probabilistic Pooling Layer\n",
    "# =============================================================================\n",
    "\n",
    "class ProbabilisticPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Probabilistic Pooling Layer for Convolutional Deep Belief Networks.\n",
    "    \n",
    "    This layer performs spatial down-sampling of hidden activation probabilities\n",
    "    while preserving probabilistic semantics, unlike traditional max pooling.\n",
    "    \n",
    "    WHY PROBABILISTIC POOLING INSTEAD OF MAX POOLING?\n",
    "    -------------------------------------------------\n",
    "    \n",
    "    1. MAX POOLING (Traditional CNNs):\n",
    "       - Takes the maximum activation in each pooling region\n",
    "       - Loses information about other activations in the region\n",
    "       - Not probabilistically meaningful (max of probabilities ‚â† probability)\n",
    "       - Example: [0.9, 0.8, 0.7, 0.6] ‚Üí 0.9 (ignores other high activations)\n",
    "    \n",
    "    2. PROBABILISTIC POOLING (CDBNs):\n",
    "       - Computes the probability that at least one unit in the region is active\n",
    "       - Preserves the probabilistic interpretation of hidden units\n",
    "       - Exact formula: P(pool) = 1 - ‚àè(1 - p_i) for all p_i in region\n",
    "       - Approximation: P(pool) ‚âà min(1, Œ£p_i) (sum of probabilities, clipped)\n",
    "       - Example: [0.9, 0.8, 0.7, 0.6] ‚Üí min(1.0, 3.0) = 1.0 (high confidence)\n",
    "    \n",
    "    3. KEY DIFFERENCES:\n",
    "       - Max pooling: sparse, winner-take-all\n",
    "       - Probabilistic pooling: aggregative, preserves uncertainty\n",
    "       - Probabilistic pooling is more suitable for generative models (RBMs)\n",
    "       - Better gradient flow during fine-tuning\n",
    "    \n",
    "    Args:\n",
    "        pool_size (int): Size of the pooling window (default: 2 for 2√ó2 pooling)\n",
    "        mode (str): Pooling mode - 'sum' (default) or 'prob' (exact probabilistic)\n",
    "        \n",
    "    Input:\n",
    "        x: Hidden probabilities [batch, channels, H, W]\n",
    "        \n",
    "    Output:\n",
    "        pooled: Pooled probabilities [batch, channels, H//pool_size, W//pool_size]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pool_size: int = 2, mode: str = 'sum'):\n",
    "        super(ProbabilisticPooling, self).__init__()\n",
    "        \n",
    "        self.pool_size = pool_size\n",
    "        self.mode = mode\n",
    "        \n",
    "        # No learnable parameters in this layer\n",
    "        # Pooling is a fixed operation that aggregates probabilities\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply probabilistic pooling to input probabilities.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of probabilities [batch, channels, H, W]\n",
    "               Values should be in range [0, 1]\n",
    "               \n",
    "        Returns:\n",
    "            pooled: Pooled probabilities [batch, channels, H', W']\n",
    "                    where H' = H // pool_size, W' = W // pool_size\n",
    "        \"\"\"\n",
    "        if self.mode == 'sum':\n",
    "            # SUM-BASED APPROXIMATION (fast and effective)\n",
    "            # -----------------------------------------\n",
    "            # Sum the probabilities in each pooling region and clip to [0, 1]\n",
    "            # This approximates: P(at least one active) ‚âà sum(p_i)\n",
    "            # Valid when probabilities are small (sum << 1)\n",
    "            \n",
    "            # Use average pooling and multiply by pool_size^2 to get sum\n",
    "            # Then clip to [0, 1] range\n",
    "            pooled = nn.functional.avg_pool2d(x, kernel_size=self.pool_size)\n",
    "            pooled = pooled * (self.pool_size ** 2)  # Convert avg to sum\n",
    "            pooled = torch.clamp(pooled, 0.0, 1.0)   # Clip to valid probability range\n",
    "            \n",
    "        elif self.mode == 'prob':\n",
    "            # EXACT PROBABILISTIC POOLING\n",
    "            # ---------------------------\n",
    "            # P(pool active) = 1 - ‚àè(1 - p_i) for all p_i in the pooling region\n",
    "            # This is the exact probability that at least one unit is active\n",
    "            \n",
    "            batch, channels, H, W = x.shape\n",
    "            H_out = H // self.pool_size\n",
    "            W_out = W // self.pool_size\n",
    "            \n",
    "            # Reshape to extract pooling regions\n",
    "            # [batch, channels, H, W] -> [batch, channels, H_out, pool_size, W_out, pool_size]\n",
    "            x_reshaped = x.view(batch, channels, H_out, self.pool_size, W_out, self.pool_size)\n",
    "            \n",
    "            # Compute (1 - p_i) for each element\n",
    "            one_minus_p = 1.0 - x_reshaped\n",
    "            \n",
    "            # Product over the pooling region dimensions (dim 3 and 5)\n",
    "            # ‚àè(1 - p_i) over all pool_size √ó pool_size elements\n",
    "            prod_term = one_minus_p.prod(dim=3).prod(dim=-1)  # [batch, channels, H_out, W_out]\n",
    "            \n",
    "            # P(at least one active) = 1 - ‚àè(1 - p_i)\n",
    "            pooled = 1.0 - prod_term\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling mode: {self.mode}. Use 'sum' or 'prob'.\")\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def get_output_size(self, input_size: tuple) -> tuple:\n",
    "        \"\"\"\n",
    "        Calculate output spatial dimensions after pooling.\n",
    "        \n",
    "        Args:\n",
    "            input_size: (H, W) of input feature maps\n",
    "            \n",
    "        Returns:\n",
    "            output_size: (H // pool_size, W // pool_size)\n",
    "        \"\"\"\n",
    "        H, W = input_size\n",
    "        return (H // self.pool_size, W // self.pool_size)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ProbabilisticPooling(pool_size={self.pool_size}, mode='{self.mode}')\"\n",
    "\n",
    "\n",
    "print(\"‚úì ProbabilisticPooling class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377868e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Test Probabilistic Pooling\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBABILISTIC POOLING VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create pooling layer\n",
    "prob_pool = ProbabilisticPooling(pool_size=2, mode='sum')\n",
    "print(f\"\\nPooling Layer: {prob_pool}\")\n",
    "\n",
    "# Test with sample hidden activations from Conv-RBM-1\n",
    "with torch.no_grad():\n",
    "    # Get a batch of images\n",
    "    test_images, _ = next(iter(train_loader))\n",
    "    test_images = test_images.to(config.DEVICE)\n",
    "    \n",
    "    # Get hidden probabilities from Conv-RBM-1\n",
    "    h1_prob = conv_rbm_1.hidden_probabilities(test_images)\n",
    "    \n",
    "    # Apply probabilistic pooling\n",
    "    h1_pooled = prob_pool(h1_prob)\n",
    "\n",
    "print(f\"\\nShape Flow:\")\n",
    "print(f\"  Input images        : {test_images.shape}\")\n",
    "print(f\"  Conv-RBM-1 hidden   : {h1_prob.shape}\")\n",
    "print(f\"  After pooling (2√ó2) : {h1_pooled.shape}\")\n",
    "\n",
    "# Verify dimensions\n",
    "expected_pooled_h = h1_prob.shape[2] // 2\n",
    "expected_pooled_w = h1_prob.shape[3] // 2\n",
    "assert h1_pooled.shape == (test_images.shape[0], 32, expected_pooled_h, expected_pooled_w), \\\n",
    "    \"Pooling output shape mismatch!\"\n",
    "print(f\"\\n‚úì Pooling shape verification passed!\")\n",
    "\n",
    "# Check value ranges\n",
    "print(f\"\\nValue Statistics:\")\n",
    "print(f\"  Hidden probs range  : [{h1_prob.min().item():.4f}, {h1_prob.max().item():.4f}]\")\n",
    "print(f\"  Pooled probs range  : [{h1_pooled.min().item():.4f}, {h1_pooled.max().item():.4f}]\")\n",
    "print(f\"  Pooled values in [0,1]: {(h1_pooled >= 0).all() and (h1_pooled <= 1).all()}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cb9ce",
   "metadata": {},
   "source": [
    "## STEP 10 ‚Äî Freeze Conv-RBM-1 and Prepare Pooled Features\n",
    "\n",
    "Freeze Conv-RBM-1 parameters and compute pooled hidden features for training Conv-RBM-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 10: Freeze Conv-RBM-1 and Prepare Pooled Features\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 10.1 Freeze Conv-RBM-1 Parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "# After unsupervised pretraining, we freeze the first layer to preserve\n",
    "# the learned features while training subsequent layers.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FREEZING CONV-RBM-1 AND PREPARING FEATURES FOR CONV-RBM-2\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Freeze all parameters in Conv-RBM-1\n",
    "for param in conv_rbm_1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\n‚úì Conv-RBM-1 parameters frozen:\")\n",
    "for name, param in conv_rbm_1.named_parameters():\n",
    "    print(f\"  {name}: requires_grad = {param.requires_grad}\")\n",
    "\n",
    "# Set to evaluation mode (affects dropout/batchnorm if any, not in our case)\n",
    "conv_rbm_1.eval()\n",
    "print(\"\\n‚úì Conv-RBM-1 set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85cbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 10.2 Create Feature Extraction Pipeline\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts pooled hidden features from Conv-RBM-1 for training Conv-RBM-2.\n",
    "    \n",
    "    Pipeline: Input Image ‚Üí Conv-RBM-1 ‚Üí Hidden Probabilities ‚Üí Pooling ‚Üí Features\n",
    "    \n",
    "    This class can either:\n",
    "    1. Compute features on-the-fly (memory efficient, slower)\n",
    "    2. Precompute all features (fast training, higher memory)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_rbm: ConvRBM,\n",
    "        pooling: ProbabilisticPooling,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.conv_rbm = conv_rbm\n",
    "        self.pooling = pooling\n",
    "        self.device = device\n",
    "        \n",
    "        # Ensure RBM is in eval mode\n",
    "        self.conv_rbm.eval()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract pooled features from a batch of images.\n",
    "        \n",
    "        Args:\n",
    "            images: Input images [batch, 1, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            pooled_features: Pooled hidden probabilities [batch, hidden_channels, H', W']\n",
    "        \"\"\"\n",
    "        images = images.to(self.device)\n",
    "        \n",
    "        # Step 1: Get hidden probabilities from Conv-RBM-1\n",
    "        h_prob = self.conv_rbm.hidden_probabilities(images)\n",
    "        \n",
    "        # Step 2: Apply probabilistic pooling\n",
    "        h_pooled = self.pooling(h_prob)\n",
    "        \n",
    "        return h_pooled\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def precompute_all_features(self, data_loader: DataLoader) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Precompute pooled features for the entire dataset.\n",
    "        \n",
    "        This is faster for training but requires more memory.\n",
    "        \n",
    "        Args:\n",
    "            data_loader: DataLoader for the dataset\n",
    "            \n",
    "        Returns:\n",
    "            all_features: Tensor of all pooled features [N, channels, H', W']\n",
    "        \"\"\"\n",
    "        all_features = []\n",
    "        \n",
    "        print(\"Precomputing pooled features...\")\n",
    "        for batch_idx, (images, _) in enumerate(data_loader):\n",
    "            features = self.extract_features(images)\n",
    "            all_features.append(features.cpu())\n",
    "            \n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"  Processed {batch_idx + 1}/{len(data_loader)} batches\")\n",
    "        \n",
    "        all_features = torch.cat(all_features, dim=0)\n",
    "        print(f\"‚úì Precomputed {all_features.shape[0]} feature maps\")\n",
    "        \n",
    "        return all_features\n",
    "\n",
    "\n",
    "# Create feature extractor\n",
    "feature_extractor = FeatureExtractor(\n",
    "    conv_rbm=conv_rbm_1,\n",
    "    pooling=prob_pool,\n",
    "    device=config.DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì FeatureExtractor created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 10.3 Compute Pooled Feature Dimensions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Calculate the dimensions for Conv-RBM-2 input\n",
    "with torch.no_grad():\n",
    "    sample_images, _ = next(iter(train_loader))\n",
    "    sample_images = sample_images.to(config.DEVICE)\n",
    "    \n",
    "    # Pass through Conv-RBM-1\n",
    "    sample_h1 = conv_rbm_1.hidden_probabilities(sample_images)\n",
    "    \n",
    "    # Pass through pooling\n",
    "    sample_pooled = prob_pool(sample_h1)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"FEATURE DIMENSIONS FOR CONV-RBM-2\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nInput to Conv-RBM-1:\")\n",
    "print(f\"  Shape: {sample_images.shape}\")\n",
    "print(f\"  [batch={sample_images.shape[0]}, channels={sample_images.shape[1]}, \"\n",
    "      f\"H={sample_images.shape[2]}, W={sample_images.shape[3]}]\")\n",
    "\n",
    "print(f\"\\nConv-RBM-1 Hidden (before pooling):\")\n",
    "print(f\"  Shape: {sample_h1.shape}\")\n",
    "print(f\"  [batch={sample_h1.shape[0]}, feature_maps={sample_h1.shape[1]}, \"\n",
    "      f\"H={sample_h1.shape[2]}, W={sample_h1.shape[3]}]\")\n",
    "\n",
    "print(f\"\\nPooled Features (input to Conv-RBM-2):\")\n",
    "print(f\"  Shape: {sample_pooled.shape}\")\n",
    "print(f\"  [batch={sample_pooled.shape[0]}, feature_maps={sample_pooled.shape[1]}, \"\n",
    "      f\"H={sample_pooled.shape[2]}, W={sample_pooled.shape[3]}]\")\n",
    "\n",
    "# Store dimensions for Conv-RBM-2 configuration\n",
    "POOLED_CHANNELS = sample_pooled.shape[1]\n",
    "POOLED_HEIGHT = sample_pooled.shape[2]\n",
    "POOLED_WIDTH = sample_pooled.shape[3]\n",
    "\n",
    "print(f\"\\n‚úì Conv-RBM-2 will receive {POOLED_CHANNELS} input channels\")\n",
    "print(f\"‚úì Spatial dimensions: {POOLED_HEIGHT}√ó{POOLED_WIDTH}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44b289",
   "metadata": {},
   "source": [
    "## STEP 11 ‚Äî Conv-RBM-2 Module\n",
    "\n",
    "Create the second Conv-RBM layer that learns higher-level features from the pooled representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7140fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 11: Conv-RBM-2 Module\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Conv-RBM-2 Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "CONVRBM2_CONFIG = {\n",
    "    'visible_channels': POOLED_CHANNELS,  # 32 (from Conv-RBM-1 pooled output)\n",
    "    'hidden_channels': 64,                 # More feature maps for higher-level features\n",
    "    'kernel_size': 5,                      # 5√ó5 kernels (smaller than layer 1)\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONV-RBM-2 CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nArchitecture:\")\n",
    "for key, value in CONVRBM2_CONFIG.items():\n",
    "    print(f\"  {key:<20}: {value}\")\n",
    "\n",
    "# Calculate output dimensions\n",
    "conv_rbm2_output_h = POOLED_HEIGHT - CONVRBM2_CONFIG['kernel_size'] + 1\n",
    "conv_rbm2_output_w = POOLED_WIDTH - CONVRBM2_CONFIG['kernel_size'] + 1\n",
    "\n",
    "print(f\"\\nExpected output dimensions:\")\n",
    "print(f\"  Input  : [batch, {CONVRBM2_CONFIG['visible_channels']}, {POOLED_HEIGHT}, {POOLED_WIDTH}]\")\n",
    "print(f\"  Output : [batch, {CONVRBM2_CONFIG['hidden_channels']}, {conv_rbm2_output_h}, {conv_rbm2_output_w}]\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1084d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate Conv-RBM-2\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Reuse the same ConvRBM class defined earlier\n",
    "conv_rbm_2 = ConvRBM(\n",
    "    visible_channels=CONVRBM2_CONFIG['visible_channels'],\n",
    "    hidden_channels=CONVRBM2_CONFIG['hidden_channels'],\n",
    "    kernel_size=CONVRBM2_CONFIG['kernel_size']\n",
    ").to(config.DEVICE)\n",
    "\n",
    "print(\"Conv-RBM-2 Architecture:\")\n",
    "print(conv_rbm_2)\n",
    "\n",
    "# Print parameter count\n",
    "total_params_rbm2 = sum(p.numel() for p in conv_rbm_2.parameters())\n",
    "print(f\"\\nTotal parameters in Conv-RBM-2: {total_params_rbm2:,}\")\n",
    "\n",
    "# Verify with a forward pass\n",
    "with torch.no_grad():\n",
    "    test_h2_prob, test_h2_sample = conv_rbm_2(sample_pooled)\n",
    "\n",
    "print(f\"\\nVerification forward pass:\")\n",
    "print(f\"  Input  : {sample_pooled.shape}\")\n",
    "print(f\"  Output : {test_h2_prob.shape}\")\n",
    "\n",
    "expected_shape = (sample_pooled.shape[0], CONVRBM2_CONFIG['hidden_channels'], \n",
    "                  conv_rbm2_output_h, conv_rbm2_output_w)\n",
    "assert test_h2_prob.shape == expected_shape, f\"Shape mismatch: {test_h2_prob.shape} vs {expected_shape}\"\n",
    "print(f\"‚úì Forward pass shape verification passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a245bb",
   "metadata": {},
   "source": [
    "## STEP 12 ‚Äî Unsupervised Pretraining of Conv-RBM-2\n",
    "\n",
    "Train Conv-RBM-2 using CD-k on the pooled features from Conv-RBM-1.\n",
    "\n",
    "**Training Pipeline:**\n",
    "```\n",
    "Image ‚Üí Conv-RBM-1 (frozen) ‚Üí Pool ‚Üí Conv-RBM-2 (train) ‚Üí Hidden Features\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 12: Unsupervised Pretraining of Conv-RBM-2\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CD Training Configuration for Conv-RBM-2\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "CD2_CONFIG = {\n",
    "    'learning_rate': 0.005,    # Slightly lower LR for second layer\n",
    "    'k': 1,                    # CD-1\n",
    "    'momentum': 0.5,           # Momentum\n",
    "    'weight_decay': 0.0001,    # L2 regularization\n",
    "    'num_epochs': 5,           # Kaggle: reduced from 10 for time limits\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONV-RBM-2 TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in CD2_CONFIG.items():\n",
    "    print(f\"  {key:<20}: {value}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize CD Trainer for Conv-RBM-2\n",
    "cd_trainer_2 = CDTrainer(\n",
    "    rbm=conv_rbm_2,\n",
    "    learning_rate=CD2_CONFIG['learning_rate'],\n",
    "    k=CD2_CONFIG['k'],\n",
    "    device=config.DEVICE,\n",
    "    momentum=CD2_CONFIG['momentum'],\n",
    "    weight_decay=CD2_CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì CD Trainer for Conv-RBM-2 initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training Loop for Conv-RBM-2 (Memory-Optimized for Local 24GB GPU)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_convrbm2(\n",
    "    conv_rbm_1: ConvRBM,\n",
    "    conv_rbm_2: ConvRBM,\n",
    "    pooling: ProbabilisticPooling,\n",
    "    trainer: CDTrainer,\n",
    "    train_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    device: torch.device,\n",
    "    memory_cleanup_freq: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train Conv-RBM-2 on pooled features from Conv-RBM-1 (Memory-Optimized).\n",
    "    \n",
    "    Pipeline per batch:\n",
    "        1. Get images from dataloader (ignore labels - unsupervised)\n",
    "        2. Extract hidden probabilities from frozen Conv-RBM-1\n",
    "        3. Apply probabilistic pooling\n",
    "        4. Train Conv-RBM-2 on pooled features using CD-k\n",
    "    \n",
    "    Args:\n",
    "        conv_rbm_1: Pretrained and frozen Conv-RBM-1\n",
    "        conv_rbm_2: Conv-RBM-2 to train\n",
    "        pooling: ProbabilisticPooling layer\n",
    "        trainer: CDTrainer for Conv-RBM-2\n",
    "        train_loader: DataLoader for training images\n",
    "        num_epochs: Number of training epochs\n",
    "        device: Computation device\n",
    "        memory_cleanup_freq: Frequency of memory cleanup (batches)\n",
    "        \n",
    "    Returns:\n",
    "        history: Training history dictionary\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'epoch_loss': [],\n",
    "        'batch_losses': [],\n",
    "    }\n",
    "    \n",
    "    # Ensure Conv-RBM-1 is frozen and in eval mode\n",
    "    conv_rbm_1.eval()\n",
    "    for param in conv_rbm_1.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING CONV-RBM-2 UNSUPERVISED PRETRAINING (Memory-Optimized)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training on pooled features from {len(train_loader.dataset)} images\")\n",
    "    print(f\"Batch size: {train_loader.batch_size}\")\n",
    "    print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "    print(f\"Memory cleanup frequency: every {memory_cleanup_freq} batches\")\n",
    "    if torch.cuda.is_available():\n",
    "        print_gpu_memory()\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # -------------------------------------------------------------\n",
    "            # Step 1: Extract pooled features from Conv-RBM-1 (frozen)\n",
    "            # -------------------------------------------------------------\n",
    "            with torch.no_grad():\n",
    "                # Get hidden probabilities from Conv-RBM-1\n",
    "                h1_prob = conv_rbm_1.hidden_probabilities(images)\n",
    "                \n",
    "                # Apply probabilistic pooling\n",
    "                h1_pooled = pooling(h1_prob)\n",
    "            \n",
    "            # -------------------------------------------------------------\n",
    "            # Step 2: Train Conv-RBM-2 on pooled features\n",
    "            # -------------------------------------------------------------\n",
    "            # h1_pooled is now the \"visible\" input to Conv-RBM-2\n",
    "            batch_loss = trainer.train_batch(h1_pooled)\n",
    "            epoch_losses.append(batch_loss)\n",
    "            \n",
    "            # Explicit memory cleanup\n",
    "            del images, h1_prob, h1_pooled\n",
    "            \n",
    "            # Progress update\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{num_epochs} | \"\n",
    "                      f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                      f\"Loss: {batch_loss:.6f}\")\n",
    "            \n",
    "            # Aggressive memory cleanup for large datasets\n",
    "            if torch.cuda.is_available() and (batch_idx + 1) % memory_cleanup_freq == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Epoch statistics\n",
    "        avg_epoch_loss = np.mean(epoch_losses)\n",
    "        history['epoch_loss'].append(avg_epoch_loss)\n",
    "        history['batch_losses'].extend(epoch_losses)\n",
    "        \n",
    "        # Clear batch losses to free memory\n",
    "        epoch_losses.clear()\n",
    "        \n",
    "        # Print epoch summary with GPU memory status\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "              f\"Avg Loss: {avg_epoch_loss:.6f} | \"\n",
    "              f\"Time: {epoch_time:.1f}s\", end=\"\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            mem_alloc = torch.cuda.memory_allocated() / 1024**3\n",
    "            print(f\" | GPU: {mem_alloc:.2f} GB\")\n",
    "        else:\n",
    "            print()\n",
    "        \n",
    "        # Clear cache after each epoch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(\"CONV-RBM-2 PRETRAINING COMPLETED!\")\n",
    "    print(f\"Final reconstruction loss: {history['epoch_loss'][-1]:.6f}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print_gpu_memory()\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Train Conv-RBM-2\n",
    "print(\"Starting Conv-RBM-2 pretraining on pooled features...\")\n",
    "training_history_2 = train_convrbm2(\n",
    "    conv_rbm_1=conv_rbm_1,\n",
    "    conv_rbm_2=conv_rbm_2,\n",
    "    pooling=prob_pool,\n",
    "    trainer=cd_trainer_2,\n",
    "    train_loader=train_loader,\n",
    "    num_epochs=CD2_CONFIG['num_epochs'],\n",
    "    device=config.DEVICE,\n",
    "    memory_cleanup_freq=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Plot Conv-RBM-2 Training Progress\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def plot_training_comparison(history1: dict, history2: dict, figsize=(14, 5)):\n",
    "    \"\"\"\n",
    "    Compare training progress of Conv-RBM-1 and Conv-RBM-2.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: Conv-RBM-2 training loss\n",
    "    ax1 = axes[0]\n",
    "    epochs = range(1, len(history2['epoch_loss']) + 1)\n",
    "    ax1.plot(epochs, history2['epoch_loss'], 'g-o', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Reconstruction Loss (MSE)', fontsize=12)\n",
    "    ax1.set_title('Conv-RBM-2 Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(epochs)\n",
    "    \n",
    "    # Plot 2: Comparison of both layers\n",
    "    ax2 = axes[1]\n",
    "    epochs1 = range(1, len(history1['epoch_loss']) + 1)\n",
    "    epochs2 = range(1, len(history2['epoch_loss']) + 1)\n",
    "    ax2.plot(epochs1, history1['epoch_loss'], 'b-o', linewidth=2, markersize=6, label='Conv-RBM-1')\n",
    "    ax2.plot(epochs2, history2['epoch_loss'], 'g-s', linewidth=2, markersize=6, label='Conv-RBM-2')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Reconstruction Loss (MSE)', fontsize=12)\n",
    "    ax2.set_title('Training Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.suptitle('CDBN Layer-wise Pretraining Progress', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\nTraining Comparison:\")\n",
    "    print(f\"  Conv-RBM-1: {history1['epoch_loss'][0]:.6f} ‚Üí {history1['epoch_loss'][-1]:.6f} \"\n",
    "          f\"({(1 - history1['epoch_loss'][-1]/history1['epoch_loss'][0])*100:.1f}% improvement)\")\n",
    "    print(f\"  Conv-RBM-2: {history2['epoch_loss'][0]:.6f} ‚Üí {history2['epoch_loss'][-1]:.6f} \"\n",
    "          f\"({(1 - history2['epoch_loss'][-1]/history2['epoch_loss'][0])*100:.1f}% improvement)\")\n",
    "\n",
    "\n",
    "# Plot training comparison\n",
    "plot_training_comparison(training_history, training_history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP E: Memory Cleanup After Conv-RBM-2 Pretraining (Kaggle)\n",
    "# =============================================================================\n",
    "# Free GPU memory by deleting the CD trainer (holds large velocity tensors)\n",
    "# This is critical for Kaggle's 16GB GPU memory limit\n",
    "\n",
    "# Delete the Conv-RBM-2 trainer\n",
    "del cd_trainer_2\n",
    "\n",
    "# Clear CUDA cache to release fragmented memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"‚úì Conv-RBM-2 trainer deleted, CUDA cache cleared\")\n",
    "    print(f\"  GPU Memory ‚Äî Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚úì Conv-RBM-2 trainer deleted (CPU mode)\")\n",
    "\n",
    "print(\"  Conv-RBM-2 weights are preserved in conv_rbm_2 module\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e443ad8",
   "metadata": {},
   "source": [
    "## STEP 13 ‚Äî Visualization\n",
    "\n",
    "### 13.1 Conv-RBM-2 Learned Filters\n",
    "Visualize the 64 learned filters (5√ó5 kernels) from Conv-RBM-2.\n",
    "\n",
    "Note: Each filter now has 32 input channels (from Conv-RBM-1 pooled features), so we visualize the filter norms or individual channel slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957fcbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 13.1: Conv-RBM-2 Learned Filters Visualization\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_multilayer_filters(\n",
    "    rbm: ConvRBM,\n",
    "    num_filters: int = 16,\n",
    "    num_channels_to_show: int = 4,\n",
    "    figsize: tuple = (14, 10),\n",
    "    layer_name: str = \"Conv-RBM-2\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize filters from a multi-channel Conv-RBM.\n",
    "    \n",
    "    For Conv-RBM-2, each filter has shape [visible_channels, kernel_h, kernel_w].\n",
    "    We show multiple visualizations:\n",
    "    1. Filter energy (L2 norm across input channels)\n",
    "    2. Individual channel slices for selected filters\n",
    "    \n",
    "    Args:\n",
    "        rbm: Trained ConvRBM\n",
    "        num_filters: Number of filters to display\n",
    "        num_channels_to_show: Number of input channel slices to show per filter\n",
    "        figsize: Figure size\n",
    "        layer_name: Name for the title\n",
    "    \"\"\"\n",
    "    # Get weights: [hidden_channels, visible_channels, kernel_h, kernel_w]\n",
    "    weights = rbm.W.data.cpu().numpy()\n",
    "    \n",
    "    num_filters = min(num_filters, weights.shape[0])\n",
    "    num_channels = weights.shape[1]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Part 1: Filter Energy Visualization (L2 norm across input channels)\n",
    "    # -------------------------------------------------------------------------\n",
    "    grid_size = int(np.ceil(np.sqrt(num_filters)))\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(len(axes)):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if i < num_filters:\n",
    "            # Compute L2 norm across input channels: sqrt(sum over channels of W^2)\n",
    "            # This gives a single 2D visualization per filter\n",
    "            filter_energy = np.sqrt(np.sum(weights[i] ** 2, axis=0))\n",
    "            \n",
    "            # Normalize for visualization\n",
    "            fe_min, fe_max = filter_energy.min(), filter_energy.max()\n",
    "            if fe_max - fe_min > 1e-8:\n",
    "                filter_energy = (filter_energy - fe_min) / (fe_max - fe_min)\n",
    "            \n",
    "            ax.imshow(filter_energy, cmap='viridis')\n",
    "            ax.set_title(f'F{i+1}', fontsize=9)\n",
    "        \n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{layer_name} Filter Energies (L2 norm across {num_channels} input channels)\\n'\n",
    "                 f'Kernel Size: {rbm.kernel_size}√ó{rbm.kernel_size}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Part 2: Individual Channel Slices for Top Filters\n",
    "    # -------------------------------------------------------------------------\n",
    "    num_filters_detail = min(8, num_filters)\n",
    "    num_ch = min(num_channels_to_show, num_channels)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_filters_detail, num_ch + 1, figsize=figsize)\n",
    "    \n",
    "    for f_idx in range(num_filters_detail):\n",
    "        # First column: filter energy\n",
    "        filter_energy = np.sqrt(np.sum(weights[f_idx] ** 2, axis=0))\n",
    "        fe_norm = (filter_energy - filter_energy.min()) / (filter_energy.max() - filter_energy.min() + 1e-8)\n",
    "        axes[f_idx, 0].imshow(fe_norm, cmap='viridis')\n",
    "        axes[f_idx, 0].set_title('Energy' if f_idx == 0 else '', fontsize=9)\n",
    "        axes[f_idx, 0].set_ylabel(f'F{f_idx+1}', fontsize=10)\n",
    "        axes[f_idx, 0].axis('off')\n",
    "        \n",
    "        # Remaining columns: individual input channel slices\n",
    "        for ch_idx in range(num_ch):\n",
    "            channel_slice = weights[f_idx, ch_idx]\n",
    "            cs_norm = (channel_slice - channel_slice.min()) / (channel_slice.max() - channel_slice.min() + 1e-8)\n",
    "            axes[f_idx, ch_idx + 1].imshow(cs_norm, cmap='gray')\n",
    "            if f_idx == 0:\n",
    "                axes[f_idx, ch_idx + 1].set_title(f'Ch{ch_idx+1}', fontsize=9)\n",
    "            axes[f_idx, ch_idx + 1].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{layer_name} Filters: Energy + Individual Channel Slices\\n'\n",
    "                 f'(Showing first {num_ch} of {num_channels} input channels)',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{layer_name} Filter Statistics:\")\n",
    "    print(f\"  Total filters      : {weights.shape[0]}\")\n",
    "    print(f\"  Input channels     : {weights.shape[1]}\")\n",
    "    print(f\"  Kernel size        : {rbm.kernel_size}√ó{rbm.kernel_size}\")\n",
    "    print(f\"  Weight range       : [{weights.min():.4f}, {weights.max():.4f}]\")\n",
    "    print(f\"  Weight mean        : {weights.mean():.6f}\")\n",
    "    print(f\"  Weight std         : {weights.std():.6f}\")\n",
    "\n",
    "\n",
    "# Visualize Conv-RBM-2 filters\n",
    "if DEBUG:\n",
    "    print(\"Visualizing Conv-RBM-2 learned filters:\")\n",
    "    visualize_multilayer_filters(conv_rbm_2, num_filters=16, num_channels_to_show=6)\n",
    "else:\n",
    "    print(\"‚úì Conv-RBM-2 visualization skipped (DEBUG=False for Kaggle)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0072f24",
   "metadata": {},
   "source": [
    "### 13.2 Conv-RBM-2 Hidden Feature Maps\n",
    "Visualize the hierarchical features learned by the 2-layer CDBN stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e30217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 13.2: Hierarchical Feature Visualization\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_hierarchical_features(\n",
    "    images: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    conv_rbm_1: ConvRBM,\n",
    "    conv_rbm_2: ConvRBM,\n",
    "    pooling: ProbabilisticPooling,\n",
    "    class_names: list,\n",
    "    num_images: int = 4,\n",
    "    num_feature_maps: int = 6,\n",
    "    figsize: tuple = (18, 16)\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize the hierarchical feature extraction through the CDBN.\n",
    "    \n",
    "    Shows: Input ‚Üí Conv-RBM-1 Features ‚Üí Pooled ‚Üí Conv-RBM-2 Features\n",
    "    \n",
    "    Args:\n",
    "        images: Input images tensor\n",
    "        labels: Class labels\n",
    "        conv_rbm_1: First Conv-RBM layer\n",
    "        conv_rbm_2: Second Conv-RBM layer\n",
    "        pooling: Probabilistic pooling layer\n",
    "        class_names: List of class names\n",
    "        num_images: Number of images to display\n",
    "        num_feature_maps: Number of feature maps per layer\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    num_images = min(num_images, images.shape[0])\n",
    "    images = images[:num_images].to(config.DEVICE)\n",
    "    labels = labels[:num_images]\n",
    "    \n",
    "    # Forward pass through the CDBN stack\n",
    "    with torch.no_grad():\n",
    "        # Layer 1: Input ‚Üí Hidden\n",
    "        h1_prob = conv_rbm_1.hidden_probabilities(images)\n",
    "        \n",
    "        # Pooling\n",
    "        h1_pooled = pooling(h1_prob)\n",
    "        \n",
    "        # Layer 2: Pooled ‚Üí Hidden\n",
    "        h2_prob = conv_rbm_2.hidden_probabilities(h1_pooled)\n",
    "    \n",
    "    # Create visualization\n",
    "    num_cols = 1 + num_feature_maps + num_feature_maps + num_feature_maps  # Input + L1 + Pooled + L2\n",
    "    fig, axes = plt.subplots(num_images, 4, figsize=figsize)\n",
    "    \n",
    "    for img_idx in range(num_images):\n",
    "        # Column 1: Original image\n",
    "        orig = images[img_idx].squeeze().cpu().numpy()\n",
    "        axes[img_idx, 0].imshow(orig, cmap='gray')\n",
    "        axes[img_idx, 0].set_title(f'{class_names[labels[img_idx]]}' if img_idx == 0 else '', fontsize=10)\n",
    "        if img_idx == 0:\n",
    "            axes[img_idx, 0].set_title('Input\\n' + class_names[labels[img_idx]], fontsize=10)\n",
    "        axes[img_idx, 0].set_ylabel(f'Img {img_idx+1}', fontsize=10)\n",
    "        axes[img_idx, 0].axis('off')\n",
    "        \n",
    "        # Column 2: Conv-RBM-1 feature map montage\n",
    "        h1_montage = create_feature_montage(h1_prob[img_idx].cpu().numpy(), num_feature_maps)\n",
    "        axes[img_idx, 1].imshow(h1_montage, cmap='viridis')\n",
    "        if img_idx == 0:\n",
    "            axes[img_idx, 1].set_title(f'Conv-RBM-1\\n({h1_prob.shape[2]}√ó{h1_prob.shape[3]})', fontsize=10)\n",
    "        axes[img_idx, 1].axis('off')\n",
    "        \n",
    "        # Column 3: Pooled feature map montage\n",
    "        pooled_montage = create_feature_montage(h1_pooled[img_idx].cpu().numpy(), num_feature_maps)\n",
    "        axes[img_idx, 2].imshow(pooled_montage, cmap='viridis')\n",
    "        if img_idx == 0:\n",
    "            axes[img_idx, 2].set_title(f'Pooled\\n({h1_pooled.shape[2]}√ó{h1_pooled.shape[3]})', fontsize=10)\n",
    "        axes[img_idx, 2].axis('off')\n",
    "        \n",
    "        # Column 4: Conv-RBM-2 feature map montage\n",
    "        h2_montage = create_feature_montage(h2_prob[img_idx].cpu().numpy(), num_feature_maps)\n",
    "        axes[img_idx, 3].imshow(h2_montage, cmap='viridis')\n",
    "        if img_idx == 0:\n",
    "            axes[img_idx, 3].set_title(f'Conv-RBM-2\\n({h2_prob.shape[2]}√ó{h2_prob.shape[3]})', fontsize=10)\n",
    "        axes[img_idx, 3].axis('off')\n",
    "    \n",
    "    plt.suptitle('Hierarchical Feature Extraction through 2-Layer CDBN\\n'\n",
    "                 'Input ‚Üí Conv-RBM-1 ‚Üí Pooling ‚Üí Conv-RBM-2',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_feature_montage(feature_maps: np.ndarray, num_maps: int = 6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a montage of feature maps for visualization.\n",
    "    \n",
    "    Args:\n",
    "        feature_maps: [channels, H, W] array\n",
    "        num_maps: Number of maps to include\n",
    "        \n",
    "    Returns:\n",
    "        montage: Combined image array\n",
    "    \"\"\"\n",
    "    num_maps = min(num_maps, feature_maps.shape[0])\n",
    "    \n",
    "    # Arrange in a 2-row grid\n",
    "    rows = 2\n",
    "    cols = (num_maps + 1) // 2\n",
    "    \n",
    "    h, w = feature_maps.shape[1], feature_maps.shape[2]\n",
    "    montage = np.zeros((rows * h, cols * w))\n",
    "    \n",
    "    for i in range(num_maps):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        fm = feature_maps[i]\n",
    "        # Normalize\n",
    "        fm = (fm - fm.min()) / (fm.max() - fm.min() + 1e-8)\n",
    "        montage[row*h:(row+1)*h, col*w:(col+1)*w] = fm\n",
    "    \n",
    "    return montage\n",
    "\n",
    "\n",
    "# Get sample images and visualize\n",
    "sample_images, sample_labels = next(iter(train_loader))\n",
    "if DEBUG:\n",
    "    print(\"Visualizing hierarchical feature extraction:\")\n",
    "    visualize_hierarchical_features(\n",
    "        images=sample_images,\n",
    "        labels=sample_labels,\n",
    "        conv_rbm_1=conv_rbm_1,\n",
    "        conv_rbm_2=conv_rbm_2,\n",
    "        pooling=prob_pool,\n",
    "        class_names=config.CLASS_NAMES,\n",
    "        num_images=4,\n",
    "        num_feature_maps=6\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úì Hierarchical feature visualization skipped (DEBUG=False for Kaggle)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Detailed Feature Map Visualization for Conv-RBM-2\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def visualize_layer2_activations(\n",
    "    images: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    conv_rbm_1: ConvRBM,\n",
    "    conv_rbm_2: ConvRBM,\n",
    "    pooling: ProbabilisticPooling,\n",
    "    class_names: list,\n",
    "    num_images: int = 2,\n",
    "    num_feature_maps: int = 16,\n",
    "    figsize: tuple = (20, 8)\n",
    "):\n",
    "    \"\"\"\n",
    "    Detailed visualization of Conv-RBM-2 feature maps.\n",
    "    \"\"\"\n",
    "    num_images = min(num_images, images.shape[0])\n",
    "    images = images[:num_images].to(config.DEVICE)\n",
    "    labels = labels[:num_images]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        h1_prob = conv_rbm_1.hidden_probabilities(images)\n",
    "        h1_pooled = pooling(h1_prob)\n",
    "        h2_prob = conv_rbm_2.hidden_probabilities(h1_pooled)\n",
    "    \n",
    "    num_feature_maps = min(num_feature_maps, h2_prob.shape[1])\n",
    "    grid_cols = int(np.ceil(np.sqrt(num_feature_maps)))\n",
    "    grid_rows = int(np.ceil(num_feature_maps / grid_cols))\n",
    "    \n",
    "    for img_idx in range(num_images):\n",
    "        fig, axes = plt.subplots(grid_rows, grid_cols + 1, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # First subplot: original image\n",
    "        orig = images[img_idx].squeeze().cpu().numpy()\n",
    "        axes[0].imshow(orig, cmap='gray')\n",
    "        axes[0].set_title(f'Input\\n{class_names[labels[img_idx]]}', fontsize=10)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Feature maps\n",
    "        for fm_idx in range(num_feature_maps):\n",
    "            ax = axes[fm_idx + 1]\n",
    "            fm = h2_prob[img_idx, fm_idx].cpu().numpy()\n",
    "            ax.imshow(fm, cmap='viridis')\n",
    "            ax.set_title(f'FM {fm_idx+1}', fontsize=8)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Hide unused axes\n",
    "        for ax in axes[num_feature_maps + 1:]:\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Conv-RBM-2 Feature Maps for Sample {img_idx+1}\\n'\n",
    "                     f'(64 feature maps of size {h2_prob.shape[2]}√ó{h2_prob.shape[3]})',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Visualize Conv-RBM-2 activations\n",
    "if DEBUG:\n",
    "    print(\"\\nDetailed Conv-RBM-2 feature map visualization:\")\n",
    "    visualize_layer2_activations(\n",
    "        images=sample_images,\n",
    "        labels=sample_labels,\n",
    "        conv_rbm_1=conv_rbm_1,\n",
    "        conv_rbm_2=conv_rbm_2,\n",
    "        pooling=prob_pool,\n",
    "        class_names=config.CLASS_NAMES,\n",
    "        num_images=2,\n",
    "        num_feature_maps=16\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úì Conv-RBM-2 activations visualization skipped (DEBUG=False for Kaggle)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2993f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary: 2-Layer CDBN Stack\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2-LAYER CDBN STACK SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"ARCHITECTURE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nLayer-by-Layer Dimensions:\")\n",
    "print(f\"  Input               : [{config.BATCH_SIZE}, {config.NUM_CHANNELS}, {config.IMAGE_SIZE[0]}, {config.IMAGE_SIZE[1]}]\")\n",
    "\n",
    "# Conv-RBM-1\n",
    "h1_h = config.IMAGE_SIZE[0] - CONVRBM1_CONFIG['kernel_size'] + 1\n",
    "h1_w = config.IMAGE_SIZE[1] - CONVRBM1_CONFIG['kernel_size'] + 1\n",
    "print(f\"  Conv-RBM-1 Hidden   : [{config.BATCH_SIZE}, {CONVRBM1_CONFIG['hidden_channels']}, {h1_h}, {h1_w}]\")\n",
    "\n",
    "# Pooling\n",
    "pool_h = h1_h // 2\n",
    "pool_w = h1_w // 2\n",
    "print(f\"  After Pooling (2√ó2) : [{config.BATCH_SIZE}, {CONVRBM1_CONFIG['hidden_channels']}, {pool_h}, {pool_w}]\")\n",
    "\n",
    "# Conv-RBM-2\n",
    "h2_h = pool_h - CONVRBM2_CONFIG['kernel_size'] + 1\n",
    "h2_w = pool_w - CONVRBM2_CONFIG['kernel_size'] + 1\n",
    "print(f\"  Conv-RBM-2 Hidden   : [{config.BATCH_SIZE}, {CONVRBM2_CONFIG['hidden_channels']}, {h2_h}, {h2_w}]\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"TRAINABLE PARAMETERS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "params_rbm1 = sum(p.numel() for p in conv_rbm_1.parameters())\n",
    "params_rbm2 = sum(p.numel() for p in conv_rbm_2.parameters())\n",
    "\n",
    "print(f\"  Conv-RBM-1          : {params_rbm1:,} parameters\")\n",
    "print(f\"  Conv-RBM-2          : {params_rbm2:,} parameters\")\n",
    "print(f\"  Total               : {params_rbm1 + params_rbm2:,} parameters\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"TRAINING RESULTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\n  Conv-RBM-1 (10 epochs):\")\n",
    "print(f\"    Initial loss: {training_history['epoch_loss'][0]:.6f}\")\n",
    "print(f\"    Final loss  : {training_history['epoch_loss'][-1]:.6f}\")\n",
    "\n",
    "print(f\"\\n  Conv-RBM-2 (10 epochs):\")\n",
    "print(f\"    Initial loss: {training_history_2['epoch_loss'][0]:.6f}\")\n",
    "print(f\"    Final loss  : {training_history_2['epoch_loss'][-1]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"COMPONENTS CREATED\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n  Models:\")\n",
    "print(f\"    ‚Ä¢ conv_rbm_1  : Pretrained, frozen ({CONVRBM1_CONFIG['hidden_channels']} filters, {CONVRBM1_CONFIG['kernel_size']}√ó{CONVRBM1_CONFIG['kernel_size']})\")\n",
    "print(f\"    ‚Ä¢ conv_rbm_2  : Pretrained ({CONVRBM2_CONFIG['hidden_channels']} filters, {CONVRBM2_CONFIG['kernel_size']}√ó{CONVRBM2_CONFIG['kernel_size']})\")\n",
    "print(f\"    ‚Ä¢ prob_pool   : 2√ó2 probabilistic pooling\")\n",
    "\n",
    "print(\"\\n  Utilities:\")\n",
    "print(f\"    ‚Ä¢ cd_trainer    : CD-1 trainer for Conv-RBM-1\")\n",
    "print(f\"    ‚Ä¢ cd_trainer_2  : CD-1 trainer for Conv-RBM-2\")\n",
    "print(f\"    ‚Ä¢ feature_extractor : Pipeline for extracting pooled features\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2-LAYER CDBN READY FOR CLASSIFICATION!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df8c01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conv-RBM-2 Stacked and Pretrained ‚Äî Hierarchical Features Learned\n",
    "\n",
    "‚úÖ **STEP 9: Probabilistic Pooling Layer** implemented\n",
    "- 2√ó2 pooling with sum-based probability aggregation\n",
    "- Differs from max pooling by preserving probabilistic semantics\n",
    "- No learnable parameters\n",
    "\n",
    "‚úÖ **STEP 10: Conv-RBM-1 Frozen** and feature extraction pipeline created\n",
    "- All Conv-RBM-1 parameters set to `requires_grad = False`\n",
    "- `FeatureExtractor` class for computing pooled features\n",
    "\n",
    "‚úÖ **STEP 11: Conv-RBM-2** instantiated\n",
    "- Input: 32 channels (pooled features from Conv-RBM-1)\n",
    "- Output: 64 feature maps with 5√ó5 kernels\n",
    "- Reuses the same `ConvRBM` class architecture\n",
    "\n",
    "‚úÖ **STEP 12: Conv-RBM-2 Pretrained** with Contrastive Divergence\n",
    "- 10 epochs of CD-1 training on pooled features\n",
    "- Unsupervised (no labels used)\n",
    "- Reconstruction loss tracked\n",
    "\n",
    "‚úÖ **STEP 13: Visualizations** completed\n",
    "- Conv-RBM-2 filter energies and channel slices\n",
    "- Hierarchical feature extraction through both layers\n",
    "- Detailed Conv-RBM-2 activation maps\n",
    "\n",
    "---\n",
    "\n",
    "**CDBN Architecture Summary:**\n",
    "\n",
    "```\n",
    "Input [B, 1, 128, 128]\n",
    "    ‚Üì\n",
    "Conv-RBM-1 (7√ó7, 32 filters) ‚Üí [B, 32, 122, 122]\n",
    "    ‚Üì\n",
    "Probabilistic Pooling (2√ó2) ‚Üí [B, 32, 61, 61]\n",
    "    ‚Üì\n",
    "Conv-RBM-2 (5√ó5, 64 filters) ‚Üí [B, 64, 57, 57]\n",
    "    ‚Üì\n",
    "Ready for Classification\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "- STEP 14: Add second pooling layer\n",
    "- STEP 15: Flatten features and add supervised classifier\n",
    "- STEP 16: Fine-tune the entire CDBN end-to-end\n",
    "- STEP 17: Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e6f6d",
   "metadata": {},
   "source": [
    "## STEP 14 ‚Äî Flattening & Feature Vector Preparation\n",
    "\n",
    "Before feeding Conv-RBM-2 hidden activations to the Fully Connected RBM (FC-RBM), we must flatten the 4D tensor into a 2D matrix.\n",
    "\n",
    "**Why Flattening is Required:**\n",
    "1. **Conv-RBM output:** `[B, C, H, W]` ‚Äî 4D tensor with spatial structure\n",
    "2. **FC-RBM input:** `[B, D]` ‚Äî 2D tensor where D = C √ó H √ó W\n",
    "3. FC-RBMs use dense weight matrices without weight sharing\n",
    "4. Flattening preserves all information but loses spatial locality\n",
    "5. This transition moves from local feature detection to global pattern learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9618630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 14: Flattening & Feature Vector Preparation\n",
    "# =============================================================================\n",
    "\n",
    "class FeatureFlattener:\n",
    "    \"\"\"\n",
    "    Flattens Conv-RBM-2 hidden probabilities for input to FC-RBM.\n",
    "    \n",
    "    WHY FLATTENING IS REQUIRED BEFORE FC-RBM:\n",
    "    -----------------------------------------\n",
    "    \n",
    "    1. CONVOLUTIONAL RBM OUTPUT:\n",
    "       - Shape: [batch_size, num_feature_maps, height, width]\n",
    "       - Preserves spatial structure of learned features\n",
    "       - Each position represents a local feature detection\n",
    "       - Example: [32, 64, 57, 57] = 32 samples √ó 64 feature maps √ó 57√ó57 spatial\n",
    "    \n",
    "    2. FULLY CONNECTED RBM INPUT:\n",
    "       - Shape: [batch_size, num_features]\n",
    "       - No spatial structure - each unit connects to all hidden units\n",
    "       - Treats all features equally regardless of original position\n",
    "       - Example: [32, 207936] = 32 samples √ó (64 √ó 57 √ó 57) features\n",
    "    \n",
    "    3. MATHEMATICAL JUSTIFICATION:\n",
    "       - Conv-RBMs: W has shape [K, C, k, k] with weight sharing\n",
    "       - FC-RBMs: W has shape [D, H] where D = C √ó H √ó W\n",
    "       - The transition from local (convolutional) to global (fully connected)\n",
    "         allows the model to learn arbitrary combinations of features\n",
    "    \n",
    "    4. INFORMATION PRESERVATION:\n",
    "       - Flattening is a bijective (one-to-one) transformation\n",
    "       - No information is lost, but spatial relationships become implicit\n",
    "       - The FC-RBM must learn spatial relationships from data\n",
    "    \n",
    "    This layer computes the feature dimension D and provides utilities\n",
    "    for flattening and unflattening operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_shape: tuple):\n",
    "        \"\"\"\n",
    "        Initialize the flattener with the expected feature shape.\n",
    "        \n",
    "        Args:\n",
    "            feature_shape: Shape of Conv-RBM-2 hidden output [C, H, W]\n",
    "                          (without batch dimension)\n",
    "        \"\"\"\n",
    "        self.feature_shape = feature_shape  # [C, H, W]\n",
    "        self.num_channels = feature_shape[0]\n",
    "        self.height = feature_shape[1]\n",
    "        self.width = feature_shape[2]\n",
    "        \n",
    "        # Compute flattened dimension: D = C √ó H √ó W\n",
    "        self.flat_dim = self.num_channels * self.height * self.width\n",
    "        \n",
    "    def flatten(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Flatten 4D feature tensor to 2D.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, channels, height, width]\n",
    "            \n",
    "        Returns:\n",
    "            Flattened tensor [batch, D] where D = channels √ó height √ó width\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        # Reshape: [B, C, H, W] -> [B, C*H*W]\n",
    "        return x.view(batch_size, -1)\n",
    "    \n",
    "    def unflatten(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Unflatten 2D tensor back to 4D.\n",
    "        \n",
    "        Args:\n",
    "            x: Flattened tensor [batch, D]\n",
    "            \n",
    "        Returns:\n",
    "            Unflattened tensor [batch, channels, height, width]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, self.num_channels, self.height, self.width)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"FeatureFlattener(\\n\"\n",
    "                f\"  feature_shape={self.feature_shape},\\n\"\n",
    "                f\"  flat_dim={self.flat_dim:,}\\n\"\n",
    "                f\")\")\n",
    "\n",
    "\n",
    "print(\"‚úì FeatureFlattener class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Compute Feature Dimensions and Create Flattener\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE FLATTENING SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Freeze Conv-RBM-2 for feature extraction\n",
    "for param in conv_rbm_2.parameters():\n",
    "    param.requires_grad = False\n",
    "conv_rbm_2.eval()\n",
    "\n",
    "print(\"\\n‚úì Conv-RBM-2 parameters frozen\")\n",
    "\n",
    "# Compute feature dimensions by passing a sample through the pipeline\n",
    "with torch.no_grad():\n",
    "    # Get a sample batch\n",
    "    sample_images, _ = next(iter(train_loader))\n",
    "    sample_images = sample_images.to(config.DEVICE)\n",
    "    \n",
    "    # Forward through Conv-RBM-1\n",
    "    h1_prob = conv_rbm_1.hidden_probabilities(sample_images)\n",
    "    \n",
    "    # Pooling\n",
    "    h1_pooled = prob_pool(h1_prob)\n",
    "    \n",
    "    # Forward through Conv-RBM-2\n",
    "    h2_prob = conv_rbm_2.hidden_probabilities(h1_pooled)\n",
    "\n",
    "# Get feature shape (without batch dimension)\n",
    "feature_shape = h2_prob.shape[1:]  # [C, H, W]\n",
    "\n",
    "print(f\"\\nConv-RBM-2 Hidden Shape: {h2_prob.shape}\")\n",
    "print(f\"  Batch size        : {h2_prob.shape[0]}\")\n",
    "print(f\"  Feature maps (C)  : {h2_prob.shape[1]}\")\n",
    "print(f\"  Height (H)        : {h2_prob.shape[2]}\")\n",
    "print(f\"  Width (W)         : {h2_prob.shape[3]}\")\n",
    "\n",
    "# Create flattener\n",
    "flattener = FeatureFlattener(feature_shape)\n",
    "print(f\"\\n{flattener}\")\n",
    "\n",
    "# Test flattening\n",
    "h2_flat = flattener.flatten(h2_prob)\n",
    "print(f\"\\nFlattened shape: {h2_flat.shape}\")\n",
    "print(f\"  Feature dimension D = {flattener.flat_dim:,}\")\n",
    "\n",
    "# Verify unflatten recovers original shape\n",
    "h2_unflat = flattener.unflatten(h2_flat)\n",
    "assert h2_unflat.shape == h2_prob.shape, \"Unflatten shape mismatch!\"\n",
    "print(f\"‚úì Flatten/Unflatten verified\")\n",
    "\n",
    "# Store the flat dimension for FC-RBM\n",
    "FLAT_DIM = flattener.flat_dim\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"FC-RBM will have {FLAT_DIM:,} visible units\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a23b7d",
   "metadata": {},
   "source": [
    "## STEP 15 ‚Äî Fully Connected RBM (FC-RBM)\n",
    "\n",
    "Implement a standard Fully Connected Restricted Boltzmann Machine with Bernoulli visible and hidden units.\n",
    "\n",
    "**FC-RBM Energy Function:**\n",
    "$$E(v, h) = -\\sum_i a_i v_i - \\sum_j b_j h_j - \\sum_{i,j} v_i W_{ij} h_j$$\n",
    "\n",
    "**Conditional Distributions:**\n",
    "$$P(h_j = 1 | v) = \\sigma(b_j + \\sum_i W_{ij} v_i)$$\n",
    "$$P(v_i = 1 | h) = \\sigma(a_i + \\sum_j W_{ij} h_j)$$\n",
    "\n",
    "where $\\sigma(x) = 1/(1 + e^{-x})$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40496cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 15: Fully Connected RBM (FC-RBM)\n",
    "# =============================================================================\n",
    "\n",
    "class FCRBM(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully Connected Restricted Boltzmann Machine (FC-RBM).\n",
    "    \n",
    "    This is a standard RBM with dense connections between visible and hidden layers.\n",
    "    Unlike Conv-RBM, there is no weight sharing - each connection has its own weight.\n",
    "    \n",
    "    Architecture:\n",
    "        visible (v) <---> hidden (h)\n",
    "        \n",
    "        v: [batch_size, n_visible] - Visible units (flattened features)\n",
    "        h: [batch_size, n_hidden]  - Hidden units (learned representations)\n",
    "        W: [n_visible, n_hidden]   - Weight matrix (dense connections)\n",
    "    \n",
    "    Unit Types:\n",
    "        - Visible units: Bernoulli (binary probabilities from Conv-RBM-2)\n",
    "        - Hidden units: Bernoulli (binary latent representations)\n",
    "    \n",
    "    Energy Function (Bernoulli-Bernoulli RBM):\n",
    "        E(v, h) = -sum_i(a_i * v_i) - sum_j(b_j * h_j) - sum_ij(v_i * W_ij * h_j)\n",
    "        \n",
    "        where:\n",
    "            a_i = visible bias for unit i\n",
    "            b_j = hidden bias for unit j\n",
    "            W_ij = weight connecting visible unit i to hidden unit j\n",
    "    \n",
    "    Conditional Distributions:\n",
    "        P(h_j = 1 | v) = sigmoid(b_j + sum_i(W_ij * v_i))\n",
    "        P(v_i = 1 | h) = sigmoid(a_i + sum_j(W_ij * h_j))\n",
    "        \n",
    "        These follow from the energy function via the Boltzmann distribution.\n",
    "    \n",
    "    Args:\n",
    "        n_visible (int): Number of visible units (flattened feature dimension)\n",
    "        n_hidden (int): Number of hidden units (latent representation size)\n",
    "        \n",
    "    Attributes:\n",
    "        W: Weight matrix [n_visible, n_hidden]\n",
    "        v_bias: Visible bias [n_visible]\n",
    "        h_bias: Hidden bias [n_hidden]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_visible: int, n_hidden: int):\n",
    "        super(FCRBM, self).__init__()\n",
    "        \n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # Learnable Parameters\n",
    "        # ---------------------------------------------------------------------\n",
    "        \n",
    "        # Weight matrix: W[i,j] connects visible unit i to hidden unit j\n",
    "        # Shape: [n_visible, n_hidden]\n",
    "        # Initialization: Small random values (helps with initial learning)\n",
    "        # Using Xavier/Glorot-like initialization scaled down\n",
    "        std = 0.01  # Small initial weights\n",
    "        self.W = nn.Parameter(\n",
    "            torch.randn(n_visible, n_hidden) * std\n",
    "        )\n",
    "        \n",
    "        # Visible bias: a_i for each visible unit\n",
    "        # Shape: [n_visible]\n",
    "        # Initialization: Zero (or can initialize to log(p/(1-p)) for mean activation p)\n",
    "        self.v_bias = nn.Parameter(\n",
    "            torch.zeros(n_visible)\n",
    "        )\n",
    "        \n",
    "        # Hidden bias: b_j for each hidden unit\n",
    "        # Shape: [n_hidden]\n",
    "        # Initialization: Zero\n",
    "        self.h_bias = nn.Parameter(\n",
    "            torch.zeros(n_hidden)\n",
    "        )\n",
    "    \n",
    "    def hidden_probabilities(self, v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute hidden unit probabilities given visible units.\n",
    "        \n",
    "        MATH:\n",
    "            P(h_j = 1 | v) = sigmoid(b_j + sum_i(W_ij * v_i))\n",
    "                           = sigmoid(b_j + v @ W[:, j])\n",
    "            \n",
    "            In matrix form for all hidden units:\n",
    "            P(h | v) = sigmoid(h_bias + v @ W)\n",
    "        \n",
    "        Args:\n",
    "            v: Visible units [batch_size, n_visible]\n",
    "               Values should be probabilities in [0, 1]\n",
    "               \n",
    "        Returns:\n",
    "            h_prob: Hidden probabilities [batch_size, n_hidden]\n",
    "                    P(h_j = 1 | v) for each hidden unit j\n",
    "        \"\"\"\n",
    "        # Linear transformation: v @ W + h_bias\n",
    "        # v: [batch, n_visible]\n",
    "        # W: [n_visible, n_hidden]\n",
    "        # Result: [batch, n_hidden]\n",
    "        pre_activation = torch.mm(v, self.W) + self.h_bias\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        # sigmoid(x) = 1 / (1 + exp(-x))\n",
    "        h_prob = torch.sigmoid(pre_activation)\n",
    "        \n",
    "        return h_prob\n",
    "    \n",
    "    def sample_hidden(self, v: torch.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        Sample hidden states given visible units.\n",
    "        \n",
    "        MATH:\n",
    "            h_j ~ Bernoulli(P(h_j = 1 | v))\n",
    "            \n",
    "            Each hidden unit is independently sampled.\n",
    "        \n",
    "        Args:\n",
    "            v: Visible units [batch_size, n_visible]\n",
    "            \n",
    "        Returns:\n",
    "            h_prob: Hidden probabilities [batch_size, n_hidden]\n",
    "            h_sample: Binary hidden samples [batch_size, n_hidden]\n",
    "        \"\"\"\n",
    "        h_prob = self.hidden_probabilities(v)\n",
    "        \n",
    "        # Sample from Bernoulli distribution\n",
    "        # Each unit is 1 with probability h_prob, 0 otherwise\n",
    "        h_sample = torch.bernoulli(h_prob)\n",
    "        \n",
    "        return h_prob, h_sample\n",
    "    \n",
    "    def visible_probabilities(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute visible unit probabilities given hidden units.\n",
    "        \n",
    "        MATH:\n",
    "            P(v_i = 1 | h) = sigmoid(a_i + sum_j(W_ij * h_j))\n",
    "                           = sigmoid(a_i + W[i, :] @ h)\n",
    "            \n",
    "            In matrix form for all visible units:\n",
    "            P(v | h) = sigmoid(v_bias + h @ W^T)\n",
    "        \n",
    "        Args:\n",
    "            h: Hidden units [batch_size, n_hidden]\n",
    "               Can be probabilities or binary samples\n",
    "               \n",
    "        Returns:\n",
    "            v_prob: Visible probabilities [batch_size, n_visible]\n",
    "                    P(v_i = 1 | h) for each visible unit i\n",
    "        \"\"\"\n",
    "        # Linear transformation: h @ W^T + v_bias\n",
    "        # h: [batch, n_hidden]\n",
    "        # W^T: [n_hidden, n_visible]\n",
    "        # Result: [batch, n_visible]\n",
    "        pre_activation = torch.mm(h, self.W.t()) + self.v_bias\n",
    "        \n",
    "        # Apply sigmoid for Bernoulli visible units\n",
    "        v_prob = torch.sigmoid(pre_activation)\n",
    "        \n",
    "        return v_prob\n",
    "    \n",
    "    def sample_visible(self, h: torch.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        Sample visible states given hidden units.\n",
    "        \n",
    "        MATH:\n",
    "            v_i ~ Bernoulli(P(v_i = 1 | h))\n",
    "            \n",
    "            Each visible unit is independently sampled.\n",
    "        \n",
    "        Args:\n",
    "            h: Hidden units [batch_size, n_hidden]\n",
    "            \n",
    "        Returns:\n",
    "            v_prob: Visible probabilities [batch_size, n_visible]\n",
    "            v_sample: Binary visible samples [batch_size, n_visible]\n",
    "        \"\"\"\n",
    "        v_prob = self.visible_probabilities(h)\n",
    "        \n",
    "        # Sample from Bernoulli distribution\n",
    "        v_sample = torch.bernoulli(v_prob)\n",
    "        \n",
    "        return v_prob, v_sample\n",
    "    \n",
    "    def forward(self, v: torch.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        Forward pass: compute hidden probabilities and samples.\n",
    "        \n",
    "        This is the inference direction: visible -> hidden\n",
    "        \n",
    "        Args:\n",
    "            v: Input visible units [batch_size, n_visible]\n",
    "            \n",
    "        Returns:\n",
    "            h_prob: Hidden probabilities [batch_size, n_hidden]\n",
    "            h_sample: Binary hidden samples [batch_size, n_hidden]\n",
    "        \"\"\"\n",
    "        return self.sample_hidden(v)\n",
    "    \n",
    "    def free_energy(self, v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the free energy of visible configurations.\n",
    "        \n",
    "        MATH:\n",
    "            F(v) = -sum_i(a_i * v_i) - sum_j(log(1 + exp(b_j + sum_i(W_ij * v_i))))\n",
    "            \n",
    "            The free energy is useful for computing the log-likelihood gradient\n",
    "            and for model comparison.\n",
    "        \n",
    "        Args:\n",
    "            v: Visible units [batch_size, n_visible]\n",
    "            \n",
    "        Returns:\n",
    "            free_energy: Free energy for each sample [batch_size]\n",
    "        \"\"\"\n",
    "        # First term: -v @ v_bias (visible bias contribution)\n",
    "        # Shape: [batch_size]\n",
    "        v_term = torch.mv(v, self.v_bias)\n",
    "        \n",
    "        # Second term: hidden contribution\n",
    "        # pre_activation: b_j + sum_i(W_ij * v_i)\n",
    "        # Shape: [batch_size, n_hidden]\n",
    "        pre_activation = torch.mm(v, self.W) + self.h_bias\n",
    "        \n",
    "        # log(1 + exp(x)) = softplus(x)\n",
    "        # Sum over hidden units\n",
    "        # Shape: [batch_size]\n",
    "        h_term = torch.sum(nn.functional.softplus(pre_activation), dim=1)\n",
    "        \n",
    "        # Free energy: F(v) = -v_term - h_term\n",
    "        return -v_term - h_term\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"FCRBM(\\n\"\n",
    "                f\"  n_visible={self.n_visible:,},\\n\"\n",
    "                f\"  n_hidden={self.n_hidden:,},\\n\"\n",
    "                f\"  W shape={tuple(self.W.shape)},\\n\"\n",
    "                f\"  total_params={self.n_visible * self.n_hidden + self.n_visible + self.n_hidden:,}\\n\"\n",
    "                f\")\")\n",
    "\n",
    "\n",
    "print(\"‚úì FCRBM class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate FC-RBM\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# FC-RBM Configuration\n",
    "FCRBM_CONFIG = {\n",
    "    'n_visible': FLAT_DIM,      # From Conv-RBM-2 flattened output\n",
    "    'n_hidden': 256,             # Latent representation size (tune as needed)\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FC-RBM CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  n_visible (D)     : {FCRBM_CONFIG['n_visible']:,}\")\n",
    "print(f\"  n_hidden          : {FCRBM_CONFIG['n_hidden']:,}\")\n",
    "print(f\"  Weight matrix     : [{FCRBM_CONFIG['n_visible']:,}, {FCRBM_CONFIG['n_hidden']}]\")\n",
    "\n",
    "# Create FC-RBM instance\n",
    "fc_rbm = FCRBM(\n",
    "    n_visible=FCRBM_CONFIG['n_visible'],\n",
    "    n_hidden=FCRBM_CONFIG['n_hidden']\n",
    ").to(config.DEVICE)\n",
    "\n",
    "print(f\"\\n{fc_rbm}\")\n",
    "\n",
    "# Parameter count\n",
    "total_params_fc = sum(p.numel() for p in fc_rbm.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params_fc:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_h_prob, test_h_sample = fc_rbm(h2_flat.to(config.DEVICE))\n",
    "\n",
    "print(f\"\\nVerification forward pass:\")\n",
    "print(f\"  Input shape  : {h2_flat.shape}\")\n",
    "print(f\"  Output shape : {test_h_prob.shape}\")\n",
    "\n",
    "assert test_h_prob.shape == (h2_flat.shape[0], FCRBM_CONFIG['n_hidden']), \"Shape mismatch!\"\n",
    "print(f\"‚úì Forward pass verification passed!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795ea60",
   "metadata": {},
   "source": [
    "## STEP 16 ‚Äî Contrastive Divergence for FC-RBM\n",
    "\n",
    "Implement CD-k training for the Fully Connected RBM.\n",
    "\n",
    "The algorithm is similar to Conv-RBM CD but uses matrix operations instead of convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a4eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 16: Contrastive Divergence Trainer for FC-RBM\n",
    "# =============================================================================\n",
    "\n",
    "class FCRBMTrainer:\n",
    "    \"\"\"\n",
    "    Contrastive Divergence (CD-k) Trainer for Fully Connected RBM.\n",
    "    \n",
    "    CD-k ALGORITHM FOR FC-RBM:\n",
    "    --------------------------\n",
    "    \n",
    "    1. POSITIVE PHASE (Data Statistics):\n",
    "       - Clamp v0 = input data (flattened features from Conv-RBM-2)\n",
    "       - Compute h0_prob = P(h|v0) = sigmoid(h_bias + v0 @ W)\n",
    "       - Sample h0 ~ Bernoulli(h0_prob)\n",
    "       - Positive gradient: ‚ü®v0^T @ h0_prob‚ü©_data\n",
    "    \n",
    "    2. GIBBS SAMPLING (k steps):\n",
    "       For each step i:\n",
    "           - Compute v_i_prob = sigmoid(v_bias + h_{i-1} @ W^T)\n",
    "           - Sample v_i ~ Bernoulli(v_i_prob) [or use mean]\n",
    "           - Compute h_i_prob = sigmoid(h_bias + v_i @ W)\n",
    "           - Sample h_i ~ Bernoulli(h_i_prob)\n",
    "    \n",
    "    3. NEGATIVE PHASE (Model Statistics):\n",
    "       - Use vk and hk_prob from Gibbs chain\n",
    "       - Negative gradient: ‚ü®vk^T @ hk_prob‚ü©_model\n",
    "    \n",
    "    4. PARAMETER UPDATES:\n",
    "       ŒîW = lr * (positive_W - negative_W) / batch_size\n",
    "       Œîv_bias = lr * mean(v0 - vk)\n",
    "       Œîh_bias = lr * mean(h0_prob - hk_prob)\n",
    "    \n",
    "    Args:\n",
    "        rbm: FCRBM instance to train\n",
    "        learning_rate: Learning rate for updates\n",
    "        k: Number of Gibbs sampling steps\n",
    "        device: Computation device\n",
    "        momentum: Momentum coefficient\n",
    "        weight_decay: L2 regularization coefficient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        rbm: FCRBM,\n",
    "        learning_rate: float = 0.01,\n",
    "        k: int = 1,\n",
    "        device: torch.device = None,\n",
    "        momentum: float = 0.0,\n",
    "        weight_decay: float = 0.0001\n",
    "    ):\n",
    "        self.rbm = rbm\n",
    "        self.lr = learning_rate\n",
    "        self.k = k\n",
    "        self.device = device if device else torch.device('cpu')\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Initialize velocity terms for momentum\n",
    "        self.W_velocity = torch.zeros_like(rbm.W.data)\n",
    "        self.v_bias_velocity = torch.zeros_like(rbm.v_bias.data)\n",
    "        self.h_bias_velocity = torch.zeros_like(rbm.h_bias.data)\n",
    "    \n",
    "    def train_batch(self, v0: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Train the FC-RBM on a single batch using CD-k.\n",
    "        \n",
    "        Args:\n",
    "            v0: Input visible batch (flattened features)\n",
    "                Shape: [batch_size, n_visible]\n",
    "                Values should be probabilities in [0, 1]\n",
    "                \n",
    "        Returns:\n",
    "            reconstruction_loss: MSE between v0 and vk\n",
    "        \"\"\"\n",
    "        batch_size = v0.shape[0]\n",
    "        v0 = v0.to(self.device)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # POSITIVE PHASE\n",
    "        # =====================================================================\n",
    "        # Compute hidden probabilities from clamped visible data\n",
    "        # h0_prob = sigmoid(h_bias + v0 @ W)\n",
    "        \n",
    "        h0_prob = self.rbm.hidden_probabilities(v0)\n",
    "        \n",
    "        # Sample hidden states for Gibbs chain initialization\n",
    "        h0_sample = torch.bernoulli(h0_prob)\n",
    "        \n",
    "        # Positive phase statistics (outer product averaged over batch)\n",
    "        # positive_W = (v0^T @ h0_prob) / batch_size\n",
    "        # This computes: sum over batch of v0_i * h0_j for all (i,j)\n",
    "        positive_W = torch.mm(v0.t(), h0_prob) / batch_size\n",
    "        \n",
    "        # Bias gradients\n",
    "        positive_v_bias = v0.mean(dim=0)\n",
    "        positive_h_bias = h0_prob.mean(dim=0)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # GIBBS SAMPLING (k steps)\n",
    "        # =====================================================================\n",
    "        hk = h0_sample\n",
    "        \n",
    "        for step in range(self.k):\n",
    "            # -----------------------------------------------------------------\n",
    "            # Reconstruct visible from hidden\n",
    "            # -----------------------------------------------------------------\n",
    "            # vk_prob = sigmoid(v_bias + hk @ W^T)\n",
    "            vk_prob = self.rbm.visible_probabilities(hk)\n",
    "            \n",
    "            # Use probabilities (more stable than sampling for visible)\n",
    "            vk = vk_prob\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # Compute hidden from reconstructed visible\n",
    "            # -----------------------------------------------------------------\n",
    "            # hk_prob = sigmoid(h_bias + vk @ W)\n",
    "            hk_prob = self.rbm.hidden_probabilities(vk)\n",
    "            \n",
    "            # Sample for next Gibbs step (except last step)\n",
    "            if step < self.k - 1:\n",
    "                hk = torch.bernoulli(hk_prob)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # NEGATIVE PHASE\n",
    "        # =====================================================================\n",
    "        # Negative phase statistics from the k-step reconstructions\n",
    "        # negative_W = (vk^T @ hk_prob) / batch_size\n",
    "        \n",
    "        negative_W = torch.mm(vk.t(), hk_prob) / batch_size\n",
    "        \n",
    "        # Bias gradients\n",
    "        negative_v_bias = vk.mean(dim=0)\n",
    "        negative_h_bias = hk_prob.mean(dim=0)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # PARAMETER UPDATES\n",
    "        # =====================================================================\n",
    "        # Gradient: positive - negative\n",
    "        # Update: Œ∏ += lr * gradient\n",
    "        \n",
    "        W_grad = positive_W - negative_W\n",
    "        v_bias_grad = positive_v_bias - negative_v_bias\n",
    "        h_bias_grad = positive_h_bias - negative_h_bias\n",
    "        \n",
    "        # Apply weight decay (L2 regularization)\n",
    "        W_grad -= self.weight_decay * self.rbm.W.data\n",
    "        \n",
    "        # Update with momentum\n",
    "        self.W_velocity = self.momentum * self.W_velocity + W_grad\n",
    "        self.v_bias_velocity = self.momentum * self.v_bias_velocity + v_bias_grad\n",
    "        self.h_bias_velocity = self.momentum * self.h_bias_velocity + h_bias_grad\n",
    "        \n",
    "        # Apply updates\n",
    "        with torch.no_grad():\n",
    "            self.rbm.W.data += self.lr * self.W_velocity\n",
    "            self.rbm.v_bias.data += self.lr * self.v_bias_velocity\n",
    "            self.rbm.h_bias.data += self.lr * self.h_bias_velocity\n",
    "        \n",
    "        # =====================================================================\n",
    "        # RECONSTRUCTION LOSS\n",
    "        # =====================================================================\n",
    "        reconstruction_loss = nn.functional.mse_loss(vk, v0).item()\n",
    "        \n",
    "        return reconstruction_loss\n",
    "    \n",
    "    def get_reconstruction(self, v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get reconstruction of visible input.\n",
    "        \n",
    "        Args:\n",
    "            v: Input visible units\n",
    "            \n",
    "        Returns:\n",
    "            v_recon: Reconstructed visible units\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            v = v.to(self.device)\n",
    "            h_prob = self.rbm.hidden_probabilities(v)\n",
    "            v_recon = self.rbm.visible_probabilities(h_prob)\n",
    "        return v_recon\n",
    "\n",
    "\n",
    "print(\"‚úì FCRBMTrainer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36716131",
   "metadata": {},
   "source": [
    "## STEP 17 ‚Äî Train FC-RBM on Hierarchical Features\n",
    "\n",
    "Now we train the FC-RBM using the complete feature extraction pipeline:\n",
    "\n",
    "**Pipeline:** Image ‚Üí Conv-RBM-1 ‚Üí Pool ‚Üí Conv-RBM-2 ‚Üí Flatten ‚Üí FC-RBM\n",
    "\n",
    "The FC-RBM receives the flattened probabilistic features from Conv-RBM-2 and learns a compressed latent representation of 256 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 17: Define Complete Feature Extraction Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "class HierarchicalFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Complete pipeline to extract features from OCT images through the CDBN.\n",
    "    \n",
    "    PIPELINE ARCHITECTURE:\n",
    "    ----------------------\n",
    "    Input Image [B, 1, 128, 128]\n",
    "          ‚Üì\n",
    "    Conv-RBM-1: 7√ó7 conv ‚Üí 32 channels\n",
    "          ‚Üì [B, 32, 122, 122]\n",
    "    Prob-Pool: 2√ó2 ‚Üí [B, 32, 61, 61]\n",
    "          ‚Üì\n",
    "    Conv-RBM-2: 5√ó5 conv ‚Üí 64 channels\n",
    "          ‚Üì [B, 64, 57, 57]\n",
    "    Flatten\n",
    "          ‚Üì [B, 207936]\n",
    "    FC-RBM (after training)\n",
    "          ‚Üì [B, 256]\n",
    "    \n",
    "    All Conv-RBMs are FROZEN (pretrained, no further updates).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_rbm_1: ConvRBM,\n",
    "        pooling: ProbabilisticPooling,\n",
    "        conv_rbm_2: ConvRBM,\n",
    "        flattener: FeatureFlattener,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.conv_rbm_1 = conv_rbm_1\n",
    "        self.pooling = pooling\n",
    "        self.conv_rbm_2 = conv_rbm_2\n",
    "        self.flattener = flattener\n",
    "        self.device = device\n",
    "        \n",
    "        # Ensure all Conv-RBMs are frozen\n",
    "        self.conv_rbm_1.eval()\n",
    "        self.conv_rbm_2.eval()\n",
    "        for param in self.conv_rbm_1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.conv_rbm_2.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def extract_flat_features(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract flattened features ready for FC-RBM.\n",
    "        \n",
    "        Args:\n",
    "            images: Input images [B, 1, 128, 128]\n",
    "            \n",
    "        Returns:\n",
    "            flat_features: Flattened features [B, n_visible]\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            images = images.to(self.device)\n",
    "            \n",
    "            # Layer 1: Conv-RBM-1 ‚Üí probabilities\n",
    "            h1_prob = self.conv_rbm_1.hidden_probabilities(images)\n",
    "            \n",
    "            # Layer 2: Probabilistic pooling\n",
    "            h1_pooled = self.pooling(h1_prob)\n",
    "            \n",
    "            # Layer 3: Conv-RBM-2 ‚Üí probabilities\n",
    "            h2_prob = self.conv_rbm_2.hidden_probabilities(h1_pooled)\n",
    "            \n",
    "            # Layer 4: Flatten\n",
    "            flat_features = self.flattener(h2_prob)\n",
    "        \n",
    "        return flat_features\n",
    "\n",
    "\n",
    "# Create the hierarchical feature extractor\n",
    "hierarchical_extractor = HierarchicalFeatureExtractor(\n",
    "    conv_rbm_1=conv_rbm_1,\n",
    "    pooling=prob_pool,\n",
    "    conv_rbm_2=conv_rbm_2,\n",
    "    flattener=flattener,\n",
    "    device=Config.DEVICE\n",
    ")\n",
    "\n",
    "# Verify with a test batch\n",
    "test_batch = next(iter(train_loader))[0][:4]  # Get 4 images\n",
    "test_flat = hierarchical_extractor.extract_flat_features(test_batch)\n",
    "print(f\"‚úì HierarchicalFeatureExtractor created!\")\n",
    "print(f\"  Input shape: {test_batch.shape}\")\n",
    "print(f\"  Output shape: {test_flat.shape}\")\n",
    "print(f\"  Output range: [{test_flat.min():.4f}, {test_flat.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 17: Train FC-RBM on Flattened Hierarchical Features (Memory-Optimized)\n",
    "# =============================================================================\n",
    "\n",
    "# Training configuration for FC-RBM\n",
    "FC_RBM_EPOCHS = 5        # Local GPU: can use more epochs if needed\n",
    "FC_RBM_LR = 0.001        # Lower learning rate for high-dimensional input\n",
    "FC_RBM_K = 1             # CD-1\n",
    "FC_RBM_MOMENTUM = 0.5\n",
    "FC_RBM_WEIGHT_DECAY = 0.0001\n",
    "MEMORY_CLEANUP_FREQ = 50  # Clean GPU cache every N batches\n",
    "\n",
    "# Create trainer\n",
    "fc_trainer = FCRBMTrainer(\n",
    "    rbm=fc_rbm,\n",
    "    learning_rate=FC_RBM_LR,\n",
    "    k=FC_RBM_K,\n",
    "    device=config.DEVICE,\n",
    "    momentum=FC_RBM_MOMENTUM,\n",
    "    weight_decay=FC_RBM_WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FC-RBM TRAINING (Memory-Optimized for Local GPU)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Epochs: {FC_RBM_EPOCHS}\")\n",
    "print(f\"Learning Rate: {FC_RBM_LR}\")\n",
    "print(f\"CD-k: {FC_RBM_K}\")\n",
    "print(f\"Momentum: {FC_RBM_MOMENTUM}\")\n",
    "print(f\"Weight Decay: {FC_RBM_WEIGHT_DECAY}\")\n",
    "print(f\"Visible Units: {fc_rbm.n_visible:,}\")\n",
    "print(f\"Hidden Units: {fc_rbm.n_hidden}\")\n",
    "print(f\"Memory cleanup frequency: every {MEMORY_CLEANUP_FREQ} batches\")\n",
    "if torch.cuda.is_available():\n",
    "    print_gpu_memory()\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Training loop with memory optimization\n",
    "fc_training_history = []\n",
    "\n",
    "for epoch in range(FC_RBM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    for batch_idx, (images, _) in enumerate(train_loader):\n",
    "        # Extract flattened features through the hierarchical pipeline\n",
    "        flat_features = hierarchical_extractor.extract_flat_features(images)\n",
    "        \n",
    "        # Train FC-RBM on flattened features\n",
    "        loss = fc_trainer.train_batch(flat_features)\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        # Explicit memory cleanup\n",
    "        del images, flat_features\n",
    "        \n",
    "        # Progress update\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            print(f\"  Epoch [{epoch+1}/{FC_RBM_EPOCHS}] \"\n",
    "                  f\"Batch [{batch_idx+1}/{len(train_loader)}] \"\n",
    "                  f\"Recon Loss: {loss:.6f}\")\n",
    "        \n",
    "        # Aggressive memory cleanup for large datasets\n",
    "        if torch.cuda.is_available() and (batch_idx + 1) % MEMORY_CLEANUP_FREQ == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Epoch summary\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    fc_training_history.append(avg_loss)\n",
    "    \n",
    "    # Clear batch losses to free memory\n",
    "    epoch_losses.clear()\n",
    "    \n",
    "    # Print epoch summary with GPU memory\n",
    "    print(f\"‚ñ∂ Epoch [{epoch+1}/{FC_RBM_EPOCHS}] Average Loss: {avg_loss:.6f} | \"\n",
    "          f\"Time: {epoch_time:.1f}s\", end=\"\")\n",
    "    if torch.cuda.is_available():\n",
    "        mem_alloc = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\" | GPU: {mem_alloc:.2f} GB\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    # Clear cache after each epoch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FC-RBM TRAINING COMPLETE!\")\n",
    "print(f\"Final Loss: {fc_training_history[-1]:.6f}\")\n",
    "if torch.cuda.is_available():\n",
    "    print_gpu_memory()\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174927d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize FC-RBM Training Progress\n",
    "# =============================================================================\n",
    "\n",
    "if DEBUG:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Plot 1: FC-RBM training loss\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(range(1, len(fc_training_history) + 1), fc_training_history, \n",
    "             'b-o', linewidth=2, markersize=8, label='FC-RBM')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Reconstruction Loss (MSE)', fontsize=12)\n",
    "    ax1.set_title('FC-RBM Training Loss', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot 2: All three RBMs training comparison (if histories available)\n",
    "    ax2 = axes[1]\n",
    "    if 'training_history' in dir() and len(training_history) > 0:\n",
    "        ax2.plot(range(1, len(training_history) + 1), training_history, \n",
    "                 'r-s', linewidth=2, markersize=6, label='Conv-RBM-1', alpha=0.7)\n",
    "    if 'training_history_2' in dir() and len(training_history_2) > 0:\n",
    "        ax2.plot(range(1, len(training_history_2) + 1), training_history_2, \n",
    "                 'g-^', linewidth=2, markersize=6, label='Conv-RBM-2', alpha=0.7)\n",
    "    ax2.plot(range(1, len(fc_training_history) + 1), fc_training_history, \n",
    "             'b-o', linewidth=2, markersize=6, label='FC-RBM', alpha=0.7)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Reconstruction Loss (MSE)', fontsize=12)\n",
    "    ax2.set_title('All RBMs Training Comparison', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.OUTPUT_DIR, 'fc_rbm_training.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úì FC-RBM training plot saved to output directory\")\n",
    "else:\n",
    "    print(\"‚úì FC-RBM visualization skipped (DEBUG=False for Kaggle)\")\n",
    "    print(f\"  Final FC-RBM Loss: {fc_training_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516adf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP E: Memory Cleanup After FC-RBM Pretraining (Kaggle)\n",
    "# =============================================================================\n",
    "# Free GPU memory by deleting the FC-RBM trainer (holds large velocity tensors)\n",
    "# This is critical for Kaggle's 16GB GPU memory limit\n",
    "\n",
    "# Delete the FC-RBM trainer\n",
    "del fc_trainer\n",
    "\n",
    "# Clear CUDA cache to release fragmented memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"‚úì FC-RBM trainer deleted, CUDA cache cleared\")\n",
    "    print(f\"  GPU Memory ‚Äî Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚úì FC-RBM trainer deleted (CPU mode)\")\n",
    "\n",
    "print(\"  FC-RBM weights are preserved in fc_rbm module\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úì ALL RBM PRETRAINING COMPLETE ‚Äî Memory optimized for supervised phase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393eac1",
   "metadata": {},
   "source": [
    "## STEP 18 ‚Äî Extract Latent Representations from Complete CDBN\n",
    "\n",
    "With all three RBM layers pretrained, we now define a complete feature extraction function that takes OCT images and outputs the top-level latent representation from the FC-RBM.\n",
    "\n",
    "**Complete CDBN Pipeline:**\n",
    "$$\n",
    "\\mathbf{z} = h^{(3)} = \\sigma\\left(\\mathbf{b}_h^{(3)} + \\text{flatten}\\left(h^{(2)}\\right) \\mathbf{W}^{(3)}\\right)\n",
    "$$\n",
    "\n",
    "where $h^{(2)}$ is the pooled output from Conv-RBM-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ae157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 18: Complete CDBN Latent Representation Extractor\n",
    "# =============================================================================\n",
    "\n",
    "class CDBNFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Complete Convolutional Deep Belief Network Feature Extractor.\n",
    "    \n",
    "    FULL CDBN ARCHITECTURE:\n",
    "    -----------------------\n",
    "    \n",
    "    Input: OCT Image [B, 1, 128, 128]\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Conv-RBM-1: Gaussian-Bernoulli      ‚îÇ\n",
    "    ‚îÇ  32 filters, 7√ó7 kernels             ‚îÇ\n",
    "    ‚îÇ  Output: [B, 32, 122, 122]           ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Probabilistic Pooling               ‚îÇ\n",
    "    ‚îÇ  2√ó2 pool, sum-based                 ‚îÇ\n",
    "    ‚îÇ  Output: [B, 32, 61, 61]             ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Conv-RBM-2: Bernoulli-Bernoulli     ‚îÇ\n",
    "    ‚îÇ  64 filters, 5√ó5 kernels             ‚îÇ\n",
    "    ‚îÇ  Output: [B, 64, 57, 57]             ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Flatten                             ‚îÇ\n",
    "    ‚îÇ  Output: [B, 207936]                 ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  FC-RBM: Bernoulli-Bernoulli         ‚îÇ\n",
    "    ‚îÇ  256 hidden units                    ‚îÇ\n",
    "    ‚îÇ  Output: [B, 256]                    ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    Latent Representation z ‚àà [0, 1]^256\n",
    "    \n",
    "    This latent representation can be used for:\n",
    "    - Supervised classification (add softmax layer)\n",
    "    - Fine-tuning with backpropagation\n",
    "    - Clustering\n",
    "    - Visualization\n",
    "    \n",
    "    All parameters are FROZEN after unsupervised pretraining.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_rbm_1: ConvRBM,\n",
    "        pooling: ProbabilisticPooling,\n",
    "        conv_rbm_2: ConvRBM,\n",
    "        flattener: FeatureFlattener,\n",
    "        fc_rbm: FCRBM,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.conv_rbm_1 = conv_rbm_1\n",
    "        self.pooling = pooling\n",
    "        self.conv_rbm_2 = conv_rbm_2\n",
    "        self.flattener = flattener\n",
    "        self.fc_rbm = fc_rbm\n",
    "        self.device = device\n",
    "        \n",
    "        # Freeze all layers\n",
    "        self._freeze_all()\n",
    "    \n",
    "    def _freeze_all(self):\n",
    "        \"\"\"Freeze all parameters in the CDBN.\"\"\"\n",
    "        for module in [self.conv_rbm_1, self.conv_rbm_2, self.fc_rbm]:\n",
    "            module.eval()\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def extract_latent(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract top-level latent representation from OCT images.\n",
    "        \n",
    "        Args:\n",
    "            images: Batch of OCT images\n",
    "                    Shape: [B, 1, 128, 128]\n",
    "                    Values: normalized to [0, 1]\n",
    "                    \n",
    "        Returns:\n",
    "            latent: Top-level hidden probabilities from FC-RBM\n",
    "                    Shape: [B, n_hidden] = [B, 256]\n",
    "                    Values: probabilities in [0, 1]\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            images = images.to(self.device)\n",
    "            \n",
    "            # Layer 1: Conv-RBM-1\n",
    "            # h1 = sigmoid(conv(v, W1) + b1)\n",
    "            h1_prob = self.conv_rbm_1.hidden_probabilities(images)\n",
    "            \n",
    "            # Layer 2: Probabilistic Pooling (2√ó2)\n",
    "            h1_pooled = self.pooling(h1_prob)\n",
    "            \n",
    "            # Layer 3: Conv-RBM-2\n",
    "            # h2 = sigmoid(conv(h1_pooled, W2) + b2)\n",
    "            h2_prob = self.conv_rbm_2.hidden_probabilities(h1_pooled)\n",
    "            \n",
    "            # Layer 4: Flatten\n",
    "            flat = self.flattener(h2_prob)\n",
    "            \n",
    "            # Layer 5: FC-RBM\n",
    "            # z = sigmoid(flat @ W3 + b3)\n",
    "            latent = self.fc_rbm.hidden_probabilities(flat)\n",
    "        \n",
    "        return latent\n",
    "    \n",
    "    def extract_all_representations(self, images: torch.Tensor) -> dict:\n",
    "        \"\"\"\n",
    "        Extract representations at all layers (for visualization/analysis).\n",
    "        \n",
    "        Args:\n",
    "            images: Batch of OCT images [B, 1, 128, 128]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing representations at each layer\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            images = images.to(self.device)\n",
    "            \n",
    "            h1 = self.conv_rbm_1.hidden_probabilities(images)\n",
    "            h1_pooled = self.pooling(h1)\n",
    "            h2 = self.conv_rbm_2.hidden_probabilities(h1_pooled)\n",
    "            flat = self.flattener(h2)\n",
    "            latent = self.fc_rbm.hidden_probabilities(flat)\n",
    "        \n",
    "        return {\n",
    "            'input': images,\n",
    "            'conv_rbm_1': h1,\n",
    "            'pooled_1': h1_pooled,\n",
    "            'conv_rbm_2': h2,\n",
    "            'flattened': flat,\n",
    "            'latent': latent\n",
    "        }\n",
    "\n",
    "\n",
    "# Create the complete CDBN feature extractor\n",
    "cdbn_extractor = CDBNFeatureExtractor(\n",
    "    conv_rbm_1=conv_rbm_1,\n",
    "    pooling=prob_pool,\n",
    "    conv_rbm_2=conv_rbm_2,\n",
    "    flattener=flattener,\n",
    "    fc_rbm=fc_rbm,\n",
    "    device=Config.DEVICE\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CDBN FEATURE EXTRACTOR CREATED\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Verify CDBN Feature Extraction\n",
    "# =============================================================================\n",
    "\n",
    "# Test with a batch of images\n",
    "test_images, test_labels = next(iter(train_loader))\n",
    "test_images = test_images[:8]  # Use 8 images for testing\n",
    "test_labels = test_labels[:8]\n",
    "\n",
    "# Extract latent representations\n",
    "latent_representations = cdbn_extractor.extract_latent(test_images)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CDBN LATENT EXTRACTION VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Input shape:  {test_images.shape}\")\n",
    "print(f\"Output shape: {latent_representations.shape}\")\n",
    "print(f\"Output dtype: {latent_representations.dtype}\")\n",
    "print(f\"Output device: {latent_representations.device}\")\n",
    "print(f\"Output range: [{latent_representations.min():.4f}, {latent_representations.max():.4f}]\")\n",
    "print(f\"Output mean:  {latent_representations.mean():.4f}\")\n",
    "print(f\"Output std:   {latent_representations.std():.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify expected shape\n",
    "expected_shape = (8, 256)\n",
    "assert latent_representations.shape == expected_shape, \\\n",
    "    f\"Expected {expected_shape}, got {latent_representations.shape}\"\n",
    "print(f\"‚úì Shape verification passed: [B, n_hidden] = {latent_representations.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize All Layer Representations\n",
    "# =============================================================================\n",
    "\n",
    "# Get all representations for visualization\n",
    "all_reps = cdbn_extractor.extract_all_representations(test_images[:1])\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Input image\n",
    "ax = axes[0, 0]\n",
    "ax.imshow(test_images[0, 0].cpu().numpy(), cmap='gray')\n",
    "ax.set_title(f'Input Image\\n{test_images[0].shape}', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "# 2. Conv-RBM-1 features (show first 16 channels as grid)\n",
    "ax = axes[0, 1]\n",
    "h1 = all_reps['conv_rbm_1'][0].cpu().numpy()\n",
    "# Create a grid of first 16 feature maps\n",
    "grid = np.zeros((4*h1.shape[1]//4, 4*h1.shape[2]//4))\n",
    "for i in range(min(16, h1.shape[0])):\n",
    "    r, c = i // 4, i % 4\n",
    "    grid[r*h1.shape[1]//4:(r+1)*h1.shape[1]//4, \n",
    "         c*h1.shape[2]//4:(c+1)*h1.shape[2]//4] = \\\n",
    "        h1[i, ::4, ::4]  # Downsample for display\n",
    "ax.imshow(grid, cmap='hot')\n",
    "ax.set_title(f'Conv-RBM-1 Output\\n{all_reps[\"conv_rbm_1\"].shape}', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "# 3. Pooled features\n",
    "ax = axes[0, 2]\n",
    "h1_pool = all_reps['pooled_1'][0].cpu().numpy()\n",
    "grid2 = np.zeros((4*h1_pool.shape[1]//4, 4*h1_pool.shape[2]//4))\n",
    "for i in range(min(16, h1_pool.shape[0])):\n",
    "    r, c = i // 4, i % 4\n",
    "    grid2[r*h1_pool.shape[1]//4:(r+1)*h1_pool.shape[1]//4, \n",
    "          c*h1_pool.shape[2]//4:(c+1)*h1_pool.shape[2]//4] = \\\n",
    "        h1_pool[i, ::4, ::4]\n",
    "ax.imshow(grid2, cmap='hot')\n",
    "ax.set_title(f'After Pooling\\n{all_reps[\"pooled_1\"].shape}', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "# 4. Conv-RBM-2 features\n",
    "ax = axes[1, 0]\n",
    "h2 = all_reps['conv_rbm_2'][0].cpu().numpy()\n",
    "grid3 = np.zeros((4*h2.shape[1]//4, 4*h2.shape[2]//4))\n",
    "for i in range(min(16, h2.shape[0])):\n",
    "    r, c = i // 4, i % 4\n",
    "    grid3[r*h2.shape[1]//4:(r+1)*h2.shape[1]//4, \n",
    "          c*h2.shape[2]//4:(c+1)*h2.shape[2]//4] = \\\n",
    "        h2[i, ::4, ::4]\n",
    "ax.imshow(grid3, cmap='hot')\n",
    "ax.set_title(f'Conv-RBM-2 Output\\n{all_reps[\"conv_rbm_2\"].shape}', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "# 5. Flattened representation (show as 1D bar)\n",
    "ax = axes[1, 1]\n",
    "flat = all_reps['flattened'][0].cpu().numpy()\n",
    "# Show a subset (every 1000th value)\n",
    "flat_subset = flat[::1000]\n",
    "ax.bar(range(len(flat_subset)), flat_subset, color='steelblue', alpha=0.7)\n",
    "ax.set_title(f'Flattened (sampled)\\n{all_reps[\"flattened\"].shape}', fontsize=11)\n",
    "ax.set_xlabel('Feature Index (√ó1000)')\n",
    "ax.set_ylabel('Activation')\n",
    "\n",
    "# 6. Final latent representation\n",
    "ax = axes[1, 2]\n",
    "latent = all_reps['latent'][0].cpu().numpy()\n",
    "ax.bar(range(len(latent)), latent, color='darkgreen', alpha=0.7)\n",
    "ax.set_title(f'FC-RBM Latent\\n{all_reps[\"latent\"].shape}', fontsize=11)\n",
    "ax.set_xlabel('Hidden Unit')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.suptitle('CDBN Layer-by-Layer Representations', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Config.OUTPUT_DIR, 'cdbn_layer_representations.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Layer-by-layer visualization saved to output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae543e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ FC-RBM Pretrained ‚Äî Unsupervised CDBN Complete\n",
    "\n",
    "## Summary\n",
    "\n",
    "The complete CDBN has been built and pretrained using **unsupervised learning** with Contrastive Divergence:\n",
    "\n",
    "### Architecture Summary\n",
    "\n",
    "| Layer | Type | Configuration | Output Shape |\n",
    "|-------|------|--------------|--------------|\n",
    "| Input | OCT Image | 128√ó128 grayscale | [B, 1, 128, 128] |\n",
    "| Layer 1 | Conv-RBM-1 | 32 filters, 7√ó7, Gaussian-Bernoulli | [B, 32, 122, 122] |\n",
    "| Pool | Probabilistic | 2√ó2 sum-based | [B, 32, 61, 61] |\n",
    "| Layer 2 | Conv-RBM-2 | 64 filters, 5√ó5, Bernoulli-Bernoulli | [B, 64, 57, 57] |\n",
    "| Flatten | - | - | [B, 207,936] |\n",
    "| Layer 3 | FC-RBM | 256 hidden, Bernoulli-Bernoulli | [B, 256] |\n",
    "\n",
    "### Training Summary\n",
    "\n",
    "- ‚úÖ **Conv-RBM-1**: Trained to extract low-level edge/texture features\n",
    "- ‚úÖ **Conv-RBM-2**: Trained to extract higher-level compositional features  \n",
    "- ‚úÖ **FC-RBM**: Trained to learn a compact 256-dimensional latent representation\n",
    "\n",
    "### Key Points\n",
    "\n",
    "1. **No labels used** ‚Äî All training was unsupervised\n",
    "2. **No backpropagation** ‚Äî Used Contrastive Divergence throughout\n",
    "3. **Layer-wise greedy training** ‚Äî Each layer trained independently\n",
    "4. **Probabilistic representations** ‚Äî All outputs are probabilities in [0, 1]\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "The pretrained CDBN can now be used for:\n",
    "- **Supervised fine-tuning**: Add a classifier and fine-tune with labeled data\n",
    "- **Feature extraction**: Use latent representations for downstream tasks\n",
    "- **Transfer learning**: Apply to related medical imaging tasks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760a955",
   "metadata": {},
   "source": [
    "# PART A ‚Äî Classifier on Frozen CDBN (Feature-Based)\n",
    "\n",
    "Now that the CDBN is pretrained using unsupervised learning, we add a supervised classification head to perform multi-class Eye OCT classification.\n",
    "\n",
    "**Strategy:**\n",
    "1. Freeze all CDBN layers (Conv-RBM-1, Pool, Conv-RBM-2, FC-RBM)\n",
    "2. Add a single linear layer (Softmax Classifier)\n",
    "3. Train ONLY the classifier head using labeled data\n",
    "4. Use CrossEntropyLoss and Adam optimizer\n",
    "\n",
    "This approach treats the pretrained CDBN as a **fixed feature extractor**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a08a46",
   "metadata": {},
   "source": [
    "## STEP 19 ‚Äî Softmax Classifier Head\n",
    "\n",
    "We implement a simple linear classifier that takes the 256-dimensional latent representation from the FC-RBM and outputs class probabilities.\n",
    "\n",
    "**Architecture:**\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(\\mathbf{z} \\mathbf{W}_{clf} + \\mathbf{b}_{clf})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z} \\in \\mathbb{R}^{256}$ is the latent vector from FC-RBM.\n",
    "\n",
    "Note: PyTorch's `CrossEntropyLoss` applies softmax internally, so we only need a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e70685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 19: Softmax Classifier Head\n",
    "# =============================================================================\n",
    "\n",
    "class CDBNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Softmax Classifier Head for CDBN.\n",
    "    \n",
    "    ARCHITECTURE:\n",
    "    -------------\n",
    "    Input:  Latent vector z from FC-RBM [B, n_hidden]\n",
    "    Output: Class logits [B, n_classes]\n",
    "    \n",
    "    The classifier is a single linear layer:\n",
    "        logits = z @ W_clf + b_clf\n",
    "    \n",
    "    Softmax is applied implicitly by CrossEntropyLoss during training.\n",
    "    For inference, use softmax to get probabilities.\n",
    "    \n",
    "    DESIGN CHOICES:\n",
    "    ---------------\n",
    "    1. Single linear layer (no hidden layers) - keeps the classifier simple\n",
    "       and forces the CDBN to learn discriminative features\n",
    "    2. No dropout - the latent representation is already compressed\n",
    "    3. Xavier initialization - suitable for linear layers with softmax\n",
    "    \n",
    "    Args:\n",
    "        n_latent: Dimensionality of latent vector (default: 256)\n",
    "        n_classes: Number of output classes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_latent: int = 256, n_classes: int = 4):\n",
    "        super(CDBNClassifier, self).__init__()\n",
    "        \n",
    "        self.n_latent = n_latent\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Single linear layer: z -> logits\n",
    "        self.fc = nn.Linear(n_latent, n_classes)\n",
    "        \n",
    "        # Xavier initialization for better convergence\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "    \n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through classifier.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent vector from FC-RBM [B, n_latent]\n",
    "            \n",
    "        Returns:\n",
    "            logits: Raw class scores [B, n_classes]\n",
    "                    (NOT probabilities - apply softmax for probabilities)\n",
    "        \"\"\"\n",
    "        logits = self.fc(z)\n",
    "        return logits\n",
    "    \n",
    "    def predict_proba(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent vector from FC-RBM [B, n_latent]\n",
    "            \n",
    "        Returns:\n",
    "            probs: Class probabilities [B, n_classes]\n",
    "        \"\"\"\n",
    "        logits = self.forward(z)\n",
    "        return torch.softmax(logits, dim=1)\n",
    "    \n",
    "    def predict(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get predicted class indices.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent vector from FC-RBM [B, n_latent]\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Predicted class indices [B]\n",
    "        \"\"\"\n",
    "        logits = self.forward(z)\n",
    "        return torch.argmax(logits, dim=1)\n",
    "\n",
    "\n",
    "# Determine number of classes from dataset\n",
    "n_classes = len(train_dataset.classes)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASSIFIER CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Latent dimension: {fc_rbm.n_hidden}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create classifier\n",
    "classifier = CDBNClassifier(\n",
    "    n_latent=fc_rbm.n_hidden,\n",
    "    n_classes=n_classes\n",
    ").to(Config.DEVICE)\n",
    "\n",
    "# Verify classifier\n",
    "test_z = torch.randn(4, fc_rbm.n_hidden).to(Config.DEVICE)\n",
    "test_logits = classifier(test_z)\n",
    "test_probs = classifier.predict_proba(test_z)\n",
    "test_preds = classifier.predict(test_z)\n",
    "\n",
    "print(f\"\\n‚úì CDBNClassifier created!\")\n",
    "print(f\"  Input shape:  {test_z.shape}\")\n",
    "print(f\"  Logits shape: {test_logits.shape}\")\n",
    "print(f\"  Probs shape:  {test_probs.shape}\")\n",
    "print(f\"  Preds shape:  {test_preds.shape}\")\n",
    "print(f\"  Prob sum (should be 1.0): {test_probs.sum(dim=1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e34c8d",
   "metadata": {},
   "source": [
    "## STEP 20 ‚Äî Dataset Wrapper for Latent Features\n",
    "\n",
    "To speed up training, we **cache** the latent representations extracted by the frozen CDBN. This avoids recomputing features every epoch.\n",
    "\n",
    "**Process:**\n",
    "1. Pass all images through the frozen CDBN once\n",
    "2. Store (latent_vector, label) pairs\n",
    "3. Create PyTorch Datasets for train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0613af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 20: Dataset Wrapper for Cached Latent Features (Disk-Backed)\n",
    "# =============================================================================\n",
    "\n",
    "class DiskBackedLatentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that loads latent features from disk on-demand.\n",
    "    \n",
    "    For large datasets (5.5 GB+), this avoids loading all features into RAM.\n",
    "    Features are stored as numpy arrays and loaded batch-by-batch.\n",
    "    \n",
    "    BENEFITS:\n",
    "    ---------\n",
    "    1. Memory efficient - only load what's needed for current batch\n",
    "    2. Scales to very large datasets\n",
    "    3. Faster subsequent runs (cached on disk)\n",
    "    \n",
    "    Args:\n",
    "        cache_dir: Directory where latent vectors are cached\n",
    "        split_name: Name of the split ('train', 'val', 'test')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str, split_name: str):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.split_name = split_name\n",
    "        self.latent_file = os.path.join(cache_dir, f\"{split_name}_latents.pt\")\n",
    "        self.labels_file = os.path.join(cache_dir, f\"{split_name}_labels.pt\")\n",
    "        \n",
    "        # Load metadata to get length\n",
    "        if os.path.exists(self.latent_file):\n",
    "            # Load the tensors\n",
    "            self.latent_vectors = torch.load(self.latent_file, map_location='cpu')\n",
    "            self.labels = torch.load(self.labels_file, map_location='cpu')\n",
    "            print(f\"  ‚úì Loaded cached {split_name} features: {len(self.labels)} samples\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Cache file not found: {self.latent_file}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        return self.latent_vectors[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class LatentFeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset of cached latent features from the frozen CDBN (in-memory).\n",
    "    \n",
    "    This dataset stores precomputed latent vectors, eliminating the need\n",
    "    to run the CDBN forward pass during each training epoch.\n",
    "    \n",
    "    For smaller datasets where RAM is available.\n",
    "    \n",
    "    Args:\n",
    "        latent_vectors: Tensor of latent vectors [N, n_hidden]\n",
    "        labels: Tensor of class labels [N]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_vectors: torch.Tensor, labels: torch.Tensor):\n",
    "        self.latent_vectors = latent_vectors\n",
    "        self.labels = labels\n",
    "        \n",
    "        assert len(latent_vectors) == len(labels), \\\n",
    "            f\"Mismatch: {len(latent_vectors)} vectors, {len(labels)} labels\"\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        return self.latent_vectors[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def extract_and_cache_features_to_disk(\n",
    "    image_loader: DataLoader,\n",
    "    feature_extractor: CDBNFeatureExtractor,\n",
    "    device: torch.device,\n",
    "    cache_dir: str,\n",
    "    split_name: str,\n",
    "    memory_cleanup_freq: int = 50\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Extract latent features and save to disk (memory-efficient for large datasets).\n",
    "    \n",
    "    Processes data in batches, saves to disk, and only keeps final tensors.\n",
    "    Uses periodic memory cleanup to avoid GPU memory spikes.\n",
    "    \n",
    "    Args:\n",
    "        image_loader: DataLoader with (image, label) batches\n",
    "        feature_extractor: Frozen CDBN feature extractor\n",
    "        device: Computation device\n",
    "        cache_dir: Directory to cache features\n",
    "        split_name: Name of the split ('train', 'val', 'test')\n",
    "        memory_cleanup_freq: How often to clean GPU cache\n",
    "        \n",
    "    Returns:\n",
    "        latent_file: Path to cached latent vectors\n",
    "        labels_file: Path to cached labels\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    latent_file = os.path.join(cache_dir, f\"{split_name}_latents.pt\")\n",
    "    labels_file = os.path.join(cache_dir, f\"{split_name}_labels.pt\")\n",
    "    \n",
    "    # Check if already cached\n",
    "    if os.path.exists(latent_file) and os.path.exists(labels_file):\n",
    "        print(f\"  ‚úì {split_name} features already cached, loading...\")\n",
    "        latent_vectors = torch.load(latent_file, map_location='cpu')\n",
    "        labels = torch.load(labels_file, map_location='cpu')\n",
    "        return latent_vectors, labels\n",
    "    \n",
    "    all_latents = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(f\"Extracting {split_name} latent features (disk-backed)...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(image_loader):\n",
    "            # Extract latent representations\n",
    "            latents = feature_extractor.extract_latent(images)\n",
    "            \n",
    "            # Store on CPU immediately to free GPU memory\n",
    "            all_latents.append(latents.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            \n",
    "            # Explicit cleanup\n",
    "            del images, latents\n",
    "            \n",
    "            # Periodic memory cleanup\n",
    "            if torch.cuda.is_available() and (batch_idx + 1) % memory_cleanup_freq == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"  Processed batch {batch_idx + 1}/{len(image_loader)}\")\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    latent_vectors = torch.cat(all_latents, dim=0)\n",
    "    labels_tensor = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    # Free the list memory\n",
    "    del all_latents, all_labels\n",
    "    \n",
    "    # Save to disk\n",
    "    torch.save(latent_vectors, latent_file)\n",
    "    torch.save(labels_tensor, labels_file)\n",
    "    \n",
    "    print(f\"‚úì Extracted and cached {len(latent_vectors)} latent vectors to disk\")\n",
    "    print(f\"  Cache files: {latent_file}\")\n",
    "    \n",
    "    # Clear GPU cache after extraction\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return latent_vectors, labels_tensor\n",
    "\n",
    "\n",
    "# Legacy function for backward compatibility\n",
    "def extract_and_cache_features(\n",
    "    image_loader: DataLoader,\n",
    "    feature_extractor: CDBNFeatureExtractor,\n",
    "    device: torch.device\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Extract latent features from all images in a dataloader (in-memory version).\n",
    "    \n",
    "    For smaller datasets. For large datasets (5.5 GB+), use \n",
    "    extract_and_cache_features_to_disk() instead.\n",
    "    \"\"\"\n",
    "    all_latents = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"Extracting latent features...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(image_loader):\n",
    "            latents = feature_extractor.extract_latent(images)\n",
    "            all_latents.append(latents.cpu())\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            del images, latents\n",
    "            \n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"  Processed batch {batch_idx}/{len(image_loader)}\")\n",
    "            \n",
    "            if torch.cuda.is_available() and batch_idx % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    latent_vectors = torch.cat(all_latents, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    print(f\"‚úì Extracted {len(latent_vectors)} latent vectors\")\n",
    "    return latent_vectors, labels\n",
    "\n",
    "\n",
    "print(\"‚úì LatentFeatureDataset classes and extraction functions defined\")\n",
    "print(f\"  Cache directory: {config.LATENT_CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create Train/Val/Test Splits and Cache Features (Disk-Backed for 5.5GB Dataset)\n",
    "# =============================================================================\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split training data into train and validation sets (80/20)\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_subset, val_subset = random_split(\n",
    "    train_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # Reproducibility\n",
    ")\n",
    "\n",
    "# Create DataLoaders for splitting (using streaming-safe settings)\n",
    "train_split_loader = DataLoader(\n",
    "    train_subset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY,\n",
    "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False,\n",
    "    prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None\n",
    ")\n",
    "val_split_loader = DataLoader(\n",
    "    val_subset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY\n",
    ")\n",
    "test_loader_for_caching = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET SPLITS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training samples:   {len(train_subset)}\")\n",
    "print(f\"Validation samples: {len(val_subset)}\")\n",
    "print(f\"Test samples:       {len(test_dataset)}\")\n",
    "print(f\"Feature cache dir:  {config.LATENT_CACHE_DIR}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract and cache latent features to DISK for each split\n",
    "# This avoids holding all features in RAM for the 5.5GB dataset\n",
    "\n",
    "print(\"\\n--- Caching Training Features to Disk ---\")\n",
    "train_latents, train_labels = extract_and_cache_features_to_disk(\n",
    "    train_split_loader, cdbn_extractor, config.DEVICE,\n",
    "    cache_dir=config.LATENT_CACHE_DIR,\n",
    "    split_name='train',\n",
    "    memory_cleanup_freq=50\n",
    ")\n",
    "\n",
    "print(\"\\n--- Caching Validation Features to Disk ---\")\n",
    "val_latents, val_labels = extract_and_cache_features_to_disk(\n",
    "    val_split_loader, cdbn_extractor, config.DEVICE,\n",
    "    cache_dir=config.LATENT_CACHE_DIR,\n",
    "    split_name='val',\n",
    "    memory_cleanup_freq=50\n",
    ")\n",
    "\n",
    "print(\"\\n--- Caching Test Features to Disk ---\")\n",
    "test_latents, test_labels = extract_and_cache_features_to_disk(\n",
    "    test_loader_for_caching, cdbn_extractor, config.DEVICE,\n",
    "    cache_dir=config.LATENT_CACHE_DIR,\n",
    "    split_name='test',\n",
    "    memory_cleanup_freq=50\n",
    ")\n",
    "\n",
    "# Clear GPU memory after feature extraction\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory()\n",
    "\n",
    "# Create LatentFeatureDatasets (now using cached tensors)\n",
    "train_latent_dataset = LatentFeatureDataset(train_latents, train_labels)\n",
    "val_latent_dataset = LatentFeatureDataset(val_latents, val_labels)\n",
    "test_latent_dataset = LatentFeatureDataset(test_latents, test_labels)\n",
    "\n",
    "# Create DataLoaders for latent features (larger batch size = CLASSIFIER_BATCH_SIZE)\n",
    "train_latent_loader = DataLoader(\n",
    "    train_latent_dataset, \n",
    "    batch_size=config.CLASSIFIER_BATCH_SIZE,  # 128 for classifier\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Features are small, no need for workers\n",
    "    pin_memory=config.PIN_MEMORY\n",
    ")\n",
    "val_latent_loader = DataLoader(\n",
    "    val_latent_dataset, \n",
    "    batch_size=config.CLASSIFIER_BATCH_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "test_latent_loader = DataLoader(\n",
    "    test_latent_dataset, \n",
    "    batch_size=config.CLASSIFIER_BATCH_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CACHED LATENT FEATURE DATASETS (Disk-Backed)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Train: {len(train_latent_dataset)} samples, {len(train_latent_loader)} batches\")\n",
    "print(f\"Val:   {len(val_latent_dataset)} samples, {len(val_latent_loader)} batches\")\n",
    "print(f\"Test:  {len(test_latent_dataset)} samples, {len(test_latent_loader)} batches\")\n",
    "print(f\"Latent vector dim:    {train_latents.shape[1]}\")\n",
    "print(f\"Classifier batch size: {config.CLASSIFIER_BATCH_SIZE}\")\n",
    "print(f\"Cache location:        {config.LATENT_CACHE_DIR}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d319c9",
   "metadata": {},
   "source": [
    "## STEP 21 ‚Äî Supervised Training (Frozen CDBN)\n",
    "\n",
    "Train only the classifier head on cached latent features.\n",
    "\n",
    "**Training Configuration:**\n",
    "- Loss: CrossEntropyLoss (includes softmax internally)\n",
    "- Optimizer: Adam with default parameters\n",
    "- Epochs: 30\n",
    "- Metrics: Training loss, Validation accuracy\n",
    "\n",
    "The CDBN remains **completely frozen** ‚Äî only the classifier weights are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 21: Supervised Training Loop for Classifier Head\n",
    "# =============================================================================\n",
    "\n",
    "def train_classifier(\n",
    "    classifier: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    n_epochs: int = 30,\n",
    "    learning_rate: float = 1e-3,\n",
    "    device: torch.device = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train the classifier head on cached latent features.\n",
    "    \n",
    "    TRAINING PROCEDURE:\n",
    "    -------------------\n",
    "    1. Forward pass: z -> logits\n",
    "    2. Compute CrossEntropyLoss\n",
    "    3. Backward pass (only classifier gradients)\n",
    "    4. Adam optimizer step\n",
    "    5. Evaluate on validation set\n",
    "    \n",
    "    Args:\n",
    "        classifier: CDBNClassifier instance\n",
    "        train_loader: DataLoader with (latent, label) pairs\n",
    "        val_loader: Validation DataLoader\n",
    "        n_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for Adam\n",
    "        device: Computation device\n",
    "        \n",
    "    Returns:\n",
    "        history: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    classifier = classifier.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler (reduce on plateau)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"CLASSIFIER TRAINING (FROZEN CDBN)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Epochs: {n_epochs}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Optimizer: Adam\")\n",
    "    print(f\"Loss: CrossEntropyLoss\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # =====================================================================\n",
    "        # TRAINING PHASE\n",
    "        # =====================================================================\n",
    "        classifier.train()\n",
    "        train_losses = []\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for latents, labels in train_loader:\n",
    "            latents = latents.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits = classifier(latents)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            train_losses.append(loss.item())\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # =====================================================================\n",
    "        # VALIDATION PHASE\n",
    "        # =====================================================================\n",
    "        classifier.eval()\n",
    "        val_losses = []\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for latents, labels in val_loader:\n",
    "                latents = latents.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                logits = classifier(latents)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = classifier.state_dict().copy()\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1:2d}/{n_epochs}] | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        classifier.load_state_dict(best_model_state)\n",
    "        print(f\"\\n‚úì Restored best model with Val Acc: {best_val_acc:.4f}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"‚úì train_classifier function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71717df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Train the Classifier Head\n",
    "# =============================================================================\n",
    "\n",
    "# Training configuration\n",
    "CLASSIFIER_EPOCHS = 20  # Kaggle: reduced from 30 for time limits\n",
    "CLASSIFIER_LR = 1e-3\n",
    "\n",
    "# Train classifier on cached latent features\n",
    "frozen_history = train_classifier(\n",
    "    classifier=classifier,\n",
    "    train_loader=train_latent_loader,\n",
    "    val_loader=val_latent_loader,\n",
    "    n_epochs=CLASSIFIER_EPOCHS,\n",
    "    learning_rate=CLASSIFIER_LR,\n",
    "    device=Config.DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d21adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize Training Progress (Frozen CDBN)\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "ax1 = axes[0]\n",
    "epochs = range(1, len(frozen_history['train_loss']) + 1)\n",
    "ax1.plot(epochs, frozen_history['train_loss'], 'b-', linewidth=2, label='Train Loss')\n",
    "ax1.plot(epochs, frozen_history['val_loss'], 'r-', linewidth=2, label='Val Loss')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Classifier Training Loss (Frozen CDBN)', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy curves\n",
    "ax2 = axes[1]\n",
    "ax2.plot(epochs, frozen_history['train_acc'], 'b-', linewidth=2, label='Train Acc')\n",
    "ax2.plot(epochs, frozen_history['val_acc'], 'r-', linewidth=2, label='Val Acc')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Classifier Training Accuracy (Frozen CDBN)', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Config.OUTPUT_DIR, 'frozen_cdbn_classifier_training.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Best Validation Accuracy: {max(frozen_history['val_acc']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f195707",
   "metadata": {},
   "source": [
    "# PART B ‚Äî End-to-End Fine-Tuning (Optional)\n",
    "\n",
    "Now we wrap the entire CDBN + Classifier into a single `nn.Module` and fine-tune all layers with backpropagation.\n",
    "\n",
    "**Key Differences from Frozen Training:**\n",
    "1. All CDBN parameters are **unfrozen**\n",
    "2. Use a **smaller learning rate** (1e-4) to avoid destroying pretrained features\n",
    "3. Monitor for **overfitting** as the model has many more parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f7f9b",
   "metadata": {},
   "source": [
    "## STEP 22 ‚Äî End-to-End CDBN Model\n",
    "\n",
    "We create a unified `nn.Module` that encapsulates the entire pipeline:\n",
    "\n",
    "**Pipeline:**\n",
    "$$\n",
    "\\text{Image} \\xrightarrow{\\text{Conv-RBM-1}} \\xrightarrow{\\text{Pool}} \\xrightarrow{\\text{Conv-RBM-2}} \\xrightarrow{\\text{Flatten}} \\xrightarrow{\\text{FC-RBM}} \\xrightarrow{\\text{Classifier}} \\text{Logits}\n",
    "$$\n",
    "\n",
    "All weights are initialized from the pretrained RBMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 22: End-to-End CDBN Model with Classifier\n",
    "# =============================================================================\n",
    "\n",
    "class EndToEndCDBN(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-End Convolutional Deep Belief Network for Classification.\n",
    "    \n",
    "    COMPLETE ARCHITECTURE:\n",
    "    ----------------------\n",
    "    \n",
    "    Input: OCT Image [B, 1, 128, 128]\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Conv-RBM-1 (Pretrained)             ‚îÇ\n",
    "    ‚îÇ  W: [32, 1, 7, 7], uses only         ‚îÇ\n",
    "    ‚îÇ  hidden_probabilities path           ‚îÇ\n",
    "    ‚îÇ  Output: [B, 32, 122, 122]           ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Probabilistic Pooling               ‚îÇ\n",
    "    ‚îÇ  2√ó2 sum-based pooling               ‚îÇ\n",
    "    ‚îÇ  Output: [B, 32, 61, 61]             ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Conv-RBM-2 (Pretrained)             ‚îÇ\n",
    "    ‚îÇ  W: [64, 32, 5, 5]                   ‚îÇ\n",
    "    ‚îÇ  Output: [B, 64, 57, 57]             ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Flatten                             ‚îÇ\n",
    "    ‚îÇ  Output: [B, 207936]                 ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  FC-RBM (Pretrained)                 ‚îÇ\n",
    "    ‚îÇ  Linear: 207936 ‚Üí 256                ‚îÇ\n",
    "    ‚îÇ  Output: [B, 256]                    ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Classifier (Trained)                ‚îÇ\n",
    "    ‚îÇ  Linear: 256 ‚Üí n_classes             ‚îÇ\n",
    "    ‚îÇ  Output: [B, n_classes]              ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    \n",
    "    FINE-TUNING CONSIDERATIONS:\n",
    "    ---------------------------\n",
    "    1. All pretrained weights are copied (not referenced)\n",
    "    2. Pooling is non-parametric (no gradients)\n",
    "    3. For fine-tuning, all RBM layers use their forward paths\n",
    "       (hidden_probabilities) which are differentiable\n",
    "    4. Use small learning rate to preserve pretrained features\n",
    "    \n",
    "    Args:\n",
    "        conv_rbm_1: Pretrained ConvRBM (layer 1)\n",
    "        pooling: ProbabilisticPooling module\n",
    "        conv_rbm_2: Pretrained ConvRBM (layer 2)\n",
    "        fc_rbm: Pretrained FCRBM\n",
    "        classifier: Trained CDBNClassifier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_rbm_1: ConvRBM,\n",
    "        pooling: ProbabilisticPooling,\n",
    "        conv_rbm_2: ConvRBM,\n",
    "        fc_rbm: FCRBM,\n",
    "        classifier: CDBNClassifier,\n",
    "        flatten_dim: int\n",
    "    ):\n",
    "        super(EndToEndCDBN, self).__init__()\n",
    "        \n",
    "        # Store layer references (will copy weights)\n",
    "        self.conv_rbm_1 = conv_rbm_1\n",
    "        self.pooling = pooling\n",
    "        self.conv_rbm_2 = conv_rbm_2\n",
    "        self.fc_rbm = fc_rbm\n",
    "        self.classifier = classifier\n",
    "        self.flatten_dim = flatten_dim\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through entire CDBN + Classifier.\n",
    "        \n",
    "        Args:\n",
    "            x: Input images [B, 1, 128, 128]\n",
    "            \n",
    "        Returns:\n",
    "            logits: Class logits [B, n_classes]\n",
    "        \"\"\"\n",
    "        # Layer 1: Conv-RBM-1 (differentiable path)\n",
    "        h1 = self.conv_rbm_1.hidden_probabilities(x)\n",
    "        \n",
    "        # Layer 2: Probabilistic Pooling\n",
    "        h1_pooled = self.pooling(h1)\n",
    "        \n",
    "        # Layer 3: Conv-RBM-2\n",
    "        h2 = self.conv_rbm_2.hidden_probabilities(h1_pooled)\n",
    "        \n",
    "        # Layer 4: Flatten\n",
    "        flat = h2.view(h2.size(0), -1)\n",
    "        \n",
    "        # Layer 5: FC-RBM\n",
    "        z = self.fc_rbm.hidden_probabilities(flat)\n",
    "        \n",
    "        # Layer 6: Classifier\n",
    "        logits = self.classifier(z)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_latent(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get latent representation before classifier.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            h1 = self.conv_rbm_1.hidden_probabilities(x)\n",
    "            h1_pooled = self.pooling(h1)\n",
    "            h2 = self.conv_rbm_2.hidden_probabilities(h1_pooled)\n",
    "            flat = h2.view(h2.size(0), -1)\n",
    "            z = self.fc_rbm.hidden_probabilities(flat)\n",
    "        return z\n",
    "    \n",
    "    def freeze_cdbn(self):\n",
    "        \"\"\"Freeze all CDBN layers (for feature-based training).\"\"\"\n",
    "        for module in [self.conv_rbm_1, self.conv_rbm_2, self.fc_rbm]:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "        print(\"‚úì CDBN layers frozen\")\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        \"\"\"Unfreeze all layers (for end-to-end fine-tuning).\"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"‚úì All layers unfrozen for fine-tuning\")\n",
    "    \n",
    "    def count_parameters(self) -> dict:\n",
    "        \"\"\"Count trainable and total parameters.\"\"\"\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return {\n",
    "            'total': total,\n",
    "            'trainable': trainable,\n",
    "            'frozen': total - trainable\n",
    "        }\n",
    "\n",
    "\n",
    "# Create end-to-end model from pretrained components\n",
    "e2e_model = EndToEndCDBN(\n",
    "    conv_rbm_1=conv_rbm_1,\n",
    "    pooling=prob_pool,\n",
    "    conv_rbm_2=conv_rbm_2,\n",
    "    fc_rbm=fc_rbm,\n",
    "    classifier=classifier,\n",
    "    flatten_dim=FLAT_DIM\n",
    ").to(Config.DEVICE)\n",
    "\n",
    "# Verify the model\n",
    "test_input = torch.randn(4, 1, 128, 128).to(Config.DEVICE)\n",
    "test_output = e2e_model(test_input)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"END-TO-END CDBN MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Input shape:  {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "params = e2e_model.count_parameters()\n",
    "print(f\"Total parameters:     {params['total']:,}\")\n",
    "print(f\"Trainable parameters: {params['trainable']:,}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b36c4",
   "metadata": {},
   "source": [
    "## STEP 23 ‚Äî Fine-Tuning with Backpropagation\n",
    "\n",
    "Now we unfreeze all layers and fine-tune the entire network with a small learning rate.\n",
    "\n",
    "**Fine-Tuning Strategy:**\n",
    "- Learning rate: 1e-4 (10x smaller than classifier training)\n",
    "- All parameters updated via backpropagation\n",
    "- Monitor for overfitting (train acc >> val acc)\n",
    "- Early stopping based on validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 23: End-to-End Fine-Tuning Training Loop\n",
    "# =============================================================================\n",
    "\n",
    "def finetune_e2e_model(\n",
    "    model: EndToEndCDBN,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    n_epochs: int = 20,\n",
    "    learning_rate: float = 1e-4,\n",
    "    device: torch.device = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Fine-tune the entire CDBN + Classifier end-to-end.\n",
    "    \n",
    "    FINE-TUNING PROCEDURE:\n",
    "    ----------------------\n",
    "    1. Unfreeze all parameters\n",
    "    2. Use small learning rate to preserve pretrained features\n",
    "    3. Standard backpropagation through entire network\n",
    "    4. Monitor overfitting behavior\n",
    "    \n",
    "    Args:\n",
    "        model: EndToEndCDBN instance\n",
    "        train_loader: DataLoader with (image, label) pairs\n",
    "        val_loader: Validation DataLoader\n",
    "        n_epochs: Number of fine-tuning epochs\n",
    "        learning_rate: Learning rate (should be small, e.g., 1e-4)\n",
    "        device: Computation device\n",
    "        \n",
    "    Returns:\n",
    "        history: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Unfreeze all layers for fine-tuning\n",
    "    model.unfreeze_all()\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Use different learning rates for different parts (optional refinement)\n",
    "    # Here we use a single small LR for simplicity\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"END-TO-END FINE-TUNING\")\n",
    "    print(\"=\" * 70)\n",
    "    params = model.count_parameters()\n",
    "    print(f\"Epochs: {n_epochs}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Trainable Parameters: {params['trainable']:,}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # =====================================================================\n",
    "        # TRAINING PHASE\n",
    "        # =====================================================================\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass through entire network\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass (gradients flow through all layers)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            train_losses.append(loss.item())\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            # Progress update\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"  Epoch [{epoch+1}/{n_epochs}] \"\n",
    "                      f\"Batch [{batch_idx}/{len(train_loader)}] \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # =====================================================================\n",
    "        # VALIDATION PHASE\n",
    "        # =====================================================================\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        overfit_gap = train_acc - val_acc\n",
    "        overfit_warning = \" ‚ö†Ô∏è OVERFITTING\" if overfit_gap > 0.1 else \"\"\n",
    "        \n",
    "        print(f\"‚ñ∂ Epoch [{epoch+1:2d}/{n_epochs}] | \"\n",
    "              f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | \"\n",
    "              f\"Gap: {overfit_gap:.4f}{overfit_warning}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\n‚úì Restored best model with Val Acc: {best_val_acc:.4f}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"FINE-TUNING COMPLETE!\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"‚úì finetune_e2e_model function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba971f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create DataLoaders for Image-based Training (Fine-tuning)\n",
    "# =============================================================================\n",
    "\n",
    "# Need image-based loaders for fine-tuning (not cached features)\n",
    "train_image_loader = DataLoader(train_subset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "val_image_loader = DataLoader(val_subset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"‚úì Image DataLoaders created for fine-tuning\")\n",
    "print(f\"  Train batches: {len(train_image_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_image_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Fine-Tune the End-to-End Model\n",
    "# =============================================================================\n",
    "\n",
    "# Fine-tuning configuration\n",
    "FINETUNE_EPOCHS = 10  # Kaggle: reduced from 20 for time limits\n",
    "FINETUNE_LR = 1e-4  # Small learning rate to preserve pretrained features\n",
    "\n",
    "# Fine-tune the model\n",
    "finetune_history = finetune_e2e_model(\n",
    "    model=e2e_model,\n",
    "    train_loader=train_image_loader,\n",
    "    val_loader=val_image_loader,\n",
    "    n_epochs=FINETUNE_EPOCHS,\n",
    "    learning_rate=FINETUNE_LR,\n",
    "    device=Config.DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fadbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize Fine-Tuning Results\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Fine-tuning Loss\n",
    "ax1 = axes[0, 0]\n",
    "epochs_ft = range(1, len(finetune_history['train_loss']) + 1)\n",
    "ax1.plot(epochs_ft, finetune_history['train_loss'], 'b-', linewidth=2, label='Train Loss')\n",
    "ax1.plot(epochs_ft, finetune_history['val_loss'], 'r-', linewidth=2, label='Val Loss')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Fine-Tuning Loss', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Fine-tuning Accuracy\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epochs_ft, finetune_history['train_acc'], 'b-', linewidth=2, label='Train Acc')\n",
    "ax2.plot(epochs_ft, finetune_history['val_acc'], 'r-', linewidth=2, label='Val Acc')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Fine-Tuning Accuracy', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# Plot 3: Comparison - Frozen vs Fine-tuned (Validation Accuracy)\n",
    "ax3 = axes[1, 0]\n",
    "epochs_frozen = range(1, len(frozen_history['val_acc']) + 1)\n",
    "ax3.plot(epochs_frozen, frozen_history['val_acc'], 'g-o', linewidth=2, \n",
    "         markersize=4, label='Frozen CDBN', alpha=0.7)\n",
    "ax3.plot(epochs_ft, finetune_history['val_acc'], 'b-s', linewidth=2, \n",
    "         markersize=4, label='Fine-tuned', alpha=0.7)\n",
    "ax3.set_xlabel('Epoch', fontsize=12)\n",
    "ax3.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax3.set_title('Frozen vs Fine-tuned Comparison', fontsize=14)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "# Plot 4: Overfitting Analysis\n",
    "ax4 = axes[1, 1]\n",
    "train_val_gap = [t - v for t, v in zip(finetune_history['train_acc'], finetune_history['val_acc'])]\n",
    "ax4.bar(epochs_ft, train_val_gap, color='orange', alpha=0.7)\n",
    "ax4.axhline(y=0.1, color='red', linestyle='--', label='Overfitting Threshold')\n",
    "ax4.set_xlabel('Epoch', fontsize=12)\n",
    "ax4.set_ylabel('Train - Val Accuracy Gap', fontsize=12)\n",
    "ax4.set_title('Overfitting Analysis', fontsize=14)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Config.OUTPUT_DIR, 'finetune_results.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Fine-tuning Results:\")\n",
    "print(f\"  Best Frozen Val Acc:    {max(frozen_history['val_acc']):.4f}\")\n",
    "print(f\"  Best Fine-tuned Val Acc: {max(finetune_history['val_acc']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33de85b",
   "metadata": {},
   "source": [
    "# PART C ‚Äî Evaluation\n",
    "\n",
    "Final evaluation on the held-out test set with comprehensive metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3b4f5",
   "metadata": {},
   "source": [
    "## STEP 24 ‚Äî Test Set Evaluation\n",
    "\n",
    "Comprehensive evaluation of both models:\n",
    "1. **Frozen CDBN + Classifier** (feature-based)\n",
    "2. **Fine-tuned End-to-End CDBN**\n",
    "\n",
    "Metrics computed:\n",
    "- Overall accuracy\n",
    "- Per-class accuracy\n",
    "- Confusion matrix\n",
    "- Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb53f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 24: Test Set Evaluation Functions\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_frozen_model(\n",
    "    classifier: CDBNClassifier,\n",
    "    test_latent_loader: DataLoader,\n",
    "    class_names: list,\n",
    "    device: torch.device\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the frozen CDBN + Classifier on test set.\n",
    "    \n",
    "    Args:\n",
    "        classifier: Trained CDBNClassifier\n",
    "        test_latent_loader: DataLoader with cached latent features\n",
    "        class_names: List of class names\n",
    "        device: Computation device\n",
    "        \n",
    "    Returns:\n",
    "        metrics: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    classifier.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for latents, labels in test_latent_loader:\n",
    "            latents = latents.to(device)\n",
    "            preds = classifier.predict(latents)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Compute metrics\n",
    "    overall_acc = accuracy_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, \n",
    "                                         target_names=class_names, \n",
    "                                         output_dict=True)\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_acc = {}\n",
    "    for i, name in enumerate(class_names):\n",
    "        mask = all_labels == i\n",
    "        if mask.sum() > 0:\n",
    "            per_class_acc[name] = (all_preds[mask] == i).mean()\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report,\n",
    "        'per_class_accuracy': per_class_acc,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_e2e_model(\n",
    "    model: EndToEndCDBN,\n",
    "    test_image_loader: DataLoader,\n",
    "    class_names: list,\n",
    "    device: torch.device\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the end-to-end CDBN on test set.\n",
    "    \n",
    "    Args:\n",
    "        model: EndToEndCDBN instance\n",
    "        test_image_loader: DataLoader with test images\n",
    "        class_names: List of class names\n",
    "        device: Computation device\n",
    "        \n",
    "    Returns:\n",
    "        metrics: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_image_loader:\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Compute metrics\n",
    "    overall_acc = accuracy_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, \n",
    "                                         target_names=class_names, \n",
    "                                         output_dict=True)\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_acc = {}\n",
    "    for i, name in enumerate(class_names):\n",
    "        mask = all_labels == i\n",
    "        if mask.sum() > 0:\n",
    "            per_class_acc[name] = (all_preds[mask] == i).mean()\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report,\n",
    "        'per_class_accuracy': per_class_acc,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55acd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Evaluate Frozen CDBN + Classifier\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATING FROZEN CDBN + CLASSIFIER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "frozen_metrics = evaluate_frozen_model(\n",
    "    classifier=classifier,\n",
    "    test_latent_loader=test_latent_loader,\n",
    "    class_names=class_names,\n",
    "    device=Config.DEVICE\n",
    ")\n",
    "\n",
    "print(f\"\\nOverall Test Accuracy: {frozen_metrics['overall_accuracy']:.4f}\")\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for name, acc in frozen_metrics['per_class_accuracy'].items():\n",
    "    print(f\"  {name}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "for name in class_names:\n",
    "    metrics = frozen_metrics['classification_report'][name]\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"    Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"    F1-Score:  {metrics['f1-score']:.4f}\")\n",
    "    print(f\"    Support:   {metrics['support']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Evaluate Fine-Tuned End-to-End Model\n",
    "# =============================================================================\n",
    "\n",
    "# Create test image loader\n",
    "test_image_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATING FINE-TUNED END-TO-END CDBN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "e2e_metrics = evaluate_e2e_model(\n",
    "    model=e2e_model,\n",
    "    test_image_loader=test_image_loader,\n",
    "    class_names=class_names,\n",
    "    device=Config.DEVICE\n",
    ")\n",
    "\n",
    "print(f\"\\nOverall Test Accuracy: {e2e_metrics['overall_accuracy']:.4f}\")\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for name, acc in e2e_metrics['per_class_accuracy'].items():\n",
    "    print(f\"  {name}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "for name in class_names:\n",
    "    metrics = e2e_metrics['classification_report'][name]\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"    Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"    F1-Score:  {metrics['f1-score']:.4f}\")\n",
    "    print(f\"    Support:   {metrics['support']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize Confusion Matrices\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Frozen CDBN Confusion Matrix\n",
    "ax1 = axes[0]\n",
    "im1 = ax1.imshow(frozen_metrics['confusion_matrix'], interpolation='nearest', cmap='Blues')\n",
    "ax1.set_title('Frozen CDBN + Classifier\\nConfusion Matrix', fontsize=14)\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax1.set_ylabel('True Label', fontsize=12)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        text = ax1.text(j, i, frozen_metrics['confusion_matrix'][i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"white\" \n",
    "                       if frozen_metrics['confusion_matrix'][i, j] > frozen_metrics['confusion_matrix'].max()/2 \n",
    "                       else \"black\")\n",
    "\n",
    "ax1.set_xticks(range(len(class_names)))\n",
    "ax1.set_yticks(range(len(class_names)))\n",
    "ax1.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax1.set_yticklabels(class_names)\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Plot 2: Fine-tuned E2E Confusion Matrix\n",
    "ax2 = axes[1]\n",
    "im2 = ax2.imshow(e2e_metrics['confusion_matrix'], interpolation='nearest', cmap='Blues')\n",
    "ax2.set_title('Fine-Tuned End-to-End CDBN\\nConfusion Matrix', fontsize=14)\n",
    "ax2.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax2.set_ylabel('True Label', fontsize=12)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        text = ax2.text(j, i, e2e_metrics['confusion_matrix'][i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"white\" \n",
    "                       if e2e_metrics['confusion_matrix'][i, j] > e2e_metrics['confusion_matrix'].max()/2 \n",
    "                       else \"black\")\n",
    "\n",
    "ax2.set_xticks(range(len(class_names)))\n",
    "ax2.set_yticks(range(len(class_names)))\n",
    "ax2.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax2.set_yticklabels(class_names)\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Config.OUTPUT_DIR, 'confusion_matrices.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f821a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Model Comparison Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Model':<35} {'Test Accuracy':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Frozen CDBN + Classifier':<35} {frozen_metrics['overall_accuracy']:.4f}\")\n",
    "print(f\"{'Fine-Tuned End-to-End CDBN':<35} {e2e_metrics['overall_accuracy']:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Determine best model\n",
    "if e2e_metrics['overall_accuracy'] > frozen_metrics['overall_accuracy']:\n",
    "    improvement = e2e_metrics['overall_accuracy'] - frozen_metrics['overall_accuracy']\n",
    "    print(f\"\\n‚úì Fine-tuning improved accuracy by {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "    best_model = \"Fine-Tuned End-to-End CDBN\"\n",
    "    best_acc = e2e_metrics['overall_accuracy']\n",
    "else:\n",
    "    best_model = \"Frozen CDBN + Classifier\"\n",
    "    best_acc = frozen_metrics['overall_accuracy']\n",
    "    print(f\"\\n‚úì Frozen CDBN performs better (no fine-tuning needed)\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   Test Accuracy: {best_acc:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Per-class comparison\n",
    "print(\"\\nPER-CLASS ACCURACY COMPARISON:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Class':<20} {'Frozen CDBN':<15} {'Fine-Tuned':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for name in class_names:\n",
    "    frozen_acc = frozen_metrics['per_class_accuracy'].get(name, 0)\n",
    "    e2e_acc = e2e_metrics['per_class_accuracy'].get(name, 0)\n",
    "    diff = e2e_acc - frozen_acc\n",
    "    diff_str = f\"+{diff:.4f}\" if diff > 0 else f\"{diff:.4f}\"\n",
    "    print(f\"{name:<20} {frozen_acc:.4f}{'':>9} {e2e_acc:.4f}{'':>9} {diff_str}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Save Models and Results\n",
    "# =============================================================================\n",
    "\n",
    "# Save model checkpoints\n",
    "torch.save({\n",
    "    'classifier_state_dict': classifier.state_dict(),\n",
    "    'classifier_config': {\n",
    "        'n_latent': classifier.n_latent,\n",
    "        'n_classes': classifier.n_classes\n",
    "    },\n",
    "    'training_history': frozen_history,\n",
    "    'test_metrics': frozen_metrics\n",
    "}, os.path.join(Config.OUTPUT_DIR, 'frozen_cdbn_classifier.pt'))\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': e2e_model.state_dict(),\n",
    "    'training_history': finetune_history,\n",
    "    'test_metrics': e2e_metrics\n",
    "}, os.path.join(Config.OUTPUT_DIR, 'finetuned_e2e_cdbn.pt'))\n",
    "\n",
    "# Save results summary to text file\n",
    "with open(os.path.join(Config.OUTPUT_DIR, 'evaluation_results.txt'), 'w') as f:\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(\"CDBN CLASSIFICATION RESULTS\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"FROZEN CDBN + CLASSIFIER\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Test Accuracy: {frozen_metrics['overall_accuracy']:.4f}\\n\\n\")\n",
    "    f.write(\"Per-Class Accuracy:\\n\")\n",
    "    for name, acc in frozen_metrics['per_class_accuracy'].items():\n",
    "        f.write(f\"  {name}: {acc:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\\nFINE-TUNED END-TO-END CDBN\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Test Accuracy: {e2e_metrics['overall_accuracy']:.4f}\\n\\n\")\n",
    "    f.write(\"Per-Class Accuracy:\\n\")\n",
    "    for name, acc in e2e_metrics['per_class_accuracy'].items():\n",
    "        f.write(f\"  {name}: {acc:.4f}\\n\")\n",
    "\n",
    "print(\"‚úì Models and results saved to output directory:\")\n",
    "print(f\"  - frozen_cdbn_classifier.pt\")\n",
    "print(f\"  - finetuned_e2e_cdbn.pt\")\n",
    "print(f\"  - evaluation_results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8c1da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ CDBN Supervised Training and Evaluation Completed\n",
    "\n",
    "## Summary\n",
    "\n",
    "The complete CDBN pipeline for Eye OCT Classification has been implemented and evaluated.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| Conv-RBM-1 | 32 filters, 7√ó7, Gaussian-Bernoulli |\n",
    "| Pooling | 2√ó2 probabilistic pooling |\n",
    "| Conv-RBM-2 | 64 filters, 5√ó5, Bernoulli-Bernoulli |\n",
    "| FC-RBM | 207,936 ‚Üí 256 units |\n",
    "| Classifier | 256 ‚Üí n_classes linear layer |\n",
    "\n",
    "### Training Phases\n",
    "\n",
    "| Phase | Description | Optimizer |\n",
    "|-------|-------------|-----------|\n",
    "| **Unsupervised Pretraining** | Layer-wise CD training | Manual updates |\n",
    "| **Feature-based Training** | Train classifier on frozen CDBN | Adam (lr=1e-3) |\n",
    "| **End-to-End Fine-tuning** | Backprop through entire network | Adam (lr=1e-4) |\n",
    "\n",
    "### Key Results\n",
    "\n",
    "| Model | Test Accuracy |\n",
    "|-------|---------------|\n",
    "| Frozen CDBN + Classifier | See output above |\n",
    "| Fine-tuned End-to-End | See output above |\n",
    "\n",
    "### Saved Artifacts\n",
    "\n",
    "- `frozen_cdbn_classifier.pt` - Classifier weights and metrics\n",
    "- `finetuned_e2e_cdbn.pt` - Full model weights and metrics\n",
    "- `evaluation_results.txt` - Text summary\n",
    "- Training plots saved as PNG files\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Unsupervised pretraining** provides a good initialization for classification\n",
    "2. **Frozen CDBN** approach is fast and often sufficient for smaller datasets\n",
    "3. **Fine-tuning** can improve performance but requires careful regularization\n",
    "4. **Per-class metrics** reveal class-specific challenges\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e49944",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Notebook Hardened for Kaggle ‚Äî Ready for Full Execution\n",
    "\n",
    "## Kaggle-Specific Modifications Applied\n",
    "\n",
    "### ‚úÖ STEP A: GPU Detection & CUDA Settings\n",
    "- GPU assertion at startup (fails fast if no GPU)\n",
    "- `torch.backends.cudnn.benchmark = True` for auto-tuning\n",
    "- `torch.backends.cudnn.deterministic = False` for performance\n",
    "\n",
    "### ‚úÖ STEP B: Dataset Paths & Sanity Checks\n",
    "- Dataset path: `/kaggle/input/<YOUR_DATASET_NAME>/OCT`\n",
    "- Pre-training validation of GPU, paths, and data shapes\n",
    "- RuntimeError if critical checks fail\n",
    "\n",
    "### ‚úÖ STEP C: Conservative Epochs & Batch Size\n",
    "| Component | Original | Kaggle | Comment |\n",
    "|-----------|----------|--------|---------|\n",
    "| Batch Size | 32 | **16** | Fits T4/P100 16GB |\n",
    "| NUM_WORKERS | 4 | **2** | Kaggle CPU cores |\n",
    "| Conv-RBM-1 | 10 epochs | **5** | Time limit |\n",
    "| Conv-RBM-2 | 10 epochs | **5** | Time limit |\n",
    "| FC-RBM | 10 epochs | **5** | Time limit |\n",
    "| Classifier | 30 epochs | **20** | Time limit |\n",
    "| Fine-tuning | 20 epochs | **10** | Time limit |\n",
    "\n",
    "### ‚úÖ STEP D: Visualization Guards\n",
    "- `DEBUG = False` disables heavy visualizations\n",
    "- Set `DEBUG = True` for local debugging only\n",
    "\n",
    "### ‚úÖ STEP E: Memory Cleanup\n",
    "- `torch.cuda.empty_cache()` after each RBM training\n",
    "- Trainer objects deleted to free velocity tensors\n",
    "\n",
    "### ‚úÖ STEP F: Output Directory\n",
    "- All outputs saved to `/kaggle/working`\n",
    "- Subdirectories: `models/`, `plots/`, `logs/`\n",
    "\n",
    "### ‚úÖ STEP G: Sanity Summary\n",
    "- Pre-flight check cell validates environment before training\n",
    "\n",
    "---\n",
    "\n",
    "## Before Running on Kaggle\n",
    "\n",
    "1. **Upload your dataset** and update `<YOUR_DATASET_NAME>` in Config\n",
    "2. **Enable GPU** in Kaggle notebook settings\n",
    "3. **Run All** cells sequentially\n",
    "4. **Download** results from `/kaggle/working`\n",
    "\n",
    "---\n",
    "\n",
    "**Expected Runtime:** ~6-8 hours on Kaggle T4 GPU (9hr limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfa09e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ Local GPU + 5.5 GB Dataset Optimized ‚Äî Ready for Execution\n",
    "\n",
    "## Optimization Summary\n",
    "\n",
    "This notebook has been optimized for:\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| **GPU Memory** | 24 GB | Full utilization with 95% memory cap |\n",
    "| **Dataset Size** | ~5.5 GB | Streaming DataLoaders, no full RAM caching |\n",
    "| **RBM Batch Size** | 32 | Optimal for Conv-RBM training |\n",
    "| **FC-RBM Batch Size** | 64 | Larger batches for FC layer |\n",
    "| **Classifier Batch Size** | 128 | Maximum throughput for small features |\n",
    "| **Workers** | 4 | Persistent workers with prefetch |\n",
    "| **Feature Caching** | Disk-backed | `./outputs_local_5gb/latent_cache/` |\n",
    "\n",
    "## Key Optimizations Applied\n",
    "\n",
    "1. **Streaming-Safe DataLoaders**\n",
    "   - `persistent_workers=True` \n",
    "   - `prefetch_factor=2`\n",
    "   - `pin_memory=True`\n",
    "\n",
    "2. **Memory Management**\n",
    "   - Periodic `torch.cuda.empty_cache()` every 100 batches\n",
    "   - Explicit tensor deletion after each batch\n",
    "   - GPU memory monitoring with `print_gpu_memory()`\n",
    "\n",
    "3. **Disk-Backed Feature Caching**\n",
    "   - Latent features saved to disk during extraction\n",
    "   - Avoids holding ~5.5 GB of features in RAM\n",
    "   - Reusable across runs\n",
    "\n",
    "4. **Pre-Flight Validation**\n",
    "   - Memory estimation before training starts\n",
    "   - Warns if memory may exceed capacity\n",
    "\n",
    "## Execution Checklist\n",
    "\n",
    "- [ ] Verify GPU is detected (run cell 4)\n",
    "- [ ] Check pre-flight memory estimation\n",
    "- [ ] Confirm dataset path in Config class\n",
    "- [ ] Run all cells sequentially\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "| Stage | Estimated Time |\n",
    "|-------|----------------|\n",
    "| Conv-RBM-1 (5 epochs) | ~15-30 min |\n",
    "| Conv-RBM-2 (3 epochs) | ~10-20 min |\n",
    "| FC-RBM (3 epochs) | ~5-10 min |\n",
    "| Classifier (30 epochs) | ~5-10 min |\n",
    "| **Total** | **~35-70 min** |\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to run!** Execute cells from top to bottom."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
